import { Groq } from "groq-sdk"

const groq = new Groq({
  apiKey: process.env.NEXT_PUBLIC_GROQ_API_KEY || "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",
  dangerouslyAllowBrowser: true,
})

// Danh s√°ch c√°c model kh·∫£ d·ª•ng (theo th·ª© t·ª± ∆∞u ti√™n)
const AVAILABLE_MODELS = [
  "llama-3.3-70b-versatile",
  "llama-3.1-8b-instant",
  "llama3-70b-8192",
  "llama3-8b-8192",
  "mixtral-8x7b-32768",
  "gemma-7b-it",
]

// Function ƒë·ªÉ t·∫°o summary th√¥ng minh c·ªßa d·ªØ li·ªáu
const createDataSummary = (data: any[]): string => {
  if (data.length === 0) return "Kh√¥ng c√≥ d·ªØ li·ªáu"

  // L·∫•y th√¥ng tin c∆° b·∫£n
  const totalRecords = data.length
  const firstRecord = data[0]
  const fieldNames = Object.keys(firstRecord.fields || {})

  // Ph√¢n t√≠ch t·ª´ng field
  const fieldAnalysis: Record<string, any> = {}

  fieldNames.forEach((fieldName) => {
    const values = data
      .map((record) => record.fields[fieldName])
      .filter((v) => v !== null && v !== undefined && v !== "")
    const uniqueValues = [...new Set(values)]

    fieldAnalysis[fieldName] = {
      totalValues: values.length,
      uniqueValues: uniqueValues.length,
      sampleValues: uniqueValues.slice(0, 5), // L·∫•y 5 gi√° tr·ªã m·∫´u
      isEmpty: values.length === 0,
    }
  })

  // T·∫°o summary
  let summary = `=== T·ªîNG QUAN D·ªÆ LI·ªÜU ===
T·ªïng s·ªë b·∫£n ghi: ${totalRecords}
S·ªë tr∆∞·ªùng d·ªØ li·ªáu: ${fieldNames.length}

=== PH√ÇN T√çCH C√ÅC TR∆Ø·ªúNG ===
`

  fieldNames.forEach((fieldName) => {
    const analysis = fieldAnalysis[fieldName]
    summary += `
üìä ${fieldName}:
   - C√≥ d·ªØ li·ªáu: ${analysis.totalValues}/${totalRecords} b·∫£n ghi
   - Gi√° tr·ªã duy nh·∫•t: ${analysis.uniqueValues}
   - M·∫´u: ${JSON.stringify(analysis.sampleValues)}
`
  })

  // Th√™m m·ªôt s·ªë records m·∫´u ƒë·∫ßy ƒë·ªß
  summary += `\n=== M·∫™U D·ªÆ LI·ªÜU CHI TI·∫æT ===\n`
  const sampleSize = Math.min(10, totalRecords)
  for (let i = 0; i < sampleSize; i++) {
    summary += `\nB·∫£n ghi ${i + 1}:\n${JSON.stringify(data[i], null, 2)}\n`
  }

  if (totalRecords > sampleSize) {
    summary += `\n... v√† ${totalRecords - sampleSize} b·∫£n ghi kh√°c v·ªõi c·∫•u tr√∫c t∆∞∆°ng t·ª±\n`
  }

  return summary
}

// Function ƒë·ªÉ chia d·ªØ li·ªáu th√†nh chunks th√¥ng minh
const createIntelligentChunks = (data: any[], maxChunkSize = 15): any[][] => {
  const chunks = []
  for (let i = 0; i < data.length; i += maxChunkSize) {
    chunks.push(data.slice(i, i + maxChunkSize))
  }
  return chunks
}

// Function th·ª≠ c√°c model kh√°c nhau
const tryWithDifferentModels = async (messages: any[], currentModelIndex = 0): Promise<string> => {
  if (currentModelIndex >= AVAILABLE_MODELS.length) {
    throw new Error("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")
  }

  const model = AVAILABLE_MODELS[currentModelIndex]
  console.log(`ü§ñ Th·ª≠ model: ${model}`)

  try {
    const chatCompletion = await groq.chat.completions.create({
      model: model,
      messages: messages,
      temperature: 0.7,
      max_tokens: 3000, // TƒÉng max_tokens
      top_p: 1,
    })

    const response = chatCompletion.choices[0].message.content
    console.log(`‚úÖ Model ${model} ho·∫°t ƒë·ªông th√†nh c√¥ng`)
    return response || "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ AI."
  } catch (error) {
    console.log(`‚ùå Model ${model} th·∫•t b·∫°i:`, error)

    // N·∫øu model b·ªã decommission ho·∫∑c kh√¥ng kh·∫£ d·ª•ng, th·ª≠ model ti·∫øp theo
    if (
      error instanceof Error &&
      (error.message.includes("decommissioned") ||
        error.message.includes("not found") ||
        error.message.includes("invalid_request_error"))
    ) {
      console.log(`üîÑ Th·ª≠ model ti·∫øp theo...`)
      return await tryWithDifferentModels(messages, currentModelIndex + 1)
    }

    // N·∫øu l√† l·ªói kh√°c (rate limit, network, etc.), throw ngay
    throw error
  }
}

export const askAI = async (context: string, question: string): Promise<string> => {
  try {
    console.log("ü§ñ B·∫Øt ƒë·∫ßu g·ªçi Groq API...")
    console.log("üìù Context length:", context.length)
    console.log("‚ùì Question:", question)

    // Parse d·ªØ li·ªáu t·ª´ context ƒë·ªÉ x·ª≠ l√Ω th√¥ng minh
    let processedContext = context
    let dataArray: any[] = []

    // T√¨m v√† extract d·ªØ li·ªáu JSON t·ª´ context
    const dataMatch = context.match(
      /D∆∞·ªõi ƒë√¢y l√†.*?d·ªØ li·ªáu t·ª´ b·∫£ng[^:]*:\s*([\s\S]*?)\s*(?:T·ªïng c·ªông c√≥|H√£y ph√¢n t√≠ch)/i,
    )

    if (dataMatch) {
      try {
        const rawData = dataMatch[1]
        dataArray = JSON.parse(rawData)
        console.log(`üìä Ph√°t hi·ªán ${dataArray.length} records trong context`)

        // N·∫øu d·ªØ li·ªáu l·ªõn, t·∫°o summary th√¥ng minh
        if (context.length > 20000 || dataArray.length > 20) {
          console.log("üß† T·∫°o summary th√¥ng minh cho d·ªØ li·ªáu l·ªõn...")

          const intelligentSummary = createDataSummary(dataArray)

          // Thay th·∫ø context v·ªõi summary th√¥ng minh
          processedContext = context.replace(
            rawData,
            `${intelligentSummary}\n\n‚ö†Ô∏è ƒê√¢y l√† t√≥m t·∫Øt th√¥ng minh c·ªßa ${dataArray.length} b·∫£n ghi. AI c√≥ th·ªÉ ph√¢n t√≠ch v√† tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin n√†y.`,
          )

          console.log("‚úÖ ƒê√£ t·∫°o summary th√¥ng minh, context length:", processedContext.length)
        }
      } catch (parseError) {
        console.log("‚ö†Ô∏è Kh√¥ng th·ªÉ parse d·ªØ li·ªáu JSON, s·ª≠ d·ª•ng context g·ªëc")
      }
    }

    // N·∫øu context v·∫´n qu√° l·ªõn, c·∫Øt b·ªõt
    if (processedContext.length > 25000) {
      console.log("‚ö†Ô∏è Context v·∫´n qu√° l·ªõn, c·∫Øt b·ªõt...")
      processedContext = processedContext.substring(0, 22000) + "\n\n... (D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn do qu√° l·ªõn)"
    }

    const messages = [
      {
        role: "system" as const,
        content: processedContext,
      },
      {
        role: "user" as const,
        content: question,
      },
    ]

    // Th·ª≠ v·ªõi c√°c model kh√°c nhau
    const response = await tryWithDifferentModels(messages)
    console.log("‚úÖ Groq API response received")

    return response
  } catch (error) {
    console.error("‚ùå Chi ti·∫øt l·ªói Groq API:", error)

    // X·ª≠ l√Ω c√°c lo·∫°i l·ªói c·ª• th·ªÉ
    if (error instanceof Error) {
      if (error.message.includes("rate_limit")) {
        return "‚è∞ API ƒë√£ ƒë·∫°t gi·ªõi h·∫°n t·ªëc ƒë·ªô. Vui l√≤ng th·ª≠ l·∫°i sau v√†i gi√¢y."
      } else if (error.message.includes("invalid_api_key")) {
        return "üîë API key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh."
      } else if (error.message.includes("context_length")) {
        return "üìè D·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω. H√£y th·ª≠ v·ªõi c√¢u h·ªèi c·ª• th·ªÉ h∆°n ho·∫∑c chia nh·ªè d·ªØ li·ªáu."
      } else if (error.message.includes("network") || error.message.includes("fetch")) {
        return "üåê L·ªói k·∫øt n·ªëi m·∫°ng. Vui l√≤ng ki·ªÉm tra internet v√† th·ª≠ l·∫°i."
      } else if (error.message.includes("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")) {
        return "ü§ñ T·∫•t c·∫£ c√°c AI model hi·ªán t·∫°i ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau."
      }

      return `‚ùå L·ªói AI: ${error.message}`
    }

    return "‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi AI. Vui l√≤ng th·ª≠ l·∫°i."
  }
}

// Function ph√¢n t√≠ch d·ªØ li·ªáu theo chunks
export const askAIWithChunks = async (data: any[], tableName: string, question: string): Promise<string> => {
  try {
    console.log(`üß† Ph√¢n t√≠ch d·ªØ li·ªáu theo chunks: ${data.length} records`)

    if (data.length <= 20) {
      // N·∫øu d·ªØ li·ªáu nh·ªè, x·ª≠ l√Ω b√¨nh th∆∞·ªùng
      const context = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† TO√ÄN B·ªò d·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}":

${JSON.stringify(data, null, 2)}

T·ªïng c·ªông c√≥ ${data.length} records trong b·∫£ng.`

      return await askAI(context, question)
    }

    // N·∫øu d·ªØ li·ªáu l·ªõn, s·ª≠ d·ª•ng summary th√¥ng minh
    const intelligentSummary = createDataSummary(data)

    const context = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh chuy√™n ph√¢n t√≠ch d·ªØ li·ªáu. D∆∞·ªõi ƒë√¢y l√† t√≥m t·∫Øt th√¥ng minh c·ªßa TO√ÄN B·ªò d·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}":

${intelligentSummary}

H√£y ph√¢n t√≠ch d·ªØ li·ªáu n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† h·ªØu √≠ch. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.

L∆∞U √ù: B·∫°n ƒë√£ nh·∫≠n ƒë∆∞·ª£c th√¥ng tin t√≥m t·∫Øt c·ªßa ${data.length} b·∫£n ghi ƒë·∫ßy ƒë·ªß, bao g·ªìm ph√¢n t√≠ch t·ª´ng tr∆∞·ªùng d·ªØ li·ªáu v√† c√°c m·∫´u d·ªØ li·ªáu chi ti·∫øt.`

    return await askAI(context, question)
  } catch (error) {
    console.error("‚ùå askAIWithChunks failed:", error)
    return `‚ùå L·ªói khi ph√¢n t√≠ch d·ªØ li·ªáu: ${error}`
  }
}

// Function test API v·ªõi model m·ªõi
export const testGroqAPI = async (): Promise<{ success: boolean; message: string; workingModel?: string }> => {
  console.log("üß™ Testing Groq API v·ªõi c√°c model kh·∫£ d·ª•ng...")

  for (const model of AVAILABLE_MODELS) {
    try {
      console.log(`üß™ Testing model: ${model}`)

      const testCompletion = await groq.chat.completions.create({
        model: model,
        messages: [
          {
            role: "user",
            content: "Xin ch√†o! H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát: 1+1 b·∫±ng bao nhi√™u?",
          },
        ],
        temperature: 0.1,
        max_tokens: 50,
      })

      const response = testCompletion.choices[0].message.content
      console.log(`‚úÖ Model ${model} ho·∫°t ƒë·ªông! Response:`, response)

      return {
        success: true,
        message: `Model ${model} ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng. Test response: ${response}`,
        workingModel: model,
      }
    } catch (error) {
      console.log(`‚ùå Model ${model} failed:`, error)
      continue
    }
  }

  return {
    success: false,
    message: "T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i sau.",
  }
}

// Function ƒë·ªÉ l·∫•y danh s√°ch model kh·∫£ d·ª•ng
export const getAvailableModels = (): string[] => {
  return AVAILABLE_MODELS
}
