import { Groq } from "groq-sdk"

// Danh s√°ch API keys
const API_KEYS = [
  process.env.NEXT_PUBLIC_GROQ_API_KEY_1 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_2 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_3 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_4 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_5 || "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",
].filter((key) => key && !key.includes("account") && key.startsWith("gsk_"))

// üî• UPDATED: Chuy·ªÉn sang llama3-70b-8192
const SINGLE_MODEL = "llama3-70b-8192"

// üî• SIMPLIFIED: Cache ƒë∆°n gi·∫£n
const testResultsCache = new Map<string, boolean>()

// Function ∆∞·ªõc t√≠nh s·ªë tokens (1 token ‚âà 4 characters)
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4)
}

// üî• NEW: Function ƒë·ªÉ extract plain text t·ª´ Lark Base field values
const extractPlainTextFromField = (value: unknown): string => {
  if (value === null || value === undefined) {
    return ""
  }

  // N·∫øu l√† string ƒë∆°n gi·∫£n
  if (typeof value === "string") {
    return value.trim()
  }

  // N·∫øu l√† number
  if (typeof value === "number") {
    return String(value)
  }

  // N·∫øu l√† boolean
  if (typeof value === "boolean") {
    return value ? "Yes" : "No"
  }

  // N·∫øu l√† object, c·ªë g·∫Øng extract text
  if (typeof value === "object") {
    try {
      const jsonStr = JSON.stringify(value)

      // Handle array of text objects: [{"type":"text","text":"Intel Pentium"}]
      if (jsonStr.includes('"type":"text"') && jsonStr.includes('"text":')) {
        const textMatches = jsonStr.match(/"text":"([^"]+)"/g)
        if (textMatches) {
          const texts = textMatches.map((match) => match.replace(/"text":"([^"]+)"/, "$1"))
          return texts.join(", ")
        }
      }

      // Handle single option object: {"id":"optr5hYAsF","text":"SSD-128"}
      if (jsonStr.includes('"text":') && jsonStr.includes('"id":')) {
        const textMatch = jsonStr.match(/"text":"([^"]+)"/)
        if (textMatch) {
          return textMatch[1]
        }
      }

      // Handle array of option objects: [{"id":"optr5hYAsF","text":"SSD-128"},{"id":"opt6DIhdHV","text":"HDD-256"}]
      if (jsonStr.startsWith("[") && jsonStr.includes('"text":')) {
        const textMatches = jsonStr.match(/"text":"([^"]+)"/g)
        if (textMatches) {
          const texts = textMatches.map((match) => match.replace(/"text":"([^"]+)"/, "$1"))
          return texts.join(", ")
        }
      }

      // Handle [null] arrays
      if (jsonStr === "[null]" || jsonStr === "null") {
        return ""
      }

      // Fallback: return first text value found
      const anyTextMatch = jsonStr.match(/"([^"]+)"/g)
      if (anyTextMatch && anyTextMatch.length > 0) {
        // Filter out keys like "type", "id", "text"
        const values = anyTextMatch
          .map((match) => match.replace(/"/g, ""))
          .filter((val) => !["type", "id", "text"].includes(val))

        if (values.length > 0) {
          return values.join(", ")
        }
      }

      return ""
    } catch (error) {
      console.warn("Error parsing field value:", error)
      return String(value).substring(0, 50) // Truncate long values
    }
  }

  return String(value)
}

// üî• UPDATED: CSV Conversion v·ªõi consistent format
const convertToCSV = (data: Array<{ recordId: string; fields: Record<string, unknown> }>): string => {
  if (data.length === 0) return ""

  console.log(`üìä Converting ${data.length} records to consistent CSV format...`)

  // üî• NEW: Get ALL unique field names t·ª´ T·∫§T C·∫¢ records ƒë·ªÉ ƒë·∫£m b·∫£o consistent structure
  const allFieldNames = new Set<string>()
  data.forEach((record) => {
    Object.keys(record.fields).forEach((fieldName) => {
      allFieldNames.add(fieldName)
    })
  })

  const fieldNames = Array.from(allFieldNames).sort()
  console.log(`üìã Found ${fieldNames.length} unique fields:`, fieldNames.slice(0, 5))

  // üî• NEW: Create consistent headers
  const headers = ["STT", ...fieldNames]
  const csvHeaders = headers.join(",")

  // üî• UPDATED: Convert records v·ªõi consistent column structure
  const csvRows = data.map((record, index) => {
    const values = [
      String(index + 1), // STT
      // üî• IMPORTANT: ƒê·∫£m b·∫£o T·∫§T C·∫¢ fields ƒë·ªÅu c√≥ value (empty n·∫øu kh√¥ng c√≥)
      ...fieldNames.map((fieldName) => {
        const rawValue = record.fields[fieldName]
        const cleanValue = extractPlainTextFromField(rawValue)

        // üî• NEW: Handle empty values consistently
        if (!cleanValue || cleanValue.trim() === "") {
          return "" // Empty cell
        }

        // Escape commas and quotes for CSV
        if (cleanValue.includes(",") || cleanValue.includes('"') || cleanValue.includes("\n")) {
          return `"${cleanValue.replace(/"/g, '""')}"`
        }

        return cleanValue
      }),
    ]

    // üî• IMPORTANT: Join v·ªõi comma, empty values s·∫Ω t·∫°o ra ,,
    return values.join(",")
  })

  const csvContent = [csvHeaders, ...csvRows].join("\n")

  // üî• NEW: Validate consistent structure
  const lines = csvContent.split("\n")
  const headerColumnCount = lines[0].split(",").length
  let inconsistentRows = 0

  lines.slice(1).forEach((line, index) => {
    const columnCount = line.split(",").length
    if (columnCount !== headerColumnCount) {
      inconsistentRows++
      console.warn(`‚ö†Ô∏è Row ${index + 1} has ${columnCount} columns, expected ${headerColumnCount}`)
    }
  })

  // Calculate compression stats
  const originalJsonSize = JSON.stringify(data).length
  const csvSize = csvContent.length
  const compressionRatio = Math.round((1 - csvSize / originalJsonSize) * 100)

  console.log(`‚úÖ Consistent CSV Conversion Complete:`)
  console.log(`  üìä Records: ${data.length}`)
  console.log(`  üìã Fields: ${fieldNames.length}`)
  console.log(`  üìÑ Total columns: ${headerColumnCount} (STT + ${fieldNames.length} fields)`)
  console.log(`  üìÑ Original JSON: ${originalJsonSize} chars`)
  console.log(`  üìÑ Consistent CSV: ${csvSize} chars`)
  console.log(`  üéØ Compression: ${compressionRatio}% smaller`)
  console.log(`  üéØ Estimated tokens: ${estimateTokens(csvContent)}`)
  console.log(`  ‚úÖ Inconsistent rows: ${inconsistentRows}/${data.length}`)

  // Show sample of consistent data
  const sampleRows = csvContent.split("\n").slice(0, 3)
  console.log(`üìã Sample consistent CSV:`)
  sampleRows.forEach((row, i) => {
    const cols = row.split(",")
    console.log(`  ${i === 0 ? "Header" : `Row ${i}`}: ${cols.length} columns`)
    console.log(`    ${row.substring(0, 100)}${row.length > 100 ? "..." : ""}`)
  })

  return csvContent
}

// üî• NEW: Smart chunking by character count, kh√¥ng c·∫Øt gi·ªØa record
const createSmartCSVChunks = (csvContent: string): { csvChunks: string[]; chunkStats: any[] } => {
  console.log(`üìä ===== SMART CSV CHUNKING =====`)

  const lines = csvContent.split("\n")
  const headerLine = lines[0]
  const dataLines = lines.slice(1)

  console.log(`üìã Total: ${lines.length} lines (1 header + ${dataLines.length} data rows)`)
  console.log(`üìÑ Total CSV size: ${csvContent.length} characters`)

  // üî• Calculate target chunk size
  const processingAPIs = API_KEYS.length - 1 // Tr·ª´ 1 API cho analysis
  const targetChunkSize = Math.floor(csvContent.length / processingAPIs)

  console.log(`üîß Processing APIs: ${processingAPIs} (${API_KEYS.length} total - 1 for analysis)`)
  console.log(`üéØ Target chunk size: ${targetChunkSize} characters each`)

  const csvChunks: string[] = []
  const chunkStats: any[] = []

  let currentChunk = headerLine + "\n" // B·∫Øt ƒë·∫ßu v·ªõi header
  let currentChunkSize = headerLine.length + 1
  let currentRowCount = 0

  for (let i = 0; i < dataLines.length; i++) {
    const dataLine = dataLines[i]
    const lineSize = dataLine.length + 1 // +1 for \n

    // üî• Check if adding this line would exceed target size
    if (currentChunkSize + lineSize > targetChunkSize && currentRowCount > 0) {
      // üî• Finish current chunk (kh√¥ng c·∫Øt gi·ªØa record)
      csvChunks.push(currentChunk.trim())
      chunkStats.push({
        chunkIndex: csvChunks.length,
        rows: currentRowCount,
        characters: currentChunkSize,
        targetSize: targetChunkSize,
        efficiency: Math.round((currentChunkSize / targetChunkSize) * 100),
      })

      console.log(
        `üì¶ Chunk ${csvChunks.length}: ${currentRowCount} rows, ${currentChunkSize} chars (${Math.round((currentChunkSize / targetChunkSize) * 100)}% of target)`,
      )

      // üî• Start new chunk with header
      currentChunk = headerLine + "\n" + dataLine + "\n"
      currentChunkSize = headerLine.length + 1 + lineSize
      currentRowCount = 1
    } else {
      // üî• Add line to current chunk
      currentChunk += dataLine + "\n"
      currentChunkSize += lineSize
      currentRowCount++
    }
  }

  // üî• Add final chunk if has data
  if (currentRowCount > 0) {
    csvChunks.push(currentChunk.trim())
    chunkStats.push({
      chunkIndex: csvChunks.length,
      rows: currentRowCount,
      characters: currentChunkSize,
      targetSize: targetChunkSize,
      efficiency: Math.round((currentChunkSize / targetChunkSize) * 100),
    })

    console.log(
      `üì¶ Final Chunk ${csvChunks.length}: ${currentRowCount} rows, ${currentChunkSize} chars (${Math.round((currentChunkSize / targetChunkSize) * 100)}% of target)`,
    )
  }

  // üî• Summary
  const totalChunks = csvChunks.length
  const totalProcessedChars = chunkStats.reduce((sum, stat) => sum + stat.characters, 0)
  const avgEfficiency = Math.round(chunkStats.reduce((sum, stat) => sum + stat.efficiency, 0) / totalChunks)

  console.log(`üìä Smart Chunking Summary:`)
  console.log(`  üéØ Target: ${processingAPIs} chunks of ~${targetChunkSize} chars each`)
  console.log(`  ‚úÖ Created: ${totalChunks} chunks`)
  console.log(`  üìÑ Total processed: ${totalProcessedChars}/${csvContent.length} chars`)
  console.log(`  üìà Average efficiency: ${avgEfficiency}% of target size`)
  console.log(`  üîß Remaining API for analysis: 1`)

  // Show chunk details
  chunkStats.forEach((stat) => {
    console.log(`    Chunk ${stat.chunkIndex}: ${stat.rows} rows, ${stat.characters} chars (${stat.efficiency}%)`)
  })

  console.log(`===============================================`)

  return { csvChunks, chunkStats }
}

// üî• SIMPLIFIED: CSV Validation
const validateCSV = (csvContent: string): { isValid: boolean; rowCount: number; error?: string } => {
  try {
    const lines = csvContent.trim().split("\n")
    if (lines.length < 2) {
      return { isValid: false, rowCount: 0, error: "CSV must have at least header and one data row" }
    }

    const headerCount = lines[0].split(",").length
    let validRows = 0

    for (let i = 1; i < lines.length; i++) {
      const rowCols = lines[i].split(",").length
      if (rowCols === headerCount) {
        validRows++
      }
    }

    return {
      isValid: validRows > 0,
      rowCount: validRows,
      error: validRows === 0 ? "No valid data rows found" : undefined,
    }
  } catch (error) {
    return {
      isValid: false,
      rowCount: 0,
      error: `CSV validation error: ${error}`,
    }
  }
}

// üî• UPDATED: Test single API key v·ªõi llama3-70b-8192
const testSingleAPI = async (keyIndex: number): Promise<boolean> => {
  const cacheKey = `test_${keyIndex}`

  // Check cache first
  if (testResultsCache.has(cacheKey)) {
    console.log(`üîÑ Using cached test result for API ${keyIndex + 1}`)
    return testResultsCache.get(cacheKey)!
  }

  try {
    const apiKey = API_KEYS[keyIndex]
    console.log(`üß™ Testing API ${keyIndex + 1} with ${SINGLE_MODEL}`)

    const groq = createGroqClient(apiKey)

    // CH·ªà 1 REQUEST DUY NH·∫§T
    const testCompletion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: "Test: Return 'OK'",
        },
      ],
      temperature: 0.1,
      max_tokens: 10,
    })

    const response = testCompletion?.choices?.[0]?.message?.content
    const success = !!response

    // Cache result
    testResultsCache.set(cacheKey, success)

    console.log(`‚úÖ API ${keyIndex + 1} ${SINGLE_MODEL}: ${success ? "OK" : "FAILED"}`)
    if (success) {
      console.log(`üîç Response: "${response}"`)
    }
    return success
  } catch (error) {
    console.log(`‚ùå API ${keyIndex + 1} ${SINGLE_MODEL} failed:`, error)
    testResultsCache.set(cacheKey, false)
    return false
  }
}

const createGroqClient = (apiKey: string): Groq => {
  return new Groq({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
  })
}

// üî• UPDATED: Process single CSV chunk v·ªõi better error handling
const processSingleCSVChunk = async (
  apiKey: string,
  keyIndex: number,
  csvChunk: string,
  chunkIndex: number,
  totalChunks: number,
): Promise<{ success: boolean; optimizedData: string; keyIndex: number; error?: string }> => {
  try {
    const estimatedTokens = estimateTokens(csvChunk)

    console.log(`\nüîß ===== API ${keyIndex + 1} - CSV CHUNK ${chunkIndex + 1}/${totalChunks} =====`)
    console.log(`üìä INPUT: ${estimatedTokens} tokens`)
    console.log(`‚ö° Model: ${SINGLE_MODEL}`)

    // Validate CSV
    const csvValidation = validateCSV(csvChunk)
    if (!csvValidation.isValid) {
      console.log(`‚ùå CSV VALIDATION FAILED: ${csvValidation.error}`)
      return {
        success: false,
        optimizedData: "",
        keyIndex: keyIndex,
        error: `CSV validation failed: ${csvValidation.error}`,
      }
    }

    console.log(`‚úÖ CSV Validation: ${csvValidation.rowCount} valid rows`)

    const groq = createGroqClient(apiKey)

    // üî• UPDATED: Simple prompt cho clean CSV
    const optimizePrompt = `Clean this CSV data, remove empty rows, keep consistent column structure:

${csvChunk}

Return clean CSV with same column structure:`

    const promptTokens = estimateTokens(optimizePrompt)
    console.log(`üì§ Sending request: ${promptTokens} input tokens`)

    const startTime = Date.now()

    // üî• CH·ªà 1 REQUEST DUY NH·∫§T v·ªõi llama3-70b-8192
    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [{ role: "user", content: optimizePrompt }],
      temperature: 0.1,
      max_tokens: 4000, // TƒÉng max_tokens cho llama3-70b
    })

    const responseTime = Date.now() - startTime
    console.log(`üì• Response received (${responseTime}ms)`)

    if (!completion?.choices?.[0]?.message?.content) {
      const errorMsg = "Empty response from API"
      console.log(`‚ùå ${errorMsg}`)
      return {
        success: false,
        optimizedData: "",
        keyIndex: keyIndex,
        error: errorMsg,
      }
    }

    const optimizedCSV = completion.choices[0].message.content.trim()
    const outputTokens = estimateTokens(optimizedCSV)

    console.log(`üìä OUTPUT: ${outputTokens} tokens`)
    console.log(`‚ö° Processing time: ${responseTime}ms`)

    // üî• IMPROVED: More lenient CSV validation
    const optimizedValidation = validateCSV(optimizedCSV)
    if (optimizedValidation.isValid) {
      console.log(`‚úÖ SUCCESS: Valid CSV with ${optimizedValidation.rowCount} rows`)
      console.log(`===== END CHUNK ${chunkIndex + 1} =====\n`)

      return {
        success: true,
        optimizedData: optimizedCSV,
        keyIndex: keyIndex,
      }
    } else {
      // üî• FALLBACK: N·∫øu validation fail, v·∫´n return original chunk
      console.log(`‚ö†Ô∏è Validation failed but using original chunk: ${optimizedValidation.error}`)
      return {
        success: true,
        optimizedData: csvChunk, // Use original chunk
        keyIndex: keyIndex,
      }
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå CHUNK ${chunkIndex + 1} FAILED: ${errorMsg}`)

    // üî• IMPROVED: Return original chunk instead of failing
    console.log(`üîÑ Using original chunk as fallback`)
    return {
      success: true,
      optimizedData: csvChunk, // Use original chunk as fallback
      keyIndex: keyIndex,
      error: `Processing failed, using original: ${errorMsg}`,
    }
  }
}

// üî• UPDATED: Main pipeline v·ªõi smart chunking
export const preprocessDataWithPipeline = async (
  data: any[],
  tableName: string,
): Promise<{ success: boolean; optimizedData: string; analysis: string; keyUsage: any }> => {
  try {
    console.log(`üöÄ Smart CSV Pipeline v·ªõi ${data.length} records - Model: ${SINGLE_MODEL}`)

    if (!API_KEYS || API_KEYS.length < 2) {
      throw new Error("C·∫ßn √≠t nh·∫•t 2 API keys (1 cho processing, 1 cho analysis)")
    }

    // üî• B∆Ø·ªöC 1: Convert to consistent CSV
    console.log(`üìä B∆Ø·ªöC 1: Convert to consistent CSV format...`)
    const fullCSV = convertToCSV(data)

    if (!fullCSV) {
      throw new Error("Kh√¥ng th·ªÉ t·∫°o CSV content")
    }

    // üî• B∆Ø·ªöC 2: Smart chunking by character count
    console.log(`üìä B∆Ø·ªöC 2: Smart chunking by character count...`)
    const { csvChunks, chunkStats } = createSmartCSVChunks(fullCSV)

    if (csvChunks.length === 0) {
      throw new Error("Kh√¥ng th·ªÉ t·∫°o CSV chunks")
    }

    // Show sample of consistent CSV
    console.log(`üìã Sample consistent CSV:`)
    const sampleLines = csvChunks[0].split("\n").slice(0, 3)
    sampleLines.forEach((line, i) => {
      const cols = line.split(",")
      console.log(`  ${i === 0 ? "Header" : `Row ${i}`}: ${cols.length} columns`)
      console.log(`    ${line.substring(0, 80)}${line.length > 80 ? "..." : ""}`)
    })

    // üî• B∆Ø·ªöC 3: Test processing APIs (kh√¥ng test analysis API)
    const processingAPICount = Math.min(csvChunks.length, API_KEYS.length - 1)
    console.log(`üß™ B∆Ø·ªöC 3: Test ${processingAPICount} processing APIs...`)

    const keyTests = []
    for (let i = 0; i < processingAPICount; i++) {
      console.log(`üß™ Testing processing API ${i + 1}...`)
      const testResult = await testSingleAPI(i)
      keyTests.push(testResult)

      if (testResult) {
        console.log(`‚úÖ Processing API ${i + 1} working`)
      } else {
        console.log(`‚ùå Processing API ${i + 1} failed`)
      }
    }

    const workingKeys = keyTests.filter(Boolean).length
    console.log(`üîë ${workingKeys}/${processingAPICount} processing APIs ho·∫°t ƒë·ªông`)

    if (workingKeys === 0) {
      console.log(`‚ö†Ô∏è No working processing APIs, using raw consistent CSV`)
      return {
        success: true,
        optimizedData: fullCSV,
        analysis: `‚ö†Ô∏è Kh√¥ng c√≥ processing APIs ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}, s·ª≠ d·ª•ng consistent CSV v·ªõi ${data.length} records.`,
        keyUsage: { error: true, format: "Consistent CSV", fallback: true, model: SINGLE_MODEL },
      }
    }

    // üî• B∆Ø·ªöC 4: Process chunks v·ªõi working APIs
    console.log(`‚è≥ B∆Ø·ªöC 4: Process ${csvChunks.length} CSV chunks v·ªõi ${workingKeys} working APIs...`)

    const processResults = []

    // X·ª≠ l√Ω t·ª´ng chunk v·ªõi API t∆∞∆°ng ·ª©ng
    for (let i = 0; i < csvChunks.length; i++) {
      const csvChunk = csvChunks[i]
      const keyIndex = i % workingKeys // Cycle through working keys

      console.log(`üîß Processing chunk ${i + 1}/${csvChunks.length} v·ªõi API ${keyIndex + 1}`)
      console.log(`üìä Chunk stats: ${chunkStats[i].rows} rows, ${chunkStats[i].characters} chars`)

      // CH·ªà 1 REQUEST DUY NH·∫§T
      const result = await processSingleCSVChunk(API_KEYS[keyIndex], keyIndex, csvChunk, i, csvChunks.length)

      processResults.push(result)

      // Delay nh·ªè gi·ªØa c√°c chunks
      if (i < csvChunks.length - 1) {
        await new Promise((resolve) => setTimeout(resolve, 1000))
      }
    }

    // üî• IMPROVED: Accept partial success
    const successfulResults = processResults.filter((r) => r && r.success)
    console.log(`üìä Results: ${successfulResults.length}/${csvChunks.length} chunks th√†nh c√¥ng`)

    // üî• IMPROVED: Ch·ªâ fail n·∫øu kh√¥ng c√≥ chunk n√†o th√†nh c√¥ng
    if (successfulResults.length === 0) {
      console.log(`‚ùå All chunks failed, using consistent raw CSV`)
      return {
        success: true,
        optimizedData: fullCSV,
        analysis: `‚ö†Ô∏è T·∫•t c·∫£ chunks th·∫•t b·∫°i v·ªõi ${SINGLE_MODEL}, s·ª≠ d·ª•ng consistent CSV v·ªõi ${data.length} records.`,
        keyUsage: {
          totalKeys: API_KEYS.length,
          processedChunks: 0,
          fallback: true,
          format: "Consistent CSV",
          model: SINGLE_MODEL,
        },
      }
    }

    // üî• B∆Ø·ªöC 5: Merge chunks back to consistent CSV
    console.log(`üìä B∆Ø·ªöC 5: Merge ${successfulResults.length} processed chunks...`)

    let combinedCSVData = ""
    let headers = ""
    const allRows: string[] = []
    let validChunks = 0

    successfulResults.forEach((result, index) => {
      try {
        const csvLines = result.optimizedData.trim().split("\n")
        if (csvLines.length < 2) {
          console.log(`‚ö†Ô∏è Chunk ${index + 1} has insufficient data`)
          return
        }

        if (validChunks === 0) {
          headers = csvLines[0]
          allRows.push(...csvLines.slice(1))
        } else {
          // üî• IMPORTANT: Skip header, only add data rows
          allRows.push(...csvLines.slice(1))
        }
        validChunks++
        console.log(`‚úÖ Merged chunk ${index + 1}: ${csvLines.length - 1} rows`)
      } catch (parseError) {
        console.warn(`‚ö†Ô∏è Kh√¥ng th·ªÉ parse CSV result t·ª´ chunk ${index + 1}:`, parseError)
      }
    })

    combinedCSVData = headers + "\n" + allRows.join("\n")
    const finalTokens = estimateTokens(combinedCSVData)

    console.log(`üìä Final Consistent CSV: ${allRows.length} total rows, ${finalTokens} tokens`)

    // Validate final CSV consistency
    const finalValidation = validateCSV(combinedCSVData)
    console.log(`‚úÖ Final CSV validation: ${finalValidation.isValid ? "PASSED" : "FAILED"}`)
    if (finalValidation.isValid) {
      console.log(`üìä Final CSV: ${finalValidation.rowCount} valid rows`)
    }

    // üî• B∆Ø·ªöC 6: Analysis v·ªõi API cu·ªëi c√πng
    const analysisAPIIndex = API_KEYS.length - 1
    console.log(`ü§ñ B∆Ø·ªöC 6: Analysis v·ªõi API ${analysisAPIIndex + 1} (dedicated analysis API) - Model: ${SINGLE_MODEL}`)

    const analysisPrompt = `Ph√¢n t√≠ch d·ªØ li·ªáu CSV t·ª´ b·∫£ng "${tableName}" (${data.length} records):

${combinedCSVData.substring(0, 3000)}${combinedCSVData.length > 3000 ? "..." : ""}

T√≥m t·∫Øt:
1. T·ªïng quan d·ªØ li·ªáu
2. Th·ªëng k√™ ch√≠nh  
3. Insights quan tr·ªçng

Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát.`

    const finalAnalysis = await analyzeWithLlama(API_KEYS[analysisAPIIndex], analysisPrompt)

    const keyUsage = {
      totalKeys: API_KEYS.length,
      processingKeys: processingAPICount,
      analysisKeys: 1,
      processedChunks: successfulResults.length,
      successRate: `${Math.round((successfulResults.length / csvChunks.length) * 100)}%`,
      chunks: csvChunks.length,
      validChunks: validChunks,
      finalDataSize: combinedCSVData.length,
      finalTokens: finalTokens,
      format: "Consistent CSV",
      model: SINGLE_MODEL,
      chunkingStrategy: "Smart by character count",
      avgChunkEfficiency: Math.round(chunkStats.reduce((sum, stat) => sum + stat.efficiency, 0) / chunkStats.length),
    }

    return {
      success: true,
      optimizedData: combinedCSVData,
      analysis: finalAnalysis,
      keyUsage: keyUsage,
    }
  } catch (error) {
    console.error("‚ùå Smart CSV Pipeline failed:", error)

    // üî• IMPROVED: Always return consistent raw CSV as fallback
    const rawCSV = convertToCSV(data)
    return {
      success: true,
      optimizedData: rawCSV,
      analysis: `‚ùå Pipeline error v·ªõi ${SINGLE_MODEL}: ${error}. S·ª≠ d·ª•ng consistent CSV v·ªõi ${data.length} records.`,
      keyUsage: { error: true, format: "Consistent CSV", model: SINGLE_MODEL, fallback: true },
    }
  }
}

// üî• UPDATED: Analysis v·ªõi llama3-70b-8192
const analyzeWithLlama = async (apiKey: string, prompt: string): Promise<string> => {
  try {
    const promptTokens = estimateTokens(prompt)
    console.log(`ü§ñ Analysis v·ªõi ${SINGLE_MODEL}: ${promptTokens} tokens`)

    const groq = createGroqClient(apiKey)
    const startTime = Date.now()

    // CH·ªà 1 REQUEST DUY NH·∫§T
    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: prompt,
        },
      ],
      temperature: 0.7,
      max_tokens: 2000,
    })

    const responseTime = Date.now() - startTime

    if (!completion?.choices?.[0]?.message?.content) {
      throw new Error("No response content")
    }

    const analysis = completion.choices[0].message.content || "Kh√¥ng c√≥ ph√¢n t√≠ch"
    const outputTokens = estimateTokens(analysis)

    console.log(`‚úÖ Analysis complete: ${outputTokens} tokens (${responseTime}ms)`)

    return analysis
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå Analysis failed: ${errorMsg}`)
    return `‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch v·ªõi ${SINGLE_MODEL}: ${errorMsg}`
  }
}

// üî• UPDATED: Answer question v·ªõi CSV
export const answerQuestionWithOptimizedData = async (
  optimizedCSVData: string,
  tableName: string,
  question: string,
  originalRecordCount: number,
): Promise<string> => {
  try {
    console.log(`ü§î Tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi consistent CSV data (${originalRecordCount} records)`)

    // üî• IMPROVED: Truncate CSV if too long
    const maxCSVLength = 4000
    const truncatedCSV =
      optimizedCSVData.length > maxCSVLength ? optimizedCSVData.substring(0, maxCSVLength) + "..." : optimizedCSVData

    const questionPrompt = `D·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}" (${originalRecordCount} records):

${truncatedCSV}

C√¢u h·ªèi: ${question}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát:`

    // CH·ªà 1 REQUEST DUY NH·∫§T v·ªõi analysis API
    const analysisAPIIndex = API_KEYS.length - 1
    return await analyzeWithLlama(API_KEYS[analysisAPIIndex], questionPrompt)
  } catch (error) {
    console.error("‚ùå answerQuestionWithOptimizedData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi ${SINGLE_MODEL}: ${error}`
  }
}

// Export function ch√≠nh
export const analyzeDataWithParallelKeys = preprocessDataWithPipeline

// üî• SIMPLIFIED: Answer question function
export const answerQuestionWithData = async (
  data: any[],
  tableName: string,
  question: string,
  previousAnalysis?: string,
  optimizedData?: string,
): Promise<string> => {
  try {
    if (optimizedData && optimizedData.length > 0) {
      return await answerQuestionWithOptimizedData(optimizedData, tableName, question, data.length)
    } else {
      const quickCSV = convertToCSV(data.slice(0, 30))
      return await answerQuestionWithOptimizedData(quickCSV, tableName, question, data.length)
    }
  } catch (error) {
    console.error("‚ùå answerQuestionWithData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi: ${error}`
  }
}

// üî• UPDATED: Test all API keys v·ªõi llama3-70b-8192
export const testAllApiKeys = async (): Promise<{
  success: boolean
  message: string
  workingKeys: number
  totalKeys: number
  keyDetails: any[]
}> => {
  console.log(`üß™ Testing ${API_KEYS.length} API keys v·ªõi ${SINGLE_MODEL}...`)

  const testPromises = API_KEYS.map(async (apiKey, index) => {
    try {
      const groq = createGroqClient(apiKey)

      const testCompletion = await groq.chat.completions.create({
        model: SINGLE_MODEL,
        messages: [
          {
            role: "user",
            content: "Test: Return 'OK'",
          },
        ],
        temperature: 0.1,
        max_tokens: 10,
      })

      const response = testCompletion?.choices?.[0]?.message?.content || "No response"
      console.log(`‚úÖ API ${index + 1}: OK`)

      return {
        keyIndex: index + 1,
        status: "success" as const,
        response: response,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
      }
    } catch (error) {
      console.log(`‚ùå API ${index + 1}: ${error}`)
      return {
        keyIndex: index + 1,
        status: "failed" as const,
        error: error instanceof Error ? error.message : String(error),
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
      }
    }
  })

  const results = await Promise.all(testPromises)
  const workingKeys = results.filter((r) => r.status === "success").length

  return {
    success: workingKeys > 0,
    message: `${workingKeys}/${API_KEYS.length} API keys ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}`,
    workingKeys: workingKeys,
    totalKeys: API_KEYS.length,
    keyDetails: results,
  }
}

// Backward compatibility functions
export const askAI = async (context: string, question: string): Promise<string> => {
  return await answerQuestionWithData([], "Unknown", question)
}

export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const testGroqAPI = async () => {
  const result = await testAllApiKeys()
  return {
    success: result.success,
    message: result.message,
    workingModel: SINGLE_MODEL,
    format: "Consistent CSV",
  }
}

export const getAvailableModels = (): string[] => {
  return [SINGLE_MODEL]
}

export const getApiKeysInfo = () => {
  return {
    totalKeys: API_KEYS.length,
    keysPreview: API_KEYS.map(
      (key, index) => `API ${index + 1}: ${key.substring(0, 10)}...${key.substring(key.length - 4)} (${SINGLE_MODEL})`,
    ),
    format: "Consistent CSV",
    model: SINGLE_MODEL,
  }
}

// üî• SIMPLIFIED: Clear cache function
export const clearApiCache = () => {
  testResultsCache.clear()
  console.log("üîÑ Cache cleared")
}
