import { Groq } from "groq-sdk"

// Danh sÃ¡ch API keys
const API_KEYS = [
  process.env.NEXT_PUBLIC_GROQ_API_KEY_1 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_2 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_3 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_4 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_5 || "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",
].filter((key) => key && !key.includes("account") && key.startsWith("gsk_"))

const AVAILABLE_MODELS = [
  "llama-3.3-70b-versatile", // v25 - chá»‰ dÃ¹ng model nÃ y
]

// Function Æ°á»›c tÃ­nh sá»‘ tokens (1 token â‰ˆ 4 characters)
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4)
}

// ThÃªm function test single chunk trÆ°á»›c khi cháº¡y pipeline
const testSingleChunk = async (chunk: any[], keyIndex: number): Promise<boolean> => {
  try {
    const apiKey = API_KEYS[keyIndex]
    const chunkText = JSON.stringify(chunk, null, 1)
    const estimatedTokens = estimateTokens(chunkText)

    console.log(`ğŸ§ª Test chunk: ${chunk.length} records, ~${estimatedTokens} tokens`)

    if (estimatedTokens > 15000) {
      console.log(`âš ï¸ Chunk quÃ¡ lá»›n (${estimatedTokens} tokens), cáº§n chia nhá» hÆ¡n`)
      return false
    }

    const groq = createGroqClient(apiKey)

    // Test vá»›i prompt Ä‘Æ¡n giáº£n
    const testCompletion = await groq.chat.completions.create({
      model: "llama-3.3-70b-versatile",
      messages: [
        {
          role: "user",
          content: "Test: Return 'OK'",
        },
      ],
      temperature: 0.1,
      max_tokens: 10,
    })

    const response = testCompletion?.choices?.[0]?.message?.content
    console.log(`âœ… Key ${keyIndex + 1} test OK: ${response}`)
    return true
  } catch (error) {
    console.log(`âŒ Key ${keyIndex + 1} test failed: ${error}`)
    return false
  }
}

// ThÃªm function test optimize Ä‘Æ¡n giáº£n
const testOptimizeSimple = async (keyIndex: number): Promise<boolean> => {
  try {
    const apiKey = API_KEYS[keyIndex]
    const groq = createGroqClient(apiKey)

    console.log(`ğŸ§ª Test optimize vá»›i key ${keyIndex + 1}...`)

    // Test vá»›i data Ä‘Æ¡n giáº£n
    const testData = [{ id: 1, name: "test", value: null }]
    const testPrompt = `Optimize JSON: ${JSON.stringify(testData)}. Return optimized JSON only:`

    const completion = await groq.chat.completions.create({
      model: "llama-3.3-70b-versatile",
      messages: [{ role: "user", content: testPrompt }],
      temperature: 0.1,
      max_tokens: 100,
    })

    const result = completion?.choices?.[0]?.message?.content
    console.log(`âœ… Key ${keyIndex + 1} optimize test result:`, result?.substring(0, 100))

    // Thá»­ parse JSON
    if (result) {
      JSON.parse(result.trim())
      return true
    }
    return false
  } catch (error) {
    console.log(`âŒ Key ${keyIndex + 1} optimize test failed:`, error)
    return false
  }
}

// Function chia dá»¯ liá»‡u theo token limit
const chunkDataByTokens = (data: any[], maxTokensPerChunk = 4000): any[][] => {
  const chunks: any[][] = []
  let currentChunk: any[] = []
  let currentTokens = 0

  console.log(`ğŸ“Š Báº¯t Ä‘áº§u chia ${data.length} records vá»›i limit ${maxTokensPerChunk} tokens/chunk`)

  for (const record of data) {
    const recordText = JSON.stringify(record, null, 1)
    const recordTokens = estimateTokens(recordText)

    // Log record Ä‘áº§u tiÃªn Ä‘á»ƒ debug
    if (currentChunk.length === 0 && chunks.length === 0) {
      console.log(`ğŸ“Š Sample record tokens: ${recordTokens} (${recordText.length} chars)`)

      // Náº¿u 1 record Ä‘Ã£ quÃ¡ lá»›n, cáº£nh bÃ¡o
      if (recordTokens > maxTokensPerChunk) {
        console.warn(`âš ï¸ Single record quÃ¡ lá»›n: ${recordTokens} tokens > ${maxTokensPerChunk} limit`)
      }
    }

    // Náº¿u thÃªm record nÃ y vÃ o chunk hiá»‡n táº¡i sáº½ vÆ°á»£t quÃ¡ limit
    if (currentTokens + recordTokens > maxTokensPerChunk && currentChunk.length > 0) {
      console.log(`ğŸ“Š Chunk ${chunks.length + 1} hoÃ n thÃ nh: ${currentChunk.length} records, ${currentTokens} tokens`)
      chunks.push([...currentChunk])
      currentChunk = [record]
      currentTokens = recordTokens
    } else {
      currentChunk.push(record)
      currentTokens += recordTokens
    }
  }

  // ThÃªm chunk cuá»‘i cÃ¹ng náº¿u cÃ³
  if (currentChunk.length > 0) {
    console.log(`ğŸ“Š Chunk cuá»‘i ${chunks.length + 1}: ${currentChunk.length} records, ${currentTokens} tokens`)
    chunks.push(currentChunk)
  }

  console.log(`ğŸ“Š Káº¿t quáº£ chia: ${chunks.length} chunks tá»« ${data.length} records`)

  // Náº¿u chá»‰ cÃ³ 1 chunk vÃ  quÃ¡ lá»›n, thá»­ chia nhá» hÆ¡n
  if (chunks.length === 1) {
    const singleChunkTokens = estimateTokens(JSON.stringify(chunks[0], null, 1))
    if (singleChunkTokens > 10000) {
      console.log(`âš ï¸ Single chunk quÃ¡ lá»›n (${singleChunkTokens} tokens), thá»­ chia nhá» hÆ¡n...`)
      return chunkDataByTokens(data, Math.floor(maxTokensPerChunk / 2)) // Chia Ä‘Ã´i
    }
  }

  return chunks
}

const createGroqClient = (apiKey: string): Groq => {
  return new Groq({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
  })
}

// Helper function Ä‘á»ƒ phÃ¢n tÃ­ch vá»›i 1 key - di chuyá»ƒn lÃªn trÆ°á»›c
const analyzeWithSingleKey = async (apiKey: string, keyIndex: number, prompt: string): Promise<string> => {
  try {
    const promptTokens = estimateTokens(prompt)
    console.log(`ğŸ¤– FINAL ANALYSIS vá»›i key ${keyIndex + 1}:`)
    console.log(`  ğŸ¯ Analysis INPUT tokens: ${promptTokens}`)
    console.log(`  âš¡ Model: llama-3.3-70b-versatile (v25)`)

    const groq = createGroqClient(apiKey)

    const startTime = Date.now()
    const completion = (await Promise.race([
      groq.chat.completions.create({
        model: "llama-3.3-70b-versatile", // Chá»‰ dÃ¹ng v25
        messages: [
          {
            role: "user",
            content: prompt,
          },
        ],
        temperature: 0.7,
        max_tokens: 25000,
      }),
      new Promise((_, reject) => setTimeout(() => reject(new Error("Timeout 90s")), 90000)),
    ])) as any

    const responseTime = Date.now() - startTime

    if (!completion?.choices?.[0]?.message?.content) {
      console.log(`âš ï¸ No response content from v25`)
      throw new Error("No response content")
    }

    const analysis = completion.choices[0].message.content || "KhÃ´ng cÃ³ phÃ¢n tÃ­ch"
    const outputTokens = estimateTokens(analysis)

    console.log(`âœ… ANALYSIS COMPLETE:`)
    console.log(`  ğŸ¯ OUTPUT tokens: ${outputTokens}`)
    console.log(`  âš¡ Processing time: ${responseTime}ms`)
    console.log(`  ğŸ“Š Token efficiency: ${Math.round((outputTokens / promptTokens) * 100)}%`)

    return analysis
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ Analysis failed with v25: ${errorMsg}`)
    return `âŒ KhÃ´ng thá»ƒ phÃ¢n tÃ­ch vá»›i v25: ${errorMsg}`
  }
}

// ThÃªm detailed error logging trong optimizeDataChunk
const optimizeDataChunk = async (
  apiKey: string,
  keyIndex: number,
  dataChunk: any[],
  chunkIndex: number,
  totalChunks: number,
): Promise<{ success: boolean; optimizedData: string; keyIndex: number; error?: string }> => {
  try {
    const chunkText = JSON.stringify(dataChunk, null, 2)
    const estimatedTokens = estimateTokens(chunkText)

    // ğŸ”¥ THÃŠM: Log chi tiáº¿t tokens ngay tá»« Ä‘áº§u
    console.log(`\nğŸ”§ ===== CHUNK ${chunkIndex + 1}/${totalChunks} TOKEN ANALYSIS =====`)
    console.log(`ğŸ“Š Key ${keyIndex + 1} Ä‘ang xá»­ lÃ½:`)
    console.log(`  ğŸ“ Records: ${dataChunk.length}`)
    console.log(`  ğŸ“„ Characters: ${chunkText.length}`)
    console.log(`  ğŸ¯ INPUT TOKENS: ${estimatedTokens}`)
    console.log(`  ğŸ“ˆ Token/record: ${Math.round(estimatedTokens / dataChunk.length)}`)
    console.log(`  ğŸ” Sample record size: ${JSON.stringify(dataChunk[0], null, 1).length} chars`)

    // Kiá»ƒm tra chunk size trÆ°á»›c khi gá»­i
    if (estimatedTokens > 8000) {
      console.log(`âŒ CHUNK QUÃ Lá»šN: ${estimatedTokens} tokens > 8000 limit`)
      return {
        success: false,
        optimizedData: "",
        keyIndex: keyIndex,
        error: `Chunk quÃ¡ lá»›n: ${estimatedTokens} tokens > 8000 limit`,
      }
    }

    const groq = createGroqClient(apiKey)

    // Prompt ngáº¯n gá»n hÆ¡n
    const optimizePrompt = `Optimize JSON - remove nulls, compact format:
${chunkText}
Return JSON only:`

    const promptTokens = estimateTokens(optimizePrompt)
    console.log(`ğŸ“¤ SENDING REQUEST:`)
    console.log(`  ğŸ¯ Total INPUT tokens: ${promptTokens} (prompt + data)`)
    console.log(`  âš¡ Model: llama-3.3-70b-versatile (v25)`)
    console.log(`  ğŸ”„ Max output tokens: 6000`)

    try {
      const startTime = Date.now()
      const completion = (await Promise.race([
        groq.chat.completions.create({
          model: "llama-3.3-70b-versatile", // Chá»‰ dÃ¹ng v25
          messages: [{ role: "user", content: optimizePrompt }],
          temperature: 0.1,
          max_tokens: 6000,
        }),
        new Promise((_, reject) => setTimeout(() => reject(new Error("Timeout 45s")), 45000)),
      ])) as any

      const responseTime = Date.now() - startTime
      console.log(`ğŸ“¥ RESPONSE RECEIVED (${responseTime}ms):`)

      if (!completion?.choices?.[0]?.message?.content) {
        throw new Error("Empty response from API")
      }

      const optimizedData = completion.choices[0].message.content.trim()
      const outputTokens = estimateTokens(optimizedData)
      const compressionRatio = Math.round((outputTokens / estimatedTokens) * 100)
      const tokensSaved = estimatedTokens - outputTokens

      console.log(`ğŸ“Š OUTPUT ANALYSIS:`)
      console.log(`  ğŸ“„ Response chars: ${optimizedData.length}`)
      console.log(`  ğŸ¯ OUTPUT TOKENS: ${outputTokens}`)
      console.log(`  ğŸ“‰ Compression: ${compressionRatio}% (${tokensSaved} tokens saved)`)
      console.log(`  âš¡ Processing time: ${responseTime}ms`)

      // Validate JSON
      try {
        const parsed = JSON.parse(optimizedData)
        const itemCount = Array.isArray(parsed) ? parsed.length : 1
        console.log(`âœ… VALIDATION SUCCESS:`)
        console.log(`  ğŸ“Š Valid JSON with ${itemCount} items`)
        console.log(`  ğŸ¯ TOKEN FLOW: ${estimatedTokens} â†’ ${outputTokens} (${compressionRatio}%)`)
        console.log(`===== END CHUNK ${chunkIndex + 1} =====\n`)

        return {
          success: true,
          optimizedData: optimizedData,
          keyIndex: keyIndex,
        }
      } catch (jsonError) {
        console.log(`âŒ JSON VALIDATION FAILED:`)
        console.log(`  ğŸ” Response preview: ${optimizedData.substring(0, 200)}...`)
        throw new Error(`Invalid JSON: ${jsonError}`)
      }
    } catch (apiError) {
      const errorMsg = apiError instanceof Error ? apiError.message : String(apiError)
      console.log(`âŒ API ERROR:`)
      console.log(`  ğŸš« Error: ${errorMsg}`)
      console.log(`  ğŸ¯ INPUT tokens attempted: ${estimatedTokens}`)

      // Log chi tiáº¿t lá»—i API
      if (errorMsg.includes("rate_limit")) {
        console.log(`  â° Rate limit exceeded for key ${keyIndex + 1}`)
      } else if (errorMsg.includes("quota")) {
        console.log(`  ğŸ’° Quota exceeded for key ${keyIndex + 1}`)
      } else if (errorMsg.includes("timeout")) {
        console.log(`  â±ï¸ Request timeout (45s) for key ${keyIndex + 1}`)
      } else {
        console.log(`  ğŸ” Unknown error for key ${keyIndex + 1}`)
      }

      throw new Error(errorMsg)
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ CHUNK ${chunkIndex + 1} FAILED: ${errorMsg}`)

    return {
      success: false,
      optimizedData: "",
      keyIndex: keyIndex,
      error: errorMsg,
    }
  }
}

// Function debug optimize process chi tiáº¿t
const debugOptimizeProcess = async (chunk: any[], keyIndex: number): Promise<void> => {
  try {
    const chunkText = JSON.stringify(chunk, null, 1)
    const estimatedTokens = estimateTokens(chunkText)

    console.log(`ğŸ” DEBUG Chunk ${keyIndex + 1}:`)
    console.log(`  - Records: ${chunk.length}`)
    console.log(`  - Characters: ${chunkText.length}`)
    console.log(`  - Estimated tokens: ${estimatedTokens}`)
    console.log(`  - Sample record:`, JSON.stringify(chunk[0], null, 1).substring(0, 200) + "...")

    // Test API key trÆ°á»›c
    const apiKey = API_KEYS[keyIndex]
    const groq = createGroqClient(apiKey)

    console.log(`ğŸ§ª Testing key ${keyIndex + 1} vá»›i simple request...`)
    const testResult = await groq.chat.completions.create({
      model: "llama-3.3-70b-versatile",
      messages: [{ role: "user", content: "Say 'test ok'" }],
      temperature: 0.1,
      max_tokens: 10,
    })

    console.log(`âœ… Key ${keyIndex + 1} test result:`, testResult?.choices?.[0]?.message?.content)
  } catch (error) {
    console.error(`âŒ DEBUG failed for key ${keyIndex + 1}:`, error)
  }
}

// Function Ä‘á»ƒ optimize/compress dá»¯ liá»‡u vá»›i 1 API key
// Function chÃ­nh: Data Preprocessing Pipeline
export const preprocessDataWithPipeline = async (
  data: any[],
  tableName: string,
): Promise<{ success: boolean; optimizedData: string; analysis: string; keyUsage: any }> => {
  try {
    console.log(`ğŸš€ Báº¯t Ä‘áº§u Data Preprocessing Pipeline vá»›i ${data.length} records`)

    if (!API_KEYS || API_KEYS.length === 0) {
      throw new Error("KhÃ´ng cÃ³ API keys há»£p lá»‡")
    }

    // Test API keys trÆ°á»›c
    console.log(`ğŸ§ª Test API keys trÆ°á»›c khi báº¯t Ä‘áº§u...`)
    const keyTests = await Promise.all(API_KEYS.slice(0, 3).map((key, index) => testSingleChunk([data[0]], index)))
    const workingKeys = keyTests.filter(Boolean).length
    console.log(`ğŸ”‘ ${workingKeys}/${keyTests.length} keys hoáº¡t Ä‘á»™ng`)

    if (workingKeys === 0) {
      throw new Error("KhÃ´ng cÃ³ API keys nÃ o hoáº¡t Ä‘á»™ng")
    }

    // ThÃªm sau pháº§n test API keys
    console.log(`ğŸ§ª Test optimize functionality...`)
    const optimizeTests = await Promise.all(API_KEYS.slice(0, 3).map((key, index) => testOptimizeSimple(index)))
    const workingOptimizeKeys = optimizeTests.filter(Boolean).length
    console.log(`ğŸ”§ ${workingOptimizeKeys}/${optimizeTests.length} keys cÃ³ thá»ƒ optimize`)

    if (workingOptimizeKeys === 0) {
      console.log(`âš ï¸ KhÃ´ng cÃ³ key nÃ o cÃ³ thá»ƒ optimize, chuyá»ƒn sang raw data mode`)

      const rawData = JSON.stringify(data.slice(0, 30), null, 1)
      return {
        success: true,
        optimizedData: rawData,
        analysis: `âš ï¸ KhÃ´ng thá»ƒ optimize dá»¯ liá»‡u (táº¥t cáº£ keys tháº¥t báº¡i), sá»­ dá»¥ng ${Math.min(30, data.length)} records Ä‘áº§u tiÃªn tá»« tá»•ng ${data.length} records.`,
        keyUsage: {
          totalKeys: API_KEYS.length,
          optimizeKeys: 0,
          analysisKey: 1,
          failedKeys: API_KEYS.length - 1,
          successRate: "0%",
          chunks: 0,
          successfulChunks: 0,
          finalDataSize: rawData.length,
          fallback: true,
        },
      }
    }

    // ThÃªm adaptive chunking
    console.log(`ğŸ“Š BÆ¯á»šC 1: Adaptive chunking dá»±a trÃªn working keys (${workingKeys} keys)`)
    const chunkSize = workingKeys >= 3 ? 4000 : 3000 // Giáº£m tá»« 5000 xuá»‘ng 4000
    console.log(`ğŸ“Š Sá»­ dá»¥ng chunk size: ${chunkSize} tokens`)

    // BÆ¯á»šC 1: Chia dá»¯ liá»‡u thÃ nh chunks theo token limit
    console.log(`ğŸ“Š BÆ¯á»šC 1: Chia dá»¯ liá»‡u theo token limit (4000 tokens/chunk)`)
    let chunks = chunkDataByTokens(data, chunkSize)

    // Náº¿u váº«n chá»‰ cÃ³ 1 chunk lá»›n, thá»­ strategy khÃ¡c
    if (chunks.length === 1) {
      const singleChunkTokens = estimateTokens(JSON.stringify(chunks[0], null, 1))
      console.log(`âš ï¸ Chá»‰ cÃ³ 1 chunk vá»›i ${singleChunkTokens} tokens`)

      if (singleChunkTokens > 10000) {
        console.log(`ğŸ”„ Fallback: Chia theo sá»‘ records thay vÃ¬ tokens`)
        // Chia theo sá»‘ records vá»›i chunks nhá» hÆ¡n
        const recordsPerChunk = Math.max(Math.ceil(data.length / (API_KEYS.length - 1)), 3) // Tá»‘i thiá»ƒu 3 records/chunk
        chunks = []
        for (let i = 0; i < data.length; i += recordsPerChunk) {
          chunks.push(data.slice(i, i + recordsPerChunk))
        }
        console.log(`ğŸ“Š Fallback result: ${chunks.length} chunks vá»›i ~${recordsPerChunk} records/chunk`)
      }
    }

    // Log thÃ´ng tin chi tiáº¿t vá» chunks
    let totalInputTokens = 0
    chunks.forEach((chunk, index) => {
      const chunkText = JSON.stringify(chunk, null, 1)
      const estimatedTokens = estimateTokens(chunkText)
      totalInputTokens += estimatedTokens
      console.log(`ğŸ“Š Chunk ${index + 1}: ${chunk.length} records, ~${estimatedTokens} tokens`)
    })

    console.log(`ğŸ“Š TOKEN DISTRIBUTION SUMMARY:`)
    console.log(`  ğŸ¯ Total INPUT tokens: ${totalInputTokens}`)
    console.log(`  ğŸ“Š Chunks: ${chunks.length}`)
    console.log(`  ğŸ“ˆ Avg tokens/chunk: ${Math.round(totalInputTokens / chunks.length)}`)
    console.log(`  ğŸ“‹ Avg tokens/record: ${Math.round(totalInputTokens / data.length)}`)
    console.log(`  âš¡ Model: llama-3.3-70b-versatile (v25) ONLY`)
    console.log(`  ğŸ”§ Max tokens per request: 8000 (safety limit)`)

    // BÆ¯á»šC 2: Optimize tá»«ng chunk vá»›i better error handling
    console.log(`â³ BÆ¯á»šC 2: Äang optimize ${chunks.length} chunks...`)

    const optimizeResults = []

    // Xá»­ lÃ½ tá»«ng chunk má»™t Ä‘á»ƒ debug tá»‘t hÆ¡n
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i]
      const keyIndex = i % (API_KEYS.length - 1)

      console.log(`ğŸ”§ Xá»­ lÃ½ chunk ${i + 1}/${chunks.length} vá»›i key ${keyIndex + 1}`)

      // Debug trÆ°á»›c khi optimize
      await debugOptimizeProcess(chunk, keyIndex)

      let result = null
      let retryCount = 0
      const maxRetries = 2

      // Retry mechanism
      while (retryCount <= maxRetries && (!result || !result.success)) {
        try {
          if (retryCount > 0) {
            console.log(`ğŸ”„ Retry ${retryCount}/${maxRetries} cho chunk ${i + 1}`)
            await new Promise((resolve) => setTimeout(resolve, 2000)) // Wait 2s
          }

          result = await optimizeDataChunk(API_KEYS[keyIndex], keyIndex, chunk, i, chunks.length)

          if (result && result.success) {
            console.log(`âœ… Chunk ${i + 1}: ThÃ nh cÃ´ng sau ${retryCount} retries`)
            break
          } else {
            console.log(`âŒ Chunk ${i + 1}: Tháº¥t báº¡i láº§n ${retryCount + 1} - ${result?.error || "Unknown error"}`)
          }
        } catch (error) {
          console.log(`âŒ Chunk ${i + 1}: Exception láº§n ${retryCount + 1} - ${error}`)
          result = {
            success: false,
            optimizedData: "",
            keyIndex: keyIndex,
            error: String(error),
          }
        }

        retryCount++
      }

      // Äáº£m báº£o result khÃ´ng null trÆ°á»›c khi push
      if (result) {
        optimizeResults.push(result)
      } else {
        optimizeResults.push({
          success: false,
          optimizedData: "",
          keyIndex: keyIndex,
          error: "No result after retries",
        })
      }
    }

    // Debug chi tiáº¿t káº¿t quáº£
    console.log(`ğŸ” DEBUG: Optimize results details:`)
    optimizeResults.forEach((result, index) => {
      if (result.success) {
        console.log(`âœ… Chunk ${index + 1}: Success (Key ${result.keyIndex + 1})`)
      } else {
        console.log(`âŒ Chunk ${index + 1}: Failed (Key ${result.keyIndex + 1}) - ${result.error}`)
      }
    })

    // Kiá»ƒm tra káº¿t quáº£ optimize
    const successfulOptimizes = optimizeResults.filter((r) => r && r.success)
    const failedOptimizes = optimizeResults.filter((r) => r && !r.success)

    console.log(`ğŸ“Š Optimize results: ${successfulOptimizes.length}/${optimizeResults.length} thÃ nh cÃ´ng`)

    // Emergency fallback náº¿u success rate quÃ¡ tháº¥p
    if (successfulOptimizes.length > 0 && successfulOptimizes.length < optimizeResults.length * 0.3) {
      console.log(
        `âš ï¸ Success rate tháº¥p (${Math.round((successfulOptimizes.length / optimizeResults.length) * 100)}%), thá»­ vá»›i chunks nhá» hÆ¡n`,
      )

      // Thá»­ láº¡i vá»›i chunks 2K tokens
      console.log(`ğŸ”„ Emergency retry vá»›i 2K tokens chunks...`)
      const smallChunks = chunkDataByTokens(data, 2000)

      if (smallChunks.length > chunks.length) {
        console.log(`ğŸ“Š Táº¡o ${smallChunks.length} chunks nhá» hÆ¡n, thá»­ optimize 3 chunks Ä‘áº§u...`)

        for (let i = 0; i < Math.min(3, smallChunks.length); i++) {
          const smallChunk = smallChunks[i]
          const keyIndex = i % (API_KEYS.length - 1)

          try {
            const smallResult = await optimizeDataChunk(API_KEYS[keyIndex], keyIndex, smallChunk, i, 3)
            if (smallResult.success) {
              console.log(`âœ… Emergency chunk ${i + 1}: ThÃ nh cÃ´ng`)
              successfulOptimizes.push(smallResult)
            }
          } catch (error) {
            console.log(`âŒ Emergency chunk ${i + 1}: ${error}`)
          }

          await new Promise((resolve) => setTimeout(resolve, 1000))
        }
      }
    }

    // Náº¿u táº¥t cáº£ tháº¥t báº¡i, thá»­ fallback vá»›i raw data
    if (successfulOptimizes.length === 0) {
      console.log(`ğŸ”„ FALLBACK: Táº¥t cáº£ optimize tháº¥t báº¡i, sá»­ dá»¥ng raw data`)

      // Sá»­ dá»¥ng raw data (rÃºt gá»n)
      const rawData = JSON.stringify(data.slice(0, 20), null, 1) // Chá»‰ láº¥y 20 records Ä‘áº§u

      const keyUsage = {
        totalKeys: API_KEYS.length,
        optimizeKeys: 0,
        analysisKey: 1,
        failedKeys: failedOptimizes.length,
        successRate: "0%",
        chunks: chunks.length,
        successfulChunks: 0,
        finalDataSize: rawData.length,
        fallback: true,
      }

      return {
        success: true,
        optimizedData: rawData,
        analysis: `âš ï¸ KhÃ´ng thá»ƒ optimize dá»¯ liá»‡u, sá»­ dá»¥ng ${data.slice(0, 20).length} records Ä‘áº§u tiÃªn tá»« tá»•ng ${data.length} records. Dá»¯ liá»‡u váº«n cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ­ch nhÆ°ng cÃ³ thá»ƒ khÃ´ng Ä‘áº§y Ä‘á»§.`,
        keyUsage: keyUsage,
      }
    }

    // BÆ¯á»šC 3: Gá»™p dá»¯ liá»‡u Ä‘Ã£ optimize
    let combinedOptimizedData = "["
    let validChunks = 0

    successfulOptimizes.forEach((result, index) => {
      // Parse vÃ  merge JSON arrays
      try {
        const parsedData = JSON.parse(result.optimizedData)
        const dataArray = Array.isArray(parsedData) ? parsedData : [parsedData]

        if (validChunks > 0) combinedOptimizedData += ","
        combinedOptimizedData += JSON.stringify(dataArray).slice(1, -1) // Remove [ ]
        validChunks++
      } catch (parseError) {
        console.warn(`âš ï¸ KhÃ´ng thá»ƒ parse optimize result tá»« key ${result.keyIndex + 1}:`, parseError)
      }
    })
    combinedOptimizedData += "]"

    console.log(`ğŸ“Š BÆ¯á»šC 3: ÄÃ£ gá»™p ${validChunks} chunks optimize (${combinedOptimizedData.length} characters)`)

    // BÆ¯á»šC 4: PhÃ¢n tÃ­ch tá»•ng há»£p vá»›i key cuá»‘i cÃ¹ng
    const finalKeyIndex = API_KEYS.length - 1
    const finalApiKey = API_KEYS[finalKeyIndex]

    if (!finalApiKey) {
      throw new Error("KhÃ´ng cÃ³ API key cho phÃ¢n tÃ­ch cuá»‘i")
    }

    console.log(`ğŸ¤– BÆ¯á»šC 4: PhÃ¢n tÃ­ch tá»•ng há»£p vá»›i key ${finalKeyIndex + 1}`)

    const analysisPrompt = `Báº¡n lÃ  má»™t AI analyst chuyÃªn nghiá»‡p. DÆ°á»›i Ä‘Ã¢y lÃ  dá»¯ liá»‡u tá»« báº£ng "${tableName}" Ä‘Ã£ Ä‘Æ°á»£c optimize (${data.length} records gá»‘c, ${validChunks}/${chunks.length} chunks thÃ nh cÃ´ng):

${combinedOptimizedData}

ÄÃ¢y lÃ  dá»¯ liá»‡u tá»« ${data.length} báº£n ghi gá»‘c, Ä‘Ã£ Ä‘Æ°á»£c optimize Ä‘á»ƒ giáº£m token nhÆ°ng váº«n giá»¯ nguyÃªn thÃ´ng tin quan trá»ng.

HÃ£y phÃ¢n tÃ­ch chi tiáº¿t:
1. ğŸ“Š Tá»•ng quan vá» dá»¯ liá»‡u
2. ğŸ“ˆ Thá»‘ng kÃª quan trá»ng  
3. ğŸ” Patterns vÃ  insights
4. ğŸ’¡ Nháº­n xÃ©t vÃ  Ä‘Ã¡nh giÃ¡

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chi tiáº¿t vÃ  cÃ³ cáº¥u trÃºc.`

    const finalAnalysis = await analyzeWithSingleKey(finalApiKey, finalKeyIndex, analysisPrompt)

    const keyUsage = {
      totalKeys: API_KEYS.length,
      optimizeKeys: successfulOptimizes.length,
      analysisKey: 1,
      failedKeys: failedOptimizes.length,
      successRate: `${Math.round((successfulOptimizes.length / optimizeResults.length) * 100)}%`,
      chunks: chunks.length,
      successfulChunks: validChunks,
      finalDataSize: combinedOptimizedData.length,
    }

    return {
      success: true,
      optimizedData: combinedOptimizedData,
      analysis: finalAnalysis,
      keyUsage: keyUsage,
    }
  } catch (error) {
    console.error("âŒ Data Preprocessing Pipeline failed:", error)
    return {
      success: false,
      optimizedData: "",
      analysis: `âŒ Lá»—i preprocessing pipeline: ${error}`,
      keyUsage: { error: true },
    }
  }
}

// Function tráº£ lá»i cÃ¢u há»i vá»›i dá»¯ liá»‡u Ä‘Ã£ optimize
export const answerQuestionWithOptimizedData = async (
  optimizedData: string,
  tableName: string,
  question: string,
  originalRecordCount: number,
): Promise<string> => {
  try {
    console.log(`ğŸ¤” Tráº£ lá»i cÃ¢u há»i vá»›i optimized data (${originalRecordCount} records)`)

    // Sá»­ dá»¥ng key cuá»‘i cÃ¹ng Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i
    const finalKeyIndex = API_KEYS.length - 1
    const finalApiKey = API_KEYS[finalKeyIndex]

    if (!finalApiKey) {
      throw new Error("KhÃ´ng cÃ³ API key Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i")
    }

    const questionPrompt = `Báº¡n lÃ  má»™t AI assistant thÃ´ng minh. DÆ°á»›i Ä‘Ã¢y lÃ  TOÃ€N Bá»˜ dá»¯ liá»‡u tá»« báº£ng "${tableName}" (${originalRecordCount} records Ä‘Ã£ Ä‘Æ°á»£c optimize):

${optimizedData}

ÄÃ¢y lÃ  dá»¯ liá»‡u HOÃ€N CHá»ˆNH tá»« ${originalRecordCount} báº£n ghi. HÃ£y dá»±a vÃ o dá»¯ liá»‡u nÃ y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  chi tiáº¿t.

CÃ¢u há»i: ${question}

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t:`

    return await analyzeWithSingleKey(finalApiKey, finalKeyIndex, questionPrompt)
  } catch (error) {
    console.error("âŒ answerQuestionWithOptimizedData failed:", error)
    return `âŒ Lá»—i khi tráº£ lá»i cÃ¢u há»i: ${error}`
  }
}

// Export function chÃ­nh thay tháº¿ analyzeDataWithParallelKeys
export const analyzeDataWithParallelKeys = preprocessDataWithPipeline

// Function tráº£ lá»i cÃ¢u há»i - sá»­ dá»¥ng optimized data
export const answerQuestionWithData = async (
  data: any[],
  tableName: string,
  question: string,
  previousAnalysis?: string,
  optimizedData?: string,
): Promise<string> => {
  try {
    if (optimizedData && optimizedData.length > 0) {
      // Náº¿u cÃ³ optimized data, sá»­ dá»¥ng nÃ³
      return await answerQuestionWithOptimizedData(optimizedData, tableName, question, data.length)
    } else {
      // Fallback: táº¡o optimized data nhanh
      const quickOptimized = JSON.stringify(data.slice(0, 30), null, 1) // 30 records Ä‘áº§u
      return await answerQuestionWithOptimizedData(quickOptimized, tableName, question, data.length)
    }
  } catch (error) {
    console.error("âŒ answerQuestionWithData failed:", error)
    return `âŒ Lá»—i khi tráº£ lá»i cÃ¢u há»i: ${error}`
  }
}

// Test functions
export const testAllApiKeys = async (): Promise<{
  success: boolean
  message: string
  workingKeys: number
  totalKeys: number
  keyDetails: any[]
}> => {
  console.log(`ğŸ§ª Testing ${API_KEYS.length} API keys...`)

  const testPromises = API_KEYS.map(async (apiKey, index) => {
    try {
      const groq = createGroqClient(apiKey)

      const testCompletion = await groq.chat.completions.create({
        model: "llama-3.3-70b-versatile", // Chá»‰ test model nÃ y
        messages: [
          {
            role: "user",
            content: "Test: 1+1=?",
          },
        ],
        temperature: 0.1,
        max_tokens: 50, // Nhá» cho test
      })

      // ThÃªm null checks
      const response = testCompletion?.choices?.[0]?.message?.content || "No response"
      console.log(`âœ… Key ${index + 1}: OK`)

      return {
        keyIndex: index + 1,
        status: "success" as const,
        response: response,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
      }
    } catch (error) {
      console.log(`âŒ Key ${index + 1}: ${error}`)
      return {
        keyIndex: index + 1,
        status: "failed" as const,
        error: error instanceof Error ? error.message : String(error),
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
      }
    }
  })

  const results = await Promise.all(testPromises)
  const workingKeys = results.filter((r) => r.status === "success").length

  return {
    success: workingKeys > 0,
    message: `${workingKeys}/${API_KEYS.length} API keys hoáº¡t Ä‘á»™ng`,
    workingKeys: workingKeys,
    totalKeys: API_KEYS.length,
    keyDetails: results,
  }
}

// Backward compatibility
export const askAI = async (context: string, question: string): Promise<string> => {
  return await answerQuestionWithData([], "Unknown", question)
}

export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const testGroqAPI = async () => {
  const result = await testAllApiKeys()
  return {
    success: result.success,
    message: result.message,
    workingModel: "pipeline",
  }
}

export const getAvailableModels = (): string[] => {
  return AVAILABLE_MODELS
}

export const getApiKeysInfo = () => {
  return {
    totalKeys: API_KEYS.length,
    keysPreview: API_KEYS.map(
      (key, index) => `Key ${index + 1}: ${key.substring(0, 10)}...${key.substring(key.length - 4)}`,
    ),
  }
}
