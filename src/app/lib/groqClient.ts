import { Groq } from "groq-sdk"

// Danh s√°ch c√°c API keys (th√™m nhi·ªÅu keys v√†o ƒë√¢y)
const API_KEYS = [
  process.env.NEXT_PUBLIC_GROQ_API_KEY_1 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_2 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_3 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_4 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_5 || "ygsk_QrymOUGpWhGISmUzmJM4WGdyb3FY9kePDDtFoUCCA6gLBZllrAeD",
].filter(
  (key) =>
    key &&
    key !== "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" &&
    key !== "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" &&
    key !== "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" &&
    key !== "ygsk_QrymOUGpWhGISmUzmJM4WGdyb3FY9kePDDtFoUCCA6gLBZllrAeD",
) // L·ªçc b·ªè keys tr·ªëng

// Danh s√°ch c√°c model kh·∫£ d·ª•ng (theo th·ª© t·ª± ∆∞u ti√™n)
const AVAILABLE_MODELS = [
  "llama-3.3-70b-versatile", // Model c√≥ context length l·ªõn nh·∫•t
  "llama-3.1-8b-instant",
  "llama3-70b-8192",
  "llama3-8b-8192",
  "mixtral-8x7b-32768",
  "gemma-7b-it",
]

// Bi·∫øn ƒë·ªÉ track key rotation
let currentKeyIndex = 0
let requestCount = 0

// Function ƒë·ªÉ l·∫•y API key ti·∫øp theo
const getNextApiKey = (): string => {
  const key = API_KEYS[currentKeyIndex]
  currentKeyIndex = (currentKeyIndex + 1) % API_KEYS.length
  requestCount++

  console.log(`üîë Using API key ${currentKeyIndex + 1}/${API_KEYS.length} (Request #${requestCount})`)
  return key
}

// Function t·∫°o Groq client v·ªõi key c·ª• th·ªÉ
const createGroqClient = (apiKey: string): Groq => {
  return new Groq({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
  })
}

// Function th·ª≠ c√°c model kh√°c nhau v·ªõi key rotation
const tryWithDifferentModels = async (
  messages: any[],
  currentModelIndex = 0,
  retryWithNewKey = false,
): Promise<string> => {
  if (currentModelIndex >= AVAILABLE_MODELS.length) {
    throw new Error("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")
  }

  const model = AVAILABLE_MODELS[currentModelIndex]
  const apiKey = getNextApiKey()
  const groq = createGroqClient(apiKey)

  console.log(`ü§ñ Th·ª≠ model: ${model} v·ªõi API key ${currentKeyIndex}/${API_KEYS.length}`)

  try {
    const chatCompletion = await groq.chat.completions.create({
      model: model,
      messages: messages,
      temperature: 0.7,
      max_tokens: 12000, // TƒÉng max_tokens nh∆∞ b·∫°n y√™u c·∫ßu
      top_p: 1,
    })

    const response = chatCompletion.choices[0].message.content
    console.log(`‚úÖ Model ${model} v·ªõi key ${currentKeyIndex}/${API_KEYS.length} ho·∫°t ƒë·ªông th√†nh c√¥ng`)
    return response || "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ AI."
  } catch (error) {
    console.log(`‚ùå Model ${model} v·ªõi key ${currentKeyIndex}/${API_KEYS.length} th·∫•t b·∫°i:`, error)

    // N·∫øu l·ªói rate limit v√† c√≤n keys kh√°c, th·ª≠ key kh√°c
    if (error instanceof Error && error.message.includes("rate_limit") && API_KEYS.length > 1 && !retryWithNewKey) {
      console.log(`üîÑ Rate limit v·ªõi key hi·ªán t·∫°i, th·ª≠ key kh√°c...`)
      return await tryWithDifferentModels(messages, currentModelIndex, true)
    }

    // N·∫øu model b·ªã decommission ho·∫∑c kh√¥ng kh·∫£ d·ª•ng, th·ª≠ model ti·∫øp theo
    if (
      error instanceof Error &&
      (error.message.includes("decommissioned") ||
        error.message.includes("not found") ||
        error.message.includes("invalid_request_error"))
    ) {
      console.log(`üîÑ Th·ª≠ model ti·∫øp theo...`)
      return await tryWithDifferentModels(messages, currentModelIndex + 1)
    }

    // N·∫øu l√† l·ªói kh√°c, throw ngay
    throw error
  }
}

// Function t·ªëi ∆∞u d·ªØ li·ªáu cho AI - gi·ªØ nguy√™n t·∫•t c·∫£ d·ªØ li·ªáu
const optimizeDataForAI = (data: any[]): string => {
  // Lo·∫°i b·ªè c√°c field null/empty ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc nh∆∞ng gi·ªØ nguy√™n c·∫•u tr√∫c
  const optimizedData = data.map((record) => {
    const optimizedFields: Record<string, any> = {}

    // Ch·ªâ gi·ªØ l·∫°i fields c√≥ gi√° tr·ªã
    for (const [key, value] of Object.entries(record.fields)) {
      if (value !== null && value !== undefined && value !== "") {
        optimizedFields[key] = value
      }
    }

    return {
      recordId: record.recordId,
      fields: optimizedFields,
    }
  })

  return JSON.stringify(optimizedData, null, 1) // D√πng indent = 1 ƒë·ªÉ ti·∫øt ki·ªám space
}

// Function chia d·ªØ li·ªáu th√†nh chunks v√† g·ª≠i ri√™ng bi·ªát v·ªõi key rotation
export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  try {
    console.log(`ü§ñ X·ª≠ l√Ω ${data.length} records v·ªõi full data approach v√† ${API_KEYS.length} API keys`)

    // Th·ª≠ g·ª≠i to√†n b·ªô d·ªØ li·ªáu ƒë√£ t·ªëi ∆∞u tr∆∞·ªõc
    const optimizedDataString = optimizeDataForAI(data)
    console.log(`üìä Optimized data length: ${optimizedDataString.length} characters`)

    // N·∫øu d·ªØ li·ªáu v·∫´n nh·ªè, g·ª≠i to√†n b·ªô
    if (optimizedDataString.length < 40000) {
      // TƒÉng threshold do c√≥ nhi·ªÅu keys
      console.log("üì§ G·ª≠i to√†n b·ªô d·ªØ li·ªáu optimized...")

      const context = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† TO√ÄN B·ªò ${data.length} records t·ª´ b·∫£ng "${tableName}" trong Lark Base:

${optimizedDataString}

ƒê√¢y l√† t·∫•t c·∫£ ${data.length} b·∫£n ghi ƒë·∫ßy ƒë·ªß, kh√¥ng b·ªã c·∫Øt b·ªõt hay t√≥m t·∫Øt.

H√£y ph√¢n t√≠ch d·ªØ li·ªáu n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

      return await askAI(context, question)
    }

    // N·∫øu d·ªØ li·ªáu l·ªõn, chia th√†nh chunks
    const chunkSize = 20 // TƒÉng chunk size do c√≥ nhi·ªÅu keys
    const chunks = []
    for (let i = 0; i < data.length; i += chunkSize) {
      chunks.push(data.slice(i, i + chunkSize))
    }

    console.log(`üìä Chia d·ªØ li·ªáu th√†nh ${chunks.length} chunks v·ªõi ${API_KEYS.length} API keys`)

    // G·ª≠i t·ª´ng chunk v√† t·ªïng h·ª£p k·∫øt qu·∫£ (m·ªói chunk d√πng key kh√°c nhau)
    let combinedAnalysis = `T√¥i ƒë√£ ph√¢n t√≠ch to√†n b·ªô ${data.length} b·∫£n ghi ƒë∆∞·ª£c chia th√†nh ${chunks.length} ph·∫ßn v·ªõi ${API_KEYS.length} API keys:\n\n`

    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i]
      const chunkDataString = JSON.stringify(
        chunk.map((record) => ({
          recordId: record.recordId,
          fields: record.fields,
        })),
        null,
        1,
      )

      console.log(`üì§ X·ª≠ l√Ω chunk ${i + 1}/${chunks.length} (${chunk.length} records) v·ªõi key rotation`)

      const chunkContext = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† ph·∫ßn ${i + 1}/${chunks.length} c·ªßa d·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}":

${chunkDataString}

ƒê√¢y l√† ${chunk.length} b·∫£n ghi (t·ª´ ${i * chunkSize + 1} ƒë·∫øn ${Math.min((i + 1) * chunkSize, data.length)}) trong t·ªïng s·ªë ${data.length} b·∫£n ghi.

H√£y ph√¢n t√≠ch ph·∫ßn d·ªØ li·ªáu n√†y v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ng·∫Øn g·ªçn. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

      try {
        const chunkResult = await askAI(chunkContext, `Ph√¢n t√≠ch ph·∫ßn ${i + 1}: ${question}`)
        combinedAnalysis += `**Ph·∫ßn ${i + 1} (Records ${i * chunkSize + 1}-${Math.min((i + 1) * chunkSize, data.length)}):**\n${chunkResult}\n\n`
      } catch (error) {
        console.error(`‚ùå L·ªói x·ª≠ l√Ω chunk ${i + 1}:`, error)
        combinedAnalysis += `**Ph·∫ßn ${i + 1}:** L·ªói x·ª≠ l√Ω - ${error}\n\n`
      }

      // Delay ng·∫Øn h∆°n do c√≥ nhi·ªÅu keys
      await new Promise((resolve) => setTimeout(resolve, 500))
    }

    // G·ª≠i c√¢u h·ªèi t·ªïng h·ª£p cu·ªëi c√πng
    const finalContext = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. T√¥i ƒë√£ ph√¢n t√≠ch to√†n b·ªô ${data.length} b·∫£n ghi t·ª´ b·∫£ng "${tableName}" theo t·ª´ng ph·∫ßn v·ªõi ${API_KEYS.length} API keys. D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ ph√¢n t√≠ch:

${combinedAnalysis}

D·ª±a tr√™n t·∫•t c·∫£ th√¥ng tin tr√™n, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p v√† ƒë·∫ßy ƒë·ªß cho c√¢u h·ªèi. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

    return await askAI(finalContext, question)
  } catch (error) {
    console.error("‚ùå askAIWithFullData failed:", error)
    return `‚ùå L·ªói khi ph√¢n t√≠ch d·ªØ li·ªáu: ${error}`
  }
}

export const askAI = async (context: string, question: string): Promise<string> => {
  try {
    console.log("ü§ñ B·∫Øt ƒë·∫ßu g·ªçi Groq API...")
    console.log("üìù Context length:", context.length)
    console.log("‚ùì Question:", question)
    console.log(`üîë Available API keys: ${API_KEYS.length}`)

    const messages = [
      {
        role: "system" as const,
        content: context,
      },
      {
        role: "user" as const,
        content: question,
      },
    ]

    // Th·ª≠ v·ªõi c√°c model kh√°c nhau v√† key rotation
    const response = await tryWithDifferentModels(messages)
    console.log("‚úÖ Groq API response received")

    return response
  } catch (error) {
    console.error("‚ùå Chi ti·∫øt l·ªói Groq API:", error)

    // X·ª≠ l√Ω c√°c lo·∫°i l·ªói c·ª• th·ªÉ
    if (error instanceof Error) {
      if (error.message.includes("rate_limit")) {
        return `‚è∞ T·∫•t c·∫£ ${API_KEYS.length} API keys ƒë·ªÅu ƒë·∫°t gi·ªõi h·∫°n t·ªëc ƒë·ªô. Vui l√≤ng th·ª≠ l·∫°i sau v√†i gi√¢y.`
      } else if (error.message.includes("invalid_api_key")) {
        return "üîë M·ªôt ho·∫∑c nhi·ªÅu API keys kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh."
      } else if (error.message.includes("context_length")) {
        return "üìè D·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω. H√£y th·ª≠ v·ªõi c√¢u h·ªèi c·ª• th·ªÉ h∆°n ho·∫∑c chia nh·ªè d·ªØ li·ªáu."
      } else if (error.message.includes("network") || error.message.includes("fetch")) {
        return "üåê L·ªói k·∫øt n·ªëi m·∫°ng. Vui l√≤ng ki·ªÉm tra internet v√† th·ª≠ l·∫°i."
      } else if (error.message.includes("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")) {
        return "ü§ñ T·∫•t c·∫£ c√°c AI model hi·ªán t·∫°i ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau."
      }

      return `‚ùå L·ªói AI: ${error.message}`
    }

    return "‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi AI. Vui l√≤ng th·ª≠ l·∫°i."
  }
}

// Function test mode - g·ª≠i to√†n b·ªô d·ªØ li·ªáu raw v·ªõi key rotation
export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  try {
    console.log(`üß™ TEST MODE: G·ª≠i to√†n b·ªô ${data.length} records raw data v·ªõi ${API_KEYS.length} keys`)

    const rawDataString = JSON.stringify(data, null, 2)
    console.log(`üìä Raw data length: ${rawDataString.length} characters`)

    const context = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† TO√ÄN B·ªò ${data.length} records t·ª´ b·∫£ng "${tableName}" trong Lark Base (RAW DATA):

${rawDataString}

QUAN TR·ªåNG: ƒê√¢y l√† t·∫•t c·∫£ ${data.length} b·∫£n ghi ƒë·∫ßy ƒë·ªß, kh√¥ng b·ªã t√≥m t·∫Øt hay c·∫Øt b·ªõt. B·∫°n c√≥ th·ªÉ truy c·∫≠p v√† ph√¢n t√≠ch t·∫•t c·∫£ d·ªØ li·ªáu n√†y.

H√£y ph√¢n t√≠ch d·ªØ li·ªáu n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

    return await askAI(context, question)
  } catch (error) {
    console.error("‚ùå askAIWithRawData failed:", error)
    return `‚ùå L·ªói khi g·ª≠i raw data: ${error}`
  }
}

// Function test API v·ªõi key rotation
export const testGroqAPI = async (): Promise<{
  success: boolean
  message: string
  workingModel?: string
  workingKeys?: number
}> => {
  console.log(`üß™ Testing Groq API v·ªõi ${API_KEYS.length} API keys v√† ${AVAILABLE_MODELS.length} models...`)

  let workingKeys = 0
  let workingModel = ""

  for (const model of AVAILABLE_MODELS) {
    for (let keyIndex = 0; keyIndex < API_KEYS.length; keyIndex++) {
      try {
        const apiKey = API_KEYS[keyIndex]
        const groq = createGroqClient(apiKey)

        console.log(`üß™ Testing model: ${model} v·ªõi key ${keyIndex + 1}/${API_KEYS.length}`)

        const testCompletion = await groq.chat.completions.create({
          model: model,
          messages: [
            {
              role: "user",
              content: "Xin ch√†o! H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát: 1+1 b·∫±ng bao nhi√™u?",
            },
          ],
          temperature: 0.1,
          max_tokens: 50,
        })

        const response = testCompletion.choices[0].message.content
        console.log(`‚úÖ Model ${model} v·ªõi key ${keyIndex + 1} ho·∫°t ƒë·ªông! Response:`, response)

        workingKeys++
        if (!workingModel) workingModel = model
      } catch (error) {
        console.log(`‚ùå Model ${model} v·ªõi key ${keyIndex + 1} failed:`, error)
        continue
      }
    }
  }

  if (workingKeys > 0) {
    return {
      success: true,
      message: `${workingKeys}/${API_KEYS.length} API keys ho·∫°t ƒë·ªông v·ªõi model ${workingModel}`,
      workingModel: workingModel,
      workingKeys: workingKeys,
    }
  }

  return {
    success: false,
    message: `T·∫•t c·∫£ ${API_KEYS.length} API keys ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra keys ho·∫∑c th·ª≠ l·∫°i sau.`,
    workingKeys: 0,
  }
}

// Function ƒë·ªÉ l·∫•y th√¥ng tin v·ªÅ API keys
export const getApiKeysInfo = () => {
  return {
    totalKeys: API_KEYS.length,
    currentKeyIndex: currentKeyIndex,
    requestCount: requestCount,
    keysPreview: API_KEYS.map(
      (key, index) => `Key ${index + 1}: ${key.substring(0, 10)}...${key.substring(key.length - 4)}`,
    ),
  }
}

// Function ƒë·ªÉ reset key rotation
export const resetKeyRotation = () => {
  currentKeyIndex = 0
  requestCount = 0
  console.log("üîÑ Key rotation reset")
}

// Function ƒë·ªÉ l·∫•y danh s√°ch model kh·∫£ d·ª•ng
export const getAvailableModels = (): string[] => {
  return AVAILABLE_MODELS
}
