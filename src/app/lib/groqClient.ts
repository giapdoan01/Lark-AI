import { Groq } from "groq-sdk"

const groq = new Groq({
  apiKey: process.env.NEXT_PUBLIC_GROQ_API_KEY || "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",
  dangerouslyAllowBrowser: true,
})

// Danh s√°ch c√°c model kh·∫£ d·ª•ng (theo th·ª© t·ª± ∆∞u ti√™n)
const AVAILABLE_MODELS = [
  "llama-3.3-70b-versatile",
  "llama-3.1-8b-instant",
  "llama3-70b-8192",
  "llama3-8b-8192",
  "mixtral-8x7b-32768",
  "gemma-7b-it",
]

// Function ƒë·ªÉ chia nh·ªè d·ªØ li·ªáu n·∫øu qu√° l·ªõn
const chunkData = (data: any[], maxChunkSize = 10): any[][] => {
  const chunks = []
  for (let i = 0; i < data.length; i += maxChunkSize) {
    chunks.push(data.slice(i, i + maxChunkSize))
  }
  return chunks
}

// Function th·ª≠ c√°c model kh√°c nhau
const tryWithDifferentModels = async (messages: any[], currentModelIndex = 0): Promise<string> => {
  if (currentModelIndex >= AVAILABLE_MODELS.length) {
    throw new Error("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")
  }

  const model = AVAILABLE_MODELS[currentModelIndex]
  console.log(`ü§ñ Th·ª≠ model: ${model}`)

  try {
    const chatCompletion = await groq.chat.completions.create({
      model: model,
      messages: messages,
      temperature: 0.7,
      max_tokens: 2048, // TƒÉng max_tokens ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi d√†i h∆°n
      top_p: 1,
    })

    const response = chatCompletion.choices[0].message.content
    console.log(`‚úÖ Model ${model} ho·∫°t ƒë·ªông thÔøΩÔøΩnh c√¥ng`)
    return response || "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ AI."
  } catch (error) {
    console.log(`‚ùå Model ${model} th·∫•t b·∫°i:`, error)

    // N·∫øu model b·ªã decommission ho·∫∑c kh√¥ng kh·∫£ d·ª•ng, th·ª≠ model ti·∫øp theo
    if (
      error instanceof Error &&
      (error.message.includes("decommissioned") ||
        error.message.includes("not found") ||
        error.message.includes("invalid_request_error"))
    ) {
      console.log(`üîÑ Th·ª≠ model ti·∫øp theo...`)
      return await tryWithDifferentModels(messages, currentModelIndex + 1)
    }

    // N·∫øu l√† l·ªói kh√°c (rate limit, network, etc.), throw ngay
    throw error
  }
}

export const askAI = async (context: string, question: string): Promise<string> => {
  try {
    console.log("ü§ñ B·∫Øt ƒë·∫ßu g·ªçi Groq API...")
    console.log("üìù Context length:", context.length)
    console.log("‚ùì Question:", question)

    // N·∫øu context qu√° l·ªõn, chia nh·ªè v√† x·ª≠ l√Ω
    if (context.length > 15000) {
      console.log("‚ö†Ô∏è Context r·∫•t l·ªõn, ƒëang x·ª≠ l√Ω theo chunks...")

      // Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ context
      const dataMatch = context.match(
        /D∆∞·ªõi ƒë√¢y l√†.*?d·ªØ li·ªáu t·ª´ b·∫£ng[^:]*:\s*([\s\S]*?)\s*(?:T·ªïng c·ªông c√≥|H√£y ph√¢n t√≠ch)/,
      )

      if (dataMatch) {
        try {
          const rawData = dataMatch[1]
          const parsedData = JSON.parse(rawData)

          // Chia d·ªØ li·ªáu th√†nh chunks nh·ªè h∆°n
          const chunks = chunkData(parsedData, 20)
          console.log(`üìä Chia d·ªØ li·ªáu th√†nh ${chunks.length} chunks`)

          // X·ª≠ l√Ω chunk ƒë·∫ßu ti√™n v·ªõi context ƒë·∫ßy ƒë·ªß
          const firstChunkContext = context.replace(
            rawData,
            JSON.stringify(chunks[0], null, 2) + `\n\n(ƒê√¢y l√† chunk 1/${chunks.length} c·ªßa d·ªØ li·ªáu)`,
          )

          const messages = [
            {
              role: "system" as const,
              content: firstChunkContext,
            },
            {
              role: "user" as const,
              content: question,
            },
          ]

          return await tryWithDifferentModels(messages)
        } catch (parseError) {
          console.log("‚ö†Ô∏è Kh√¥ng th·ªÉ parse d·ªØ li·ªáu, s·ª≠ d·ª•ng context g·ªëc r√∫t g·ªçn")
        }
      }

      // Fallback: r√∫t g·ªçn context
      const truncatedContext = context.substring(0, 12000) + "\n\n... (D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn do qu√° l·ªõn)"
      context = truncatedContext
    }

    const messages = [
      {
        role: "system" as const,
        content: context,
      },
      {
        role: "user" as const,
        content: question,
      },
    ]

    // Th·ª≠ v·ªõi c√°c model kh√°c nhau
    const response = await tryWithDifferentModels(messages)
    console.log("‚úÖ Groq API response received")

    return response
  } catch (error) {
    console.error("‚ùå Chi ti·∫øt l·ªói Groq API:", error)

    // X·ª≠ l√Ω c√°c lo·∫°i l·ªói c·ª• th·ªÉ
    if (error instanceof Error) {
      if (error.message.includes("rate_limit")) {
        return "‚è∞ API ƒë√£ ƒë·∫°t gi·ªõi h·∫°n t·ªëc ƒë·ªô. Vui l√≤ng th·ª≠ l·∫°i sau v√†i gi√¢y."
      } else if (error.message.includes("invalid_api_key")) {
        return "üîë API key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh."
      } else if (error.message.includes("context_length")) {
        return "üìè D·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω. H√£y th·ª≠ v·ªõi c√¢u h·ªèi c·ª• th·ªÉ h∆°n."
      } else if (error.message.includes("network") || error.message.includes("fetch")) {
        return "üåê L·ªói k·∫øt n·ªëi m·∫°ng. Vui l√≤ng ki·ªÉm tra internet v√† th·ª≠ l·∫°i."
      } else if (error.message.includes("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")) {
        return "ü§ñ T·∫•t c·∫£ c√°c AI model hi·ªán t·∫°i ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau."
      }

      return `‚ùå L·ªói AI: ${error.message}`
    }

    return "‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi AI. Vui l√≤ng th·ª≠ l·∫°i."
  }
}

// Function test API v·ªõi model m·ªõi
export const testGroqAPI = async (): Promise<{ success: boolean; message: string; workingModel?: string }> => {
  console.log("üß™ Testing Groq API v·ªõi c√°c model kh·∫£ d·ª•ng...")

  for (const model of AVAILABLE_MODELS) {
    try {
      console.log(`üß™ Testing model: ${model}`)

      const testCompletion = await groq.chat.completions.create({
        model: model,
        messages: [
          {
            role: "user",
            content: "Xin ch√†o! H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát: 1+1 b·∫±ng bao nhi√™u?",
          },
        ],
        temperature: 0.1,
        max_tokens: 50,
      })

      const response = testCompletion.choices[0].message.content
      console.log(`‚úÖ Model ${model} ho·∫°t ƒë·ªông! Response:`, response)

      return {
        success: true,
        message: `Model ${model} ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng. Test response: ${response}`,
        workingModel: model,
      }
    } catch (error) {
      console.log(`‚ùå Model ${model} failed:`, error)
      continue
    }
  }

  return {
    success: false,
    message: "T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i sau.",
  }
}

// Function ƒë·ªÉ l·∫•y danh s√°ch model kh·∫£ d·ª•ng
export const getAvailableModels = (): string[] => {
  return AVAILABLE_MODELS
}
