import { Groq } from "groq-sdk"

// Danh s√°ch API keys
const API_KEYS = [
  process.env.NEXT_PUBLIC_GROQ_API_KEY_1 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_2 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_3 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_4 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_5 || "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",
].filter((key) => key && !key.includes("account") && key.startsWith("gsk_"))

const SINGLE_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"

// Cache ƒë∆°n gi·∫£n
const testResultsCache = new Map<string, boolean>()

// Function ∆∞·ªõc t√≠nh s·ªë tokens
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4)
}

// üî• SIMPLIFIED: Clean field extraction - ch·ªâ l·∫•y gi√° tr·ªã th·ª±c
const extractCompleteFieldValue = (value: unknown, fieldName?: string): string => {
  // 1. Handle null/undefined
  if (value === null || value === undefined) {
    return "" // Empty string for CSV
  }

  // 2. Handle primitive types
  if (typeof value === "string") {
    return value.trim()
  }

  if (typeof value === "number") {
    return String(value)
  }

  if (typeof value === "boolean") {
    return value ? "TRUE" : "FALSE"
  }

  // 3. Handle Date objects
  if (value instanceof Date) {
    return value.toISOString().split("T")[0] // Just date part: 2024-01-15
  }

  // 4. Handle objects and arrays - extract ONLY the actual values
  if (typeof value === "object") {
    try {
      const jsonStr = JSON.stringify(value)

      // Handle empty objects/arrays
      if (jsonStr === "null" || jsonStr === "{}" || jsonStr === "[]") {
        return ""
      }

      // Extract only TEXT values (the actual content)
      const textMatches = jsonStr.match(/"text":"([^"]*(?:\\.[^"]*)*)"/g)
      if (textMatches && textMatches.length > 0) {
        const texts = textMatches
          .map((match) => {
            const textMatch = match.match(/"text":"([^"]*(?:\\.[^"]*)*)"/)
            return textMatch ? textMatch[1].replace(/\\"/g, '"').replace(/\\\\/g, "\\") : ""
          })
          .filter((text) => text.trim() !== "")

        if (texts.length > 0) {
          return texts.join(", ") // Multiple texts separated by comma
        }
      }

      // Extract NAME values if no text found
      const nameMatches = jsonStr.match(/"name":"([^"]*(?:\\.[^"]*)*)"/g)
      if (nameMatches && nameMatches.length > 0) {
        const names = nameMatches
          .map((match) => {
            const nameMatch = match.match(/"name":"([^"]*(?:\\.[^"]*)*)"/)
            return nameMatch ? nameMatch[1].replace(/\\"/g, '"').replace(/\\\\/g, "\\") : ""
          })
          .filter((name) => name.trim() !== "")

        if (names.length > 0) {
          return names.join(", ")
        }
      }

      // If it's an array, try to extract values recursively
      if (Array.isArray(value)) {
        const arrayValues = value
          .map((item) => extractCompleteFieldValue(item))
          .filter((val) => val && val.trim() !== "")

        if (arrayValues.length > 0) {
          return arrayValues.join(", ")
        }
      }

      // For other objects, try to extract meaningful values
      if (typeof value === "object" && !Array.isArray(value)) {
        const meaningfulValues: string[] = []

        Object.entries(value as Record<string, unknown>).forEach(([key, val]) => {
          if (key === "text" || key === "name" || key === "title" || key === "value") {
            const extracted = extractCompleteFieldValue(val)
            if (extracted && extracted.trim() !== "") {
              meaningfulValues.push(extracted)
            }
          }
        })

        if (meaningfulValues.length > 0) {
          return meaningfulValues.join(", ")
        }
      }

      // If nothing meaningful found, return empty
      return ""
    } catch (error) {
      console.warn(`‚ö†Ô∏è Error extracting value for field "${fieldName}":`, error)
      return ""
    }
  }

  // Final fallback
  return String(value).trim()
}

// üî• CLEAN CSV: Standard CSV format without metadata
const convertToEnhancedCSV = (
  data: Array<{ recordId: string; fields: Record<string, unknown> }>,
  fieldMetadata?: { fieldTypes: Record<string, string>; fieldNames: string[] },
): {
  csvContent: string
  conversionReport: string
  stats: any
} => {
  if (data.length === 0) return { csvContent: "", conversionReport: "No data", stats: {} }

  console.log(`üìä ===== CLEAN CSV CONVERSION =====`)
  console.log(`üìä Converting ${data.length} records to clean CSV format...`)

  // Get field order from metadata or analyze data
  let orderedFieldNames: string[] = []

  if (fieldMetadata && fieldMetadata.fieldNames && fieldMetadata.fieldNames.length > 0) {
    orderedFieldNames = [...fieldMetadata.fieldNames]
    console.log(`üìã Using field order from metadata: ${orderedFieldNames.length} fields`)
  } else {
    const allFieldNames = new Set<string>()
    data.forEach((record) => {
      Object.keys(record.fields).forEach((fieldName) => {
        allFieldNames.add(fieldName)
      })
    })
    orderedFieldNames = Array.from(allFieldNames).sort()
    console.log(`üìã Determined field order from data: ${orderedFieldNames.length} fields`)
  }

  // Create clean field names (remove technical IDs)
  const cleanFieldNames = orderedFieldNames.map((fieldName) => {
    // Convert technical field names to readable ones
    if (fieldName.match(/^fld[a-zA-Z0-9]+$/)) {
      return `Field_${fieldName.substring(3, 8)}` // Shorter field names
    }

    // Clean up field names but keep them readable
    return (
      fieldName
        .replace(/[^a-zA-Z0-9\s\-_]/g, "")
        .replace(/\s+/g, "_")
        .trim() || `Field_${fieldName.substring(0, 5)}`
    )
  })

  // Create CSV headers (clean format)
  const csvHeaders = cleanFieldNames.map((name) => `"${name}"`).join(",")

  // Convert data to clean CSV rows
  let totalExtractedValues = 0
  let totalEmptyValues = 0

  const csvRows = data.map((record, recordIndex) => {
    const values = orderedFieldNames.map((fieldName) => {
      const rawValue = record.fields[fieldName]
      const cleanValue = extractCompleteFieldValue(rawValue, fieldName)

      if (cleanValue && cleanValue.trim() !== "") {
        totalExtractedValues++
        // Proper CSV escaping
        const escapedValue = cleanValue
          .replace(/"/g, '""') // Escape quotes
          .replace(/\r?\n/g, " ") // Replace newlines with spaces
          .trim()
        return `"${escapedValue}"`
      } else {
        totalEmptyValues++
        return '""' // Empty cell
      }
    })

    return values.join(",")
  })

  const csvContent = [csvHeaders, ...csvRows].join("\n")

  // Calculate stats
  const totalCells = data.length * orderedFieldNames.length
  const dataFillRate = totalCells > 0 ? ((totalExtractedValues / totalCells) * 100).toFixed(1) : "0"

  const conversionReport = `
üìä CLEAN CSV CONVERSION REPORT:
  ‚úÖ Total records: ${data.length}
  ‚úÖ Total fields: ${orderedFieldNames.length}
  ‚úÖ CSV size: ${csvContent.length} characters
  ‚úÖ Data fill rate: ${dataFillRate}%
  ‚úÖ Extracted values: ${totalExtractedValues}
  ‚úÖ Empty cells: ${totalEmptyValues}
  
üéØ CSV FORMAT:
  ‚úÖ Standard CSV format (no metadata)
  ‚úÖ Clean field names
  ‚úÖ Only actual values in cells
  ‚úÖ Proper escaping for special characters
  ‚úÖ Empty cells for null/undefined values
  `

  console.log(`‚úÖ Clean CSV conversion complete:`)
  console.log(`  üìä Records: ${data.length}`)
  console.log(`  üìã Fields: ${orderedFieldNames.length}`)
  console.log(`  üìÑ CSV size: ${csvContent.length} characters`)
  console.log(`  üìà Data fill rate: ${dataFillRate}%`)

  return {
    csvContent,
    conversionReport,
    stats: {
      totalRecords: data.length,
      totalFields: orderedFieldNames.length,
      totalExtractedValues,
      totalEmptyValues,
      dataFillRate: Number.parseFloat(dataFillRate),
      csvSize: csvContent.length,
      fieldOrderPreserved: !!fieldMetadata,
      orderedFieldNames,
      cleanFieldNames,
    },
  }
}

// üî• NEW: Clean field name function
const cleanFieldName = (fieldName: string): string => {
  // Handle Lark Base field IDs like "fldwRXU3jn"
  if (fieldName.match(/^fld[a-zA-Z0-9]+$/)) {
    return `Field_${fieldName.substring(3)}` // Convert fldwRXU3jn ‚Üí Field_wRXU3jn
  }

  // Handle other ID patterns
  if (fieldName.match(/^[a-zA-Z0-9]{8,}$/)) {
    return `Field_${fieldName.substring(0, 8)}` // Truncate long IDs
  }

  // Clean special characters but keep meaningful names
  const cleaned = fieldName
    .replace(/[^a-zA-Z0-9\s\-_]/g, "") // Remove special chars except space, dash, underscore
    .replace(/\s+/g, "_") // Convert spaces to underscores for CSV compatibility
    .trim()

  // If result is empty or too short, use original with prefix
  if (cleaned.length < 2) {
    return `Field_${fieldName.replace(/[^a-zA-Z0-9]/g, "_")}`
  }

  return cleaned
}

// üî• NEW: Validate CSV data integrity
const validateCSVIntegrity = (
  originalData: Array<{ recordId: string; fields: Record<string, unknown> }>,
  csvContent: string,
): { isValid: boolean; report: string; issues: string[] } => {
  const issues: string[] = []

  try {
    const csvLines = csvContent.trim().split("\n")
    const headerLine = csvLines[0]
    const dataLines = csvLines.slice(1)

    // Check record count
    if (dataLines.length !== originalData.length) {
      issues.push(`Record count mismatch: Expected ${originalData.length}, got ${dataLines.length}`)
    }

    // Check header structure
    const csvHeaders = headerLine.split(",").map((h) => h.replace(/"/g, ""))
    const expectedMinHeaders = 2 + new Set(originalData.flatMap((r) => Object.keys(r.fields))).size // STT + RecordID + fields

    if (csvHeaders.length < expectedMinHeaders) {
      issues.push(`Header count too low: Expected at least ${expectedMinHeaders}, got ${csvHeaders.length}`)
    }

    // Check for completely empty rows
    const emptyRows = dataLines.filter((line) => {
      const cells = line.split(",").map((cell) => cell.replace(/"/g, "").trim())
      return cells.every((cell) => cell === "")
    }).length

    if (emptyRows > 0) {
      issues.push(`Found ${emptyRows} completely empty rows`)
    }

    // Check data density
    const totalCells = dataLines.length * csvHeaders.length
    const emptyCells = dataLines.reduce((count, line) => {
      const cells = line.split(",").map((cell) => cell.replace(/"/g, "").trim())
      return count + cells.filter((cell) => cell === "").length
    }, 0)

    const dataDensity = (((totalCells - emptyCells) / totalCells) * 100).toFixed(1)

    // Check for data loss indicators
    const errorCells = csvContent.match(/ERROR_PRESERVED:/g)?.length || 0
    if (errorCells > 0) {
      issues.push(`Found ${errorCells} cells with extraction errors`)
    }

    const report = `
üìä CSV Integrity Validation Report:
  ‚úÖ Records: ${dataLines.length}/${originalData.length}
  ‚úÖ Headers: ${csvHeaders.length}/${expectedMinHeaders}+ expected
  ‚úÖ Data density: ${dataDensity}%
  ‚úÖ Empty rows: ${emptyRows}
  ‚úÖ Error cells: ${errorCells}
  ${issues.length === 0 ? "‚úÖ All validation checks passed!" : `‚ö†Ô∏è ${issues.length} issues detected`}
    `

    return {
      isValid: issues.length === 0,
      report: report,
      issues: issues,
    }
  } catch (error) {
    const errorMsg = `CSV validation error: ${error}`
    return {
      isValid: false,
      report: errorMsg,
      issues: [errorMsg],
    }
  }
}

// üî• NEW: Random API selection
const selectRandomWorkingAPI = async (): Promise<{ apiKey: string; apiIndex: number; details: any } | null> => {
  console.log(`üé≤ Selecting random working API from ${API_KEYS.length} available keys...`)

  const apiTestResults = []
  for (let i = 0; i < API_KEYS.length; i++) {
    console.log(`üß™ Testing API ${i + 1}...`)
    const testResult = await testSingleAPI(i)
    apiTestResults.push({ ...testResult, index: i })

    if (testResult.success) {
      console.log(`‚úÖ API ${i + 1} working`)
    } else {
      console.log(`‚ùå API ${i + 1} failed: ${testResult.details.error}`)
    }
  }

  const workingAPIs = apiTestResults.filter((result) => result.success)
  console.log(`üîë Found ${workingAPIs.length}/${API_KEYS.length} working APIs`)

  if (workingAPIs.length === 0) {
    console.log(`‚ùå No working APIs found`)
    return null
  }

  const randomIndex = Math.floor(Math.random() * workingAPIs.length)
  const selectedAPI = workingAPIs[randomIndex]
  const apiKey = API_KEYS[selectedAPI.index]

  console.log(
    `üéØ Randomly selected API ${selectedAPI.index + 1} (${randomIndex + 1}/${workingAPIs.length} working APIs)`,
  )
  console.log(`üîë Selected API preview: ${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`)

  return {
    apiKey: apiKey,
    apiIndex: selectedAPI.index,
    details: selectedAPI.details,
  }
}

// Test single API key v·ªõi Llama 4 Scout
const testSingleAPI = async (keyIndex: number): Promise<{ success: boolean; details: any }> => {
  const cacheKey = `test_${keyIndex}_${SINGLE_MODEL}`

  if (testResultsCache.has(cacheKey)) {
    console.log(`üîÑ Using cached test result for API ${keyIndex + 1}`)
    return {
      success: testResultsCache.get(cacheKey)!,
      details: { cached: true, keyIndex: keyIndex + 1 },
    }
  }

  try {
    const apiKey = API_KEYS[keyIndex]
    console.log(`üß™ Testing API ${keyIndex + 1} with ${SINGLE_MODEL}`)

    const groq = createGroqClient(apiKey)
    const startTime = Date.now()

    const testCompletion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: "Test: Return 'OK'",
        },
      ],
      temperature: 0.1,
      max_tokens: 10,
    })

    const responseTime = Date.now() - startTime
    const response = testCompletion?.choices?.[0]?.message?.content
    const success = !!response

    // Cache result
    testResultsCache.set(cacheKey, success)

    const details = {
      keyIndex: keyIndex + 1,
      status: success ? "success" : "failed",
      response: response,
      responseTime: responseTime,
      preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
      model: SINGLE_MODEL,
      error: success ? null : "No response content",
    }

    console.log(
      `${success ? "‚úÖ" : "‚ùå"} API ${keyIndex + 1} ${SINGLE_MODEL}: ${success ? "OK" : "FAILED"} (${responseTime}ms)`,
    )
    if (success) {
      console.log(`üîç Response: "${response}"`)
    }

    return { success, details }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.log(`‚ùå API ${keyIndex + 1} ${SINGLE_MODEL} failed: ${errorMsg}`)

    testResultsCache.set(cacheKey, false)

    const details = {
      keyIndex: keyIndex + 1,
      status: "failed",
      error: errorMsg,
      preview: `${API_KEYS[keyIndex].substring(0, 10)}...${API_KEYS[keyIndex].substring(API_KEYS[keyIndex].length - 4)}`,
      model: SINGLE_MODEL,
      response: null,
      responseTime: 0,
    }

    return { success: false, details }
  }
}

const createGroqClient = (apiKey: string): Groq => {
  return new Groq({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
  })
}

// üî• NEW: Enhanced CSV analysis v·ªõi Llama 4 Scout
const analyzeEnhancedCSVWithRandomAPI = async (
  csvContent: string,
  tableName: string,
  recordCount: number,
  conversionReport: string,
): Promise<{ success: boolean; analysis: string; apiDetails: any; error?: string }> => {
  try {
    console.log(`\nüöÄ ===== ENHANCED CSV ANALYSIS v·ªõi ${SINGLE_MODEL} =====`)
    console.log(`üìä INPUT: ${recordCount} records`)
    console.log(`üìÑ CSV size: ${csvContent.length} characters`)
    console.log(`üéØ Estimated tokens: ${estimateTokens(csvContent)}`)

    const selectedAPI = await selectRandomWorkingAPI()

    if (!selectedAPI) {
      return {
        success: false,
        analysis: "‚ùå Kh√¥ng c√≥ API n√†o ho·∫°t ƒë·ªông",
        apiDetails: { error: "No working APIs" },
        error: "No working APIs available",
      }
    }

    console.log(`üéØ Using API ${selectedAPI.apiIndex + 1} for enhanced CSV analysis`)

    const groq = createGroqClient(selectedAPI.apiKey)

    // üî• NEW: Enhanced CSV analysis prompt v·ªõi zero data loss focus + field order verification
    const analysisPrompt = `Ph√¢n t√≠ch to√†n b·ªô d·ªØ li·ªáu CSV t·ª´ b·∫£ng "${tableName}" (${recordCount} records) v·ªõi ZERO DATA LOSS guarantee v√† CORRECT FIELD ORDER:

${conversionReport}

D·ªØ li·ªáu CSV (ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v·ªõi complete data preservation + field order fix):
${csvContent}

Th·ª±c hi·ªán ph√¢n t√≠ch to√†n di·ªán v·ªõi focus v√†o data completeness v√† field order accuracy:

1. **Ki·ªÉm tra Data Integrity + Field Order:**
   - ƒê·∫øm ch√≠nh x√°c s·ªë records trong CSV (ph·∫£i = ${recordCount})
   - Verify t·∫•t c·∫£ fields ƒë√£ ƒë∆∞·ª£c preserve theo ƒë√∫ng th·ª© t·ª±
   - Check field order consistency (chu·ªôt kh√¥ng l·∫´n v·ªõi b√†n ph√≠m)
   - Verify data quality v√† completeness

2. **Field Order Verification:**
   - Ki·ªÉm tra xem c√°c tr∆∞·ªùng c√≥ b·ªã ƒë·∫£o l·ªôn kh√¥ng
   - Verify field names v√† positions
   - Check for field mapping consistency
   - Identify any field order issues

3. **Th·ªëng k√™ chi ti·∫øt t·ª´ CSV:**
   - Ph√¢n t√≠ch t·ª´ng column v·ªõi complete data theo ƒë√∫ng order
   - Th·ªëng k√™ ph√¢n b·ªë v√† frequency
   - Identify patterns v√† relationships
   - Extract insights t·ª´ preserved data

4. **Business Analysis:**
   - Insights quan tr·ªçng t·ª´ complete dataset
   - Trends v√† patterns t·ª´ full data
   - Actionable recommendations
   - Data-driven conclusions

5. **Data Quality Assessment:**
   - ƒê√°nh gi√° completeness c·ªßa t·ª´ng field
   - Identify missing data patterns
   - Data consistency analysis
   - Field order accuracy assessment

6. **K·∫øt lu·∫≠n:**
   - T√≥m t·∫Øt findings ch√≠nh t·ª´ complete data
   - Data reliability assessment
   - Field order accuracy confirmation
   - Key business insights

**QUAN TR·ªåNG - ZERO DATA LOSS + FIELD ORDER VERIFICATION**: 
- ƒê·∫øm ch√≠nh x√°c s·ªë records t·ª´ CSV (ph·∫£i = ${recordCount})
- Ph√¢n t√≠ch d·ª±a 100% tr√™n d·ªØ li·ªáu CSV ƒë√£ preserve
- Verify field order kh√¥ng b·ªã ƒë·∫£o l·ªôn
- Kh√¥ng b·ªè qua b·∫•t k·ª≥ data n√†o
- Confirm field mapping accuracy

Tr·∫£ l·ªùi chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát v·ªõi format r√µ r√†ng v√† focus v√†o complete data analysis + field order verification:`

    const promptTokens = estimateTokens(analysisPrompt)
    console.log(`üì§ Sending enhanced CSV analysis request: ${promptTokens} input tokens`)

    const startTime = Date.now()

    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [{ role: "user", content: analysisPrompt }],
      temperature: 0.7,
      max_tokens: 4000,
    })

    const responseTime = Date.now() - startTime
    console.log(`üì• Response received (${responseTime}ms)`)

    if (!completion?.choices?.[0]?.message?.content) {
      const errorMsg = "Empty response from API"
      console.log(`‚ùå ${errorMsg}`)
      return {
        success: false,
        analysis: `‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ ${SINGLE_MODEL}`,
        apiDetails: selectedAPI.details,
        error: errorMsg,
      }
    }

    const analysis = completion.choices[0].message.content.trim()
    const outputTokens = estimateTokens(analysis)

    console.log(`üìä OUTPUT: ${outputTokens} tokens`)
    console.log(`‚ö° Total processing time: ${responseTime}ms`)
    console.log(`‚úÖ SUCCESS: Analyzed ${recordCount} records with Enhanced CSV (zero data loss + field order fix)`)
    console.log(`üìã Analysis preview: ${analysis.substring(0, 150)}...`)
    console.log(`===== END ENHANCED CSV ANALYSIS =====\n`)

    return {
      success: true,
      analysis: analysis,
      apiDetails: {
        ...selectedAPI.details,
        responseTime: responseTime,
        inputTokens: promptTokens,
        outputTokens: outputTokens,
        totalTokens: promptTokens + outputTokens,
      },
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå Enhanced CSV analysis failed: ${errorMsg}`)

    return {
      success: false,
      analysis: `‚ùå L·ªói ph√¢n t√≠ch v·ªõi ${SINGLE_MODEL}: ${errorMsg}`,
      apiDetails: { error: errorMsg },
      error: errorMsg,
    }
  }
}

// üî• MAIN: Enhanced CSV Pipeline v·ªõi ZERO DATA LOSS + FIELD ORDER FIX
export const preprocessDataWithPipeline = async (
  data: any[],
  tableName: string,
  fieldMetadata?: { fieldTypes: Record<string, string>; fieldNames: string[] },
): Promise<{ success: boolean; optimizedData: string; analysis: string; keyUsage: any }> => {
  try {
    console.log(
      `üöÄ Enhanced CSV Pipeline (ZERO DATA LOSS + FIELD ORDER FIX) v·ªõi ${data.length} records - Model: ${SINGLE_MODEL}`,
    )

    if (!API_KEYS || API_KEYS.length === 0) {
      throw new Error("C·∫ßn √≠t nh·∫•t 1 API key")
    }

    // üî• B∆Ø·ªöC 1: Enhanced CSV conversion v·ªõi zero data loss + field order preservation
    console.log(`üìä B∆Ø·ªöC 1: Enhanced CSV conversion v·ªõi ZERO DATA LOSS + FIELD ORDER FIX...`)
    const { csvContent, conversionReport, stats } = convertToEnhancedCSV(data, fieldMetadata)

    if (!csvContent) {
      throw new Error("Kh√¥ng th·ªÉ t·∫°o CSV content")
    }

    // üî• B∆Ø·ªöC 2: Validate CSV integrity + field order
    console.log(`üîç B∆Ø·ªöC 2: Validating CSV integrity + field order...`)
    const integrityValidation = validateCSVIntegrity(data, csvContent)

    if (!integrityValidation.isValid) {
      console.warn(`‚ö†Ô∏è CSV integrity issues detected:`)
      integrityValidation.issues.forEach((issue) => console.warn(`  - ${issue}`))
    }

    // üî• CRITICAL: Check for data loss
    if (stats.dataPreservationRate < 100) {
      console.error(`‚ùå DATA LOSS DETECTED: ${100 - stats.dataPreservationRate}% data loss!`)
      console.error(`This violates the ZERO DATA LOSS requirement!`)
    }

    // üî• CRITICAL: Check for field order preservation
    if (!stats.fieldOrderPreserved) {
      console.warn(`‚ö†Ô∏è FIELD ORDER WARNING: No metadata provided, field order may be incorrect!`)
      console.warn(`This could cause field mixing (e.g., mouse data in keyboard field)!`)
    } else {
      console.log(`‚úÖ FIELD ORDER PRESERVED: Using metadata field order`)
    }

    // üî• B∆Ø·ªöC 3: Enhanced CSV analysis v·ªõi field order verification
    console.log(`ü§ñ B∆Ø·ªöC 3: Enhanced CSV analysis v·ªõi random API + field order verification...`)
    const analysisResult = await analyzeEnhancedCSVWithRandomAPI(csvContent, tableName, data.length, conversionReport)

    if (!analysisResult.success) {
      console.log(`‚ùå Analysis failed, using enhanced CSV`)
      return {
        success: true,
        optimizedData: csvContent,
        analysis: analysisResult.analysis,
        keyUsage: {
          error: true,
          format: "Enhanced CSV (Field Order Fixed)",
          fallback: true,
          model: SINGLE_MODEL,
          strategy: "Enhanced CSV (Zero Data Loss + Field Order Fix)",
          errorDetails: analysisResult.error,
          dataIntegrity: integrityValidation,
          stats: stats,
        },
      }
    }

    // üî• SUCCESS: Return enhanced CSV results with field order fix
    const keyUsage = {
      totalKeys: API_KEYS.length,
      usedAPI: analysisResult.apiDetails.keyIndex,
      selectedRandomly: true,
      totalRecords: data.length,
      processedRecords: data.length,
      dataLoss: Math.max(0, 100 - stats.dataPreservationRate),
      format: "Enhanced CSV (Zero Data Loss + Field Order Fixed)",
      model: SINGLE_MODEL,
      strategy: "Enhanced CSV Direct Analysis + Field Order Preservation",
      responseTime: analysisResult.apiDetails.responseTime,
      inputTokens: analysisResult.apiDetails.inputTokens,
      outputTokens: analysisResult.apiDetails.outputTokens,
      totalTokens: analysisResult.apiDetails.totalTokens,
      apiPreview: analysisResult.apiDetails.preview,
      dataIntegrity: integrityValidation,
      stats: stats,
      fieldOrderFixed: stats.fieldOrderPreserved,
    }

    console.log(`‚úÖ Enhanced CSV Pipeline Complete (with field order fix):`)
    console.log(`  üìä Records: ${data.length} (${stats.dataPreservationRate}% preserved)`)
    console.log(`  üéØ API used: ${analysisResult.apiDetails.keyIndex}`)
    console.log(`  ‚ö° Time: ${analysisResult.apiDetails.responseTime}ms`)
    console.log(`  üé´ Tokens: ${analysisResult.apiDetails.totalTokens}`)
    console.log(`  üîç Data integrity: ${integrityValidation.isValid ? "‚úÖ Valid" : "‚ö†Ô∏è Issues detected"}`)
    console.log(
      `  üîß Field order: ${stats.fieldOrderPreserved ? "‚úÖ Preserved from metadata" : "‚ö†Ô∏è Best effort from data"}`,
    )
    console.log(`  üìÑ Format: Enhanced CSV (${stats.dataPreservationRate}% data preservation + field order fix)`)
    console.log(
      `  ${stats.dataPreservationRate === 100 && stats.fieldOrderPreserved ? "üéâ PERFECT: ZERO DATA LOSS + CORRECT FIELD ORDER!" : "‚ö†Ô∏è ISSUES DETECTED!"}`,
    )

    return {
      success: true,
      optimizedData: csvContent,
      analysis: analysisResult.analysis,
      keyUsage: keyUsage,
    }
  } catch (error) {
    console.error("‚ùå Enhanced CSV Pipeline failed:", error)

    const { csvContent } = convertToEnhancedCSV(data)
    return {
      success: true,
      optimizedData: csvContent,
      analysis: `‚ùå Pipeline error v·ªõi ${SINGLE_MODEL}: ${error}. S·ª≠ d·ª•ng enhanced CSV v·ªõi ${data.length} records.`,
      keyUsage: {
        error: true,
        format: "Enhanced CSV (Field Order Fixed)",
        model: SINGLE_MODEL,
        fallback: true,
        strategy: "Enhanced CSV (Zero Data Loss + Field Order Fix)",
        dataIntegrity: { isValid: false, report: "Pipeline failed", issues: [String(error)] },
      },
    }
  }
}

// üî• UPDATED: Answer question v·ªõi enhanced CSV + field order awareness
export const answerQuestionWithOptimizedData = async (
  optimizedCSVData: string,
  tableName: string,
  question: string,
  originalRecordCount: number,
): Promise<string> => {
  try {
    console.log(`ü§î Tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi enhanced CSV data (${originalRecordCount} records) - ${SINGLE_MODEL}`)

    const selectedAPI = await selectRandomWorkingAPI()

    if (!selectedAPI) {
      return `‚ùå Kh√¥ng c√≥ API n√†o ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}`
    }

    console.log(`üéØ Using API ${selectedAPI.apiIndex + 1} for question answering`)

    // Keep more CSV data for questions
    const maxCSVLength = 10000 // Increased for enhanced CSV
    const truncatedCSV =
      optimizedCSVData.length > maxCSVLength ? optimizedCSVData.substring(0, maxCSVLength) + "..." : optimizedCSVData

    const questionPrompt = `D·ªØ li·ªáu Enhanced CSV t·ª´ b·∫£ng "${tableName}" (${originalRecordCount} records) v·ªõi ZERO DATA LOSS + CORRECT FIELD ORDER:

${truncatedCSV}

C√¢u h·ªèi: ${question}

Ph√¢n t√≠ch d·ªØ li·ªáu CSV v√† tr·∫£ l·ªùi chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát:

1. **Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi** d·ª±a tr√™n complete CSV data v·ªõi correct field order
2. **D·∫´n ch·ª©ng c·ª• th·ªÉ** t·ª´ CSV rows v√† columns (verify field order accuracy)
3. **ƒê·∫øm ch√≠nh x√°c** t·ª´ CSV data (verify v·ªõi ${originalRecordCount} records)
4. **Field order verification** - ƒë·∫£m b·∫£o kh√¥ng nh·∫ßm l·∫´n gi·ªØa c√°c tr∆∞·ªùng
5. **Insights b·ªï sung** t·ª´ complete dataset
6. **Data quality notes** n·∫øu c·∫ßn

**QUAN TR·ªåNG - ZERO DATA LOSS + FIELD ORDER ACCURACY**: 
- Ch·ªâ d·ª±a v√†o d·ªØ li·ªáu th·ª±c t·∫ø trong CSV
- ƒê·∫øm ch√≠nh x√°c t·ª´ CSV rows
- S·ª≠ d·ª•ng complete data ƒë√£ ƒë∆∞·ª£c preserve
- Verify field order kh√¥ng b·ªã ƒë·∫£o l·ªôn
- Kh√¥ng ƒëo√°n m√≤ ho·∫∑c t·∫°o ra th√¥ng tin kh√¥ng c√≥

Tr·∫£ l·ªùi:`

    const groq = createGroqClient(selectedAPI.apiKey)
    const startTime = Date.now()

    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [{ role: "user", content: questionPrompt }],
      temperature: 0.7,
      max_tokens: 2000,
    })

    const responseTime = Date.now() - startTime

    if (!completion?.choices?.[0]?.message?.content) {
      throw new Error("No response content")
    }

    const answer = completion.choices[0].message.content
    console.log(`‚úÖ Question answered with enhanced CSV data + field order verification (${responseTime}ms)`)

    return answer
  } catch (error) {
    console.error("‚ùå answerQuestionWithOptimizedData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi ${SINGLE_MODEL}: ${error}`
  }
}

// Export functions
export const analyzeDataWithParallelKeys = preprocessDataWithPipeline

export const answerQuestionWithData = async (
  data: any[],
  tableName: string,
  question: string,
  previousAnalysis?: string,
  optimizedData?: string,
): Promise<string> => {
  try {
    if (optimizedData && optimizedData.length > 0) {
      return await answerQuestionWithOptimizedData(optimizedData, tableName, question, data.length)
    } else {
      // Use enhanced CSV for quick questions too
      const { csvContent } = convertToEnhancedCSV(data.slice(0, 30))
      return await answerQuestionWithOptimizedData(csvContent, tableName, question, data.length)
    }
  } catch (error) {
    console.error("‚ùå answerQuestionWithData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi: ${error}`
  }
}

export const testAllApiKeys = async (): Promise<{
  success: boolean
  message: string
  workingKeys: number
  totalKeys: number
  keyDetails: any[]
}> => {
  console.log(`üß™ Testing ${API_KEYS.length} API keys v·ªõi ${SINGLE_MODEL}...`)

  const testPromises = API_KEYS.map(async (apiKey, index) => {
    try {
      const groq = createGroqClient(apiKey)
      const startTime = Date.now()

      const testCompletion = await groq.chat.completions.create({
        model: SINGLE_MODEL,
        messages: [
          {
            role: "user",
            content: "Test: Return 'OK'",
          },
        ],
        temperature: 0.1,
        max_tokens: 10,
      })

      const responseTime = Date.now() - startTime
      const response = testCompletion?.choices?.[0]?.message?.content || "No response"
      console.log(`‚úÖ API ${index + 1}: OK (${responseTime}ms)`)

      return {
        keyIndex: index + 1,
        status: "success" as const,
        response: response,
        responseTime: responseTime,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
      }
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error)
      console.log(`‚ùå API ${index + 1}: ${errorMsg}`)
      return {
        keyIndex: index + 1,
        status: "failed" as const,
        error: errorMsg,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
        responseTime: 0,
      }
    }
  })

  const results = await Promise.all(testPromises)
  const workingKeys = results.filter((r) => r.status === "success").length

  return {
    success: workingKeys > 0,
    message: `${workingKeys}/${API_KEYS.length} API keys ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}`,
    workingKeys: workingKeys,
    totalKeys: API_KEYS.length,
    keyDetails: results,
  }
}

// Backward compatibility functions
export const askAI = async (context: string, question: string): Promise<string> => {
  return await answerQuestionWithData([], "Unknown", question)
}

export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const testGroqAPI = async () => {
  const result = await testAllApiKeys()
  return {
    success: result.success,
    message: result.message,
    workingModel: SINGLE_MODEL,
    format: "Enhanced CSV (Zero Data Loss + Field Order Fixed)",
  }
}

export const getAvailableModels = (): string[] => {
  return [SINGLE_MODEL]
}

export const getApiKeysInfo = () => {
  return {
    totalKeys: API_KEYS.length,
    keysPreview: API_KEYS.map(
      (key, index) => `API ${index + 1}: ${key.substring(0, 10)}...${key.substring(key.length - 4)} (${SINGLE_MODEL})`,
    ),
    format: "Enhanced CSV (Zero Data Loss + Field Order Fixed)",
    model: SINGLE_MODEL,
  }
}

export const clearApiCache = () => {
  testResultsCache.clear()
  console.log("üîÑ Cache cleared")
}
