import { Groq } from "groq-sdk"

// Danh s√°ch API keys
const API_KEYS = [
  process.env.NEXT_PUBLIC_GROQ_API_KEY_1 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_2 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_3 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_4 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_5 || "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",
].filter((key) => key && !key.includes("account") && key.startsWith("gsk_"))

const SINGLE_MODEL = "llama3-70b-8192"

// Cache ƒë∆°n gi·∫£n
const testResultsCache = new Map<string, boolean>()

// Function ∆∞·ªõc t√≠nh s·ªë tokens
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4)
}

// Extract plain text t·ª´ Lark Base fields
const extractPlainTextFromField = (value: unknown): string => {
  if (value === null || value === undefined) {
    return ""
  }

  if (typeof value === "string") {
    return value.trim()
  }

  if (typeof value === "number") {
    return String(value)
  }

  if (typeof value === "boolean") {
    return value ? "Yes" : "No"
  }

  if (typeof value === "object") {
    try {
      const jsonStr = JSON.stringify(value)

      // Handle array of text objects: [{"type":"text","text":"Intel Pentium"}]
      if (jsonStr.includes('"type":"text"') && jsonStr.includes('"text":')) {
        const textMatches = jsonStr.match(/"text":"([^"]+)"/g)
        if (textMatches) {
          const texts = textMatches.map((match) => match.replace(/"text":"([^"]+)"/, "$1"))
          return texts.join(", ")
        }
      }

      // Handle single option object: {"id":"optr5hYAsF","text":"SSD-128"}
      if (jsonStr.includes('"text":') && jsonStr.includes('"id":')) {
        const textMatch = jsonStr.match(/"text":"([^"]+)"/)
        if (textMatch) {
          return textMatch[1]
        }
      }

      // Handle array of option objects
      if (jsonStr.startsWith("[") && jsonStr.includes('"text":')) {
        const textMatches = jsonStr.match(/"text":"([^"]+)"/g)
        if (textMatches) {
          const texts = textMatches.map((match) => match.replace(/"text":"([^"]+)"/, "$1"))
          return texts.join(", ")
        }
      }

      if (jsonStr === "[null]" || jsonStr === "null") {
        return ""
      }

      // Fallback: return first text value found
      const anyTextMatch = jsonStr.match(/"([^"]+)"/g)
      if (anyTextMatch && anyTextMatch.length > 0) {
        const values = anyTextMatch
          .map((match) => match.replace(/"/g, ""))
          .filter((val) => !["type", "id", "text"].includes(val))

        if (values.length > 0) {
          return values.join(", ")
        }
      }

      return ""
    } catch (error) {
      console.warn("Error parsing field value:", error)
      return String(value).substring(0, 50)
    }
  }

  return String(value)
}

// CSV Conversion v·ªõi consistent format
const convertToCSV = (data: Array<{ recordId: string; fields: Record<string, unknown> }>): string => {
  if (data.length === 0) return ""

  console.log(`üìä Converting ${data.length} records to consistent CSV format...`)

  // Get ALL unique field names t·ª´ T·∫§T C·∫¢ records
  const allFieldNames = new Set<string>()
  data.forEach((record) => {
    Object.keys(record.fields).forEach((fieldName) => {
      allFieldNames.add(fieldName)
    })
  })

  const fieldNames = Array.from(allFieldNames).sort()
  console.log(`üìã Found ${fieldNames.length} unique fields:`, fieldNames.slice(0, 5))

  // Create consistent headers
  const headers = ["STT", ...fieldNames]
  const csvHeaders = headers.join(",")

  // Convert records v·ªõi consistent column structure
  const csvRows = data.map((record, index) => {
    const values = [
      String(index + 1), // STT
      ...fieldNames.map((fieldName) => {
        const rawValue = record.fields[fieldName]
        const cleanValue = extractPlainTextFromField(rawValue)

        if (!cleanValue || cleanValue.trim() === "") {
          return ""
        }

        // Escape commas and quotes for CSV
        if (cleanValue.includes(",") || cleanValue.includes('"') || cleanValue.includes("\n")) {
          return `"${cleanValue.replace(/"/g, '""')}"`
        }

        return cleanValue
      }),
    ]

    return values.join(",")
  })

  const csvContent = [csvHeaders, ...csvRows].join("\n")

  // Calculate compression stats
  const originalJsonSize = JSON.stringify(data).length
  const csvSize = csvContent.length
  const compressionRatio = Math.round((1 - csvSize / originalJsonSize) * 100)

  console.log(`‚úÖ Consistent CSV Conversion Complete:`)
  console.log(`  üìä Records: ${data.length}`)
  console.log(`  üìã Fields: ${fieldNames.length}`)
  console.log(`  üìÑ Total columns: ${headers.length}`)
  console.log(`  üìÑ Original JSON: ${originalJsonSize} chars`)
  console.log(`  üìÑ Consistent CSV: ${csvSize} chars`)
  console.log(`  üéØ Compression: ${compressionRatio}% smaller`)

  return csvContent
}

// üî• NEW: Equal distribution chunking v·ªõi detailed logging
const createEqualDistributionChunks = (
  csvContent: string,
  workingAPICount: number,
): { csvChunks: string[]; chunkStats: any[] } => {
  console.log(`üìä ===== EQUAL DISTRIBUTION CHUNKING =====`)

  const lines = csvContent.split("\n")
  const headerLine = lines[0]
  const dataLines = lines.slice(1)

  console.log(`üìã Total: ${lines.length} lines (1 header + ${dataLines.length} data rows)`)
  console.log(`üîß Working APIs for processing: ${workingAPICount}`)

  // üî• IMPORTANT: Chia ƒë·ªÅu records cho working APIs
  const recordsPerAPI = Math.floor(dataLines.length / workingAPICount)
  const remainingRecords = dataLines.length % workingAPICount

  console.log(`üìä Records per API: ${recordsPerAPI}`)
  console.log(`üìä Remaining records: ${remainingRecords}`)

  const csvChunks: string[] = []
  const chunkStats: any[] = []

  // üî• Chia ƒë·ªÅu records cho t·ª´ng working API
  for (let apiIndex = 0; apiIndex < workingAPICount; apiIndex++) {
    const startIndex = apiIndex * recordsPerAPI
    let endIndex = startIndex + recordsPerAPI

    // üî• API cu·ªëi c√πng nh·∫≠n th√™m remaining records
    if (apiIndex === workingAPICount - 1) {
      endIndex += remainingRecords
    }

    const chunkDataLines = dataLines.slice(startIndex, endIndex)
    const chunkCSV = headerLine + "\n" + chunkDataLines.join("\n")

    csvChunks.push(chunkCSV)

    const chunkStat = {
      apiIndex: apiIndex + 1,
      startRecord: startIndex + 1,
      endRecord: endIndex,
      recordCount: chunkDataLines.length,
      characters: chunkCSV.length,
      estimatedTokens: estimateTokens(chunkCSV),
    }

    chunkStats.push(chunkStat)

    console.log(
      `üì¶ Working API ${apiIndex + 1}: Records ${startIndex + 1}-${endIndex} (${chunkDataLines.length} records, ${chunkCSV.length} chars)`,
    )
  }

  // üî• Validation: ƒê·∫£m b·∫£o kh√¥ng m·∫•t records
  const totalProcessedRecords = chunkStats.reduce((sum, stat) => sum + stat.recordCount, 0)
  console.log(`‚úÖ Validation: ${totalProcessedRecords}/${dataLines.length} records distributed`)

  if (totalProcessedRecords !== dataLines.length) {
    console.error(`‚ùå DATA LOSS DETECTED: ${dataLines.length - totalProcessedRecords} records missing!`)
  } else {
    console.log(`‚úÖ NO DATA LOSS: All ${dataLines.length} records distributed correctly`)
  }

  console.log(`===============================================`)

  return { csvChunks, chunkStats }
}

// CSV Validation
const validateCSV = (csvContent: string): { isValid: boolean; rowCount: number; error?: string } => {
  try {
    const lines = csvContent.trim().split("\n")
    if (lines.length < 2) {
      return { isValid: false, rowCount: 0, error: "CSV must have at least header and one data row" }
    }

    const headerCount = lines[0].split(",").length
    let validRows = 0

    for (let i = 1; i < lines.length; i++) {
      const rowCols = lines[i].split(",").length
      if (rowCols === headerCount) {
        validRows++
      }
    }

    return {
      isValid: validRows > 0,
      rowCount: validRows,
      error: validRows === 0 ? "No valid data rows found" : undefined,
    }
  } catch (error) {
    return {
      isValid: false,
      rowCount: 0,
      error: `CSV validation error: ${error}`,
    }
  }
}

// üî• ENHANCED: Test single API key v·ªõi detailed logging
const testSingleAPI = async (keyIndex: number): Promise<{ success: boolean; details: any }> => {
  const cacheKey = `test_${keyIndex}`

  if (testResultsCache.has(cacheKey)) {
    console.log(`üîÑ Using cached test result for API ${keyIndex + 1}`)
    return {
      success: testResultsCache.get(cacheKey)!,
      details: { cached: true, keyIndex: keyIndex + 1 },
    }
  }

  try {
    const apiKey = API_KEYS[keyIndex]
    console.log(`üß™ Testing API ${keyIndex + 1} with ${SINGLE_MODEL}`)

    const groq = createGroqClient(apiKey)
    const startTime = Date.now()

    const testCompletion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: "Test: Return 'OK'",
        },
      ],
      temperature: 0.1,
      max_tokens: 10,
    })

    const responseTime = Date.now() - startTime
    const response = testCompletion?.choices?.[0]?.message?.content
    const success = !!response

    // Cache result
    testResultsCache.set(cacheKey, success)

    const details = {
      keyIndex: keyIndex + 1,
      status: success ? "success" : "failed",
      response: response,
      responseTime: responseTime,
      preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
      model: SINGLE_MODEL,
      error: success ? null : "No response content",
    }

    console.log(
      `${success ? "‚úÖ" : "‚ùå"} API ${keyIndex + 1} ${SINGLE_MODEL}: ${success ? "OK" : "FAILED"} (${responseTime}ms)`,
    )
    if (success) {
      console.log(`üîç Response: "${response}"`)
    }

    return { success, details }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.log(`‚ùå API ${keyIndex + 1} ${SINGLE_MODEL} failed: ${errorMsg}`)

    testResultsCache.set(cacheKey, false)

    const details = {
      keyIndex: keyIndex + 1,
      status: "failed",
      error: errorMsg,
      preview: `${API_KEYS[keyIndex].substring(0, 10)}...${API_KEYS[keyIndex].substring(API_KEYS[keyIndex].length - 4)}`,
      model: SINGLE_MODEL,
      response: null,
      responseTime: 0,
    }

    return { success: false, details }
  }
}

const createGroqClient = (apiKey: string): Groq => {
  return new Groq({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
  })
}

// Summarize CSV chunk thay v√¨ optimize
const summarizeCSVChunk = async (
  apiKey: string,
  keyIndex: number,
  csvChunk: string,
  chunkIndex: number,
  totalChunks: number,
): Promise<{ success: boolean; summary: string; keyIndex: number; recordCount: number; error?: string }> => {
  try {
    const lines = csvChunk.trim().split("\n")
    const dataRowCount = lines.length - 1 // Tr·ª´ header

    console.log(`\nüìä ===== API ${keyIndex + 1} - SUMMARIZE CHUNK ${chunkIndex + 1}/${totalChunks} =====`)
    console.log(`üìä INPUT: ${dataRowCount} records`)
    console.log(`‚ö° Model: ${SINGLE_MODEL}`)

    // Validate CSV
    const csvValidation = validateCSV(csvChunk)
    if (!csvValidation.isValid) {
      console.log(`‚ùå CSV VALIDATION FAILED: ${csvValidation.error}`)
      return {
        success: false,
        summary: "",
        keyIndex: keyIndex,
        recordCount: 0,
        error: `CSV validation failed: ${csvValidation.error}`,
      }
    }

    console.log(`‚úÖ CSV Validation: ${csvValidation.rowCount} valid rows`)

    const groq = createGroqClient(apiKey)

    // Summarize prompt thay v√¨ optimize
    const summarizePrompt = `Th·ªëng k√™ d·ªØ li·ªáu CSV n√†y m·ªôt c√°ch ng·∫Øn g·ªçn nh·∫•t c√≥ th·ªÉ:

${csvChunk}

Tr·∫£ v·ªÅ th·ªëng k√™ theo format:
- T·ªïng records: [s·ªë]
- C√°c tr∆∞·ªùng ch√≠nh: [li·ªát k√™]
- Th·ªëng k√™ quan tr·ªçng: [ng·∫Øn g·ªçn]
- Insights: [1-2 c√¢u]

Ch·ªâ tr·∫£ v·ªÅ th·ªëng k√™, kh√¥ng gi·∫£i th√≠ch th√™m:`

    const promptTokens = estimateTokens(summarizePrompt)
    console.log(`üì§ Sending summarize request: ${promptTokens} input tokens`)

    const startTime = Date.now()

    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [{ role: "user", content: summarizePrompt }],
      temperature: 0.1,
      max_tokens: 500, // Gi·∫£m tokens cho summary
    })

    const responseTime = Date.now() - startTime
    console.log(`üì• Response received (${responseTime}ms)`)

    if (!completion?.choices?.[0]?.message?.content) {
      const errorMsg = "Empty response from API"
      console.log(`‚ùå ${errorMsg}`)
      return {
        success: false,
        summary: "",
        keyIndex: keyIndex,
        recordCount: dataRowCount,
        error: errorMsg,
      }
    }

    const summary = completion.choices[0].message.content.trim()
    const outputTokens = estimateTokens(summary)

    console.log(`üìä OUTPUT: ${outputTokens} tokens`)
    console.log(`‚ö° Processing time: ${responseTime}ms`)
    console.log(`‚úÖ SUCCESS: Summarized ${dataRowCount} records`)
    console.log(`üìã Summary preview: ${summary.substring(0, 100)}...`)
    console.log(`===== END CHUNK ${chunkIndex + 1} =====\n`)

    return {
      success: true,
      summary: summary,
      keyIndex: keyIndex,
      recordCount: dataRowCount,
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå CHUNK ${chunkIndex + 1} FAILED: ${errorMsg}`)

    return {
      success: false,
      summary: `‚ùå L·ªói th·ªëng k√™ chunk ${chunkIndex + 1}: ${errorMsg}`,
      keyIndex: keyIndex,
      recordCount: 0,
      error: errorMsg,
    }
  }
}

// üî• UPDATED: Main pipeline v·ªõi enhanced API testing
export const preprocessDataWithPipeline = async (
  data: any[],
  tableName: string,
): Promise<{ success: boolean; optimizedData: string; analysis: string; keyUsage: any }> => {
  try {
    console.log(`üöÄ Equal Distribution Summarization Pipeline v·ªõi ${data.length} records - Model: ${SINGLE_MODEL}`)

    if (!API_KEYS || API_KEYS.length < 2) {
      throw new Error("C·∫ßn √≠t nh·∫•t 2 API keys (1 cho processing, 1 cho analysis)")
    }

    // üî• B∆Ø·ªöC 1: Convert to consistent CSV
    console.log(`üìä B∆Ø·ªöC 1: Convert to consistent CSV format...`)
    const fullCSV = convertToCSV(data)

    if (!fullCSV) {
      throw new Error("Kh√¥ng th·ªÉ t·∫°o CSV content")
    }

    // üî• B∆Ø·ªöC 2: Test ALL processing APIs (kh√¥ng test analysis API)
    const processingAPICount = API_KEYS.length - 1
    console.log(`üß™ B∆Ø·ªöC 2: Test ${processingAPICount} processing APIs...`)

    const apiTestResults = []
    for (let i = 0; i < processingAPICount; i++) {
      console.log(`üß™ Testing processing API ${i + 1}...`)
      const testResult = await testSingleAPI(i)
      apiTestResults.push(testResult)

      if (testResult.success) {
        console.log(`‚úÖ Processing API ${i + 1} working`)
      } else {
        console.log(`‚ùå Processing API ${i + 1} failed: ${testResult.details.error}`)
      }
    }

    const workingAPIResults = apiTestResults.filter((result) => result.success)
    const workingAPICount = workingAPIResults.length
    console.log(`üîë ${workingAPICount}/${processingAPICount} processing APIs ho·∫°t ƒë·ªông`)

    // üî• Log failed APIs
    const failedAPIs = apiTestResults.filter((result) => !result.success)
    if (failedAPIs.length > 0) {
      console.log(`‚ùå Failed APIs:`)
      failedAPIs.forEach((failed) => {
        console.log(`  API ${failed.details.keyIndex}: ${failed.details.error}`)
      })
    }

    if (workingAPICount === 0) {
      console.log(`‚ö†Ô∏è No working processing APIs, using raw CSV`)
      return {
        success: true,
        optimizedData: fullCSV,
        analysis: `‚ö†Ô∏è Kh√¥ng c√≥ processing APIs ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}, s·ª≠ d·ª•ng raw CSV v·ªõi ${data.length} records.`,
        keyUsage: {
          error: true,
          format: "Raw CSV",
          fallback: true,
          model: SINGLE_MODEL,
          failedAPIs: failedAPIs.length,
          totalAPIs: processingAPICount,
        },
      }
    }

    // üî• B∆Ø·ªöC 3: Equal distribution chunking v·ªõi working APIs
    console.log(`üìä B∆Ø·ªöC 3: Equal distribution chunking v·ªõi ${workingAPICount} working APIs...`)
    const { csvChunks, chunkStats } = createEqualDistributionChunks(fullCSV, workingAPICount)

    if (csvChunks.length === 0) {
      throw new Error("Kh√¥ng th·ªÉ t·∫°o CSV chunks")
    }

    // Show distribution summary
    console.log(`üìä Distribution Summary:`)
    chunkStats.forEach((stat) => {
      console.log(`  Working API ${stat.apiIndex}: ${stat.recordCount} records (${stat.startRecord}-${stat.endRecord})`)
    })

    // üî• B∆Ø·ªöC 4: Summarize chunks v·ªõi working APIs
    console.log(`‚è≥ B∆Ø·ªöC 4: Summarize ${csvChunks.length} CSV chunks v·ªõi ${workingAPICount} working APIs...`)

    const summaryResults = []

    // X·ª≠ l√Ω t·ª´ng chunk v·ªõi working API t∆∞∆°ng ·ª©ng
    for (let i = 0; i < csvChunks.length; i++) {
      const csvChunk = csvChunks[i]
      const workingAPIResult = workingAPIResults[i] // S·ª≠ d·ª•ng working API t∆∞∆°ng ·ª©ng
      const actualAPIIndex = workingAPIResult.details.keyIndex - 1 // Convert to 0-based index

      console.log(
        `üìä Summarizing chunk ${i + 1}/${csvChunks.length} v·ªõi working API ${workingAPIResult.details.keyIndex}`,
      )
      console.log(`üìä Chunk stats: ${chunkStats[i].recordCount} records`)

      // CH·ªà 1 REQUEST DUY NH·∫§T - SUMMARIZE
      const result = await summarizeCSVChunk(API_KEYS[actualAPIIndex], actualAPIIndex, csvChunk, i, csvChunks.length)

      summaryResults.push(result)

      // Delay nh·ªè gi·ªØa c√°c chunks
      if (i < csvChunks.length - 1) {
        await new Promise((resolve) => setTimeout(resolve, 1000))
      }
    }

    // üî• B∆Ø·ªöC 5: Validate results
    const successfulResults = summaryResults.filter((r) => r && r.success)
    console.log(`üìä Results: ${successfulResults.length}/${csvChunks.length} chunks th√†nh c√¥ng`)

    if (successfulResults.length === 0) {
      console.log(`‚ùå All chunks failed, using raw CSV`)
      return {
        success: true,
        optimizedData: fullCSV,
        analysis: `‚ö†Ô∏è T·∫•t c·∫£ chunks th·∫•t b·∫°i v·ªõi ${SINGLE_MODEL}, s·ª≠ d·ª•ng raw CSV v·ªõi ${data.length} records.`,
        keyUsage: {
          totalKeys: API_KEYS.length,
          processedChunks: 0,
          fallback: true,
          format: "Raw CSV",
          model: SINGLE_MODEL,
          failedAPIs: failedAPIs.length,
          workingAPIs: workingAPICount,
        },
      }
    }

    // üî• B∆Ø·ªöC 6: Calculate data loss
    const totalProcessedRecords = successfulResults.reduce((sum, result) => sum + result.recordCount, 0)
    const dataLoss = data.length - totalProcessedRecords
    console.log(`üìä Total processed records: ${totalProcessedRecords}/${data.length}`)

    // üî• IMPORTANT: Validation - ƒë·∫£m b·∫£o tracking data loss
    if (dataLoss > 0) {
      console.warn(`‚ö†Ô∏è DATA LOSS: ${dataLoss} records not processed due to ${failedAPIs.length} failed API(s)`)
    } else {
      console.log(`‚úÖ NO DATA LOSS: All ${data.length} records processed`)
    }

    const combinedSummaries = successfulResults
      .map((result, index) => `=== CHUNK ${index + 1} (${result.recordCount} records) ===\n${result.summary}`)
      .join("\n\n")

    // üî• B∆Ø·ªöC 7: Final analysis v·ªõi API cu·ªëi c√πng
    const analysisAPIIndex = API_KEYS.length - 1
    console.log(`ü§ñ B∆Ø·ªöC 7: Final analysis v·ªõi API ${analysisAPIIndex + 1} - Model: ${SINGLE_MODEL}`)

    const finalAnalysisPrompt = `Ph√¢n t√≠ch t·ªïng h·ª£p d·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}" (${data.length} records, processed: ${totalProcessedRecords}):

${combinedSummaries}

T·ªïng h·ª£p ph√¢n t√≠ch:
1. T·ªïng quan to√†n b·ªô d·ªØ li·ªáu
2. Th·ªëng k√™ ch√≠nh t·ª´ t·∫•t c·∫£ chunks
3. Insights quan tr·ªçng nh·∫•t
4. K·∫øt lu·∫≠n

${dataLoss > 0 ? `\nL∆∞u √Ω: C√≥ ${dataLoss} records kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω do API l·ªói.` : ""}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát:`

    const finalAnalysis = await analyzeWithLlama(API_KEYS[analysisAPIIndex], finalAnalysisPrompt)

    const keyUsage = {
      totalKeys: API_KEYS.length,
      processingKeys: processingAPICount,
      workingKeys: workingAPICount,
      failedKeys: failedAPIs.length,
      analysisKeys: 1,
      processedChunks: successfulResults.length,
      totalRecords: data.length,
      processedRecords: totalProcessedRecords,
      dataLoss: dataLoss,
      successRate: `${Math.round((successfulResults.length / csvChunks.length) * 100)}%`,
      format: "Summarized CSV",
      model: SINGLE_MODEL,
      strategy: "Equal Distribution + Summarization",
      failedAPIDetails: failedAPIs.map((f) => ({
        apiIndex: f.details.keyIndex,
        error: f.details.error,
        preview: f.details.preview,
      })),
    }

    return {
      success: true,
      optimizedData: fullCSV, // Return original CSV for reference
      analysis: finalAnalysis,
      keyUsage: keyUsage,
    }
  } catch (error) {
    console.error("‚ùå Equal Distribution Pipeline failed:", error)

    const rawCSV = convertToCSV(data)
    return {
      success: true,
      optimizedData: rawCSV,
      analysis: `‚ùå Pipeline error v·ªõi ${SINGLE_MODEL}: ${error}. S·ª≠ d·ª•ng raw CSV v·ªõi ${data.length} records.`,
      keyUsage: { error: true, format: "Raw CSV", model: SINGLE_MODEL, fallback: true },
    }
  }
}

// Analysis v·ªõi llama3-70b-8192
const analyzeWithLlama = async (apiKey: string, prompt: string): Promise<string> => {
  try {
    const promptTokens = estimateTokens(prompt)
    console.log(`ü§ñ Final analysis v·ªõi ${SINGLE_MODEL}: ${promptTokens} tokens`)

    const groq = createGroqClient(apiKey)
    const startTime = Date.now()

    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: prompt,
        },
      ],
      temperature: 0.7,
      max_tokens: 2000,
    })

    const responseTime = Date.now() - startTime

    if (!completion?.choices?.[0]?.message?.content) {
      throw new Error("No response content")
    }

    const analysis = completion.choices[0].message.content || "Kh√¥ng c√≥ ph√¢n t√≠ch"
    const outputTokens = estimateTokens(analysis)

    console.log(`‚úÖ Final analysis complete: ${outputTokens} tokens (${responseTime}ms)`)

    return analysis
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå Final analysis failed: ${errorMsg}`)
    return `‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch v·ªõi ${SINGLE_MODEL}: ${errorMsg}`
  }
}

// Answer question v·ªõi CSV
export const answerQuestionWithOptimizedData = async (
  optimizedCSVData: string,
  tableName: string,
  question: string,
  originalRecordCount: number,
): Promise<string> => {
  try {
    console.log(`ü§î Tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi CSV data (${originalRecordCount} records)`)

    const maxCSVLength = 4000
    const truncatedCSV =
      optimizedCSVData.length > maxCSVLength ? optimizedCSVData.substring(0, maxCSVLength) + "..." : optimizedCSVData

    const questionPrompt = `D·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}" (${originalRecordCount} records):

${truncatedCSV}

C√¢u h·ªèi: ${question}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát:`

    const analysisAPIIndex = API_KEYS.length - 1
    return await analyzeWithLlama(API_KEYS[analysisAPIIndex], questionPrompt)
  } catch (error) {
    console.error("‚ùå answerQuestionWithOptimizedData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi ${SINGLE_MODEL}: ${error}`
  }
}

// Export functions
export const analyzeDataWithParallelKeys = preprocessDataWithPipeline

export const answerQuestionWithData = async (
  data: any[],
  tableName: string,
  question: string,
  previousAnalysis?: string,
  optimizedData?: string,
): Promise<string> => {
  try {
    if (optimizedData && optimizedData.length > 0) {
      return await answerQuestionWithOptimizedData(optimizedData, tableName, question, data.length)
    } else {
      const quickCSV = convertToCSV(data.slice(0, 30))
      return await answerQuestionWithOptimizedData(quickCSV, tableName, question, data.length)
    }
  } catch (error) {
    console.error("‚ùå answerQuestionWithData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi: ${error}`
  }
}

// üî• ENHANCED: Test all API keys v·ªõi detailed results
export const testAllApiKeys = async (): Promise<{
  success: boolean
  message: string
  workingKeys: number
  totalKeys: number
  keyDetails: any[]
}> => {
  console.log(`üß™ Testing ${API_KEYS.length} API keys v·ªõi ${SINGLE_MODEL}...`)

  const testPromises = API_KEYS.map(async (apiKey, index) => {
    try {
      const groq = createGroqClient(apiKey)
      const startTime = Date.now()

      const testCompletion = await groq.chat.completions.create({
        model: SINGLE_MODEL,
        messages: [
          {
            role: "user",
            content: "Test: Return 'OK'",
          },
        ],
        temperature: 0.1,
        max_tokens: 10,
      })

      const responseTime = Date.now() - startTime
      const response = testCompletion?.choices?.[0]?.message?.content || "No response"
      console.log(`‚úÖ API ${index + 1}: OK (${responseTime}ms)`)

      return {
        keyIndex: index + 1,
        status: "success" as const,
        response: response,
        responseTime: responseTime,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
      }
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error)
      console.log(`‚ùå API ${index + 1}: ${errorMsg}`)
      return {
        keyIndex: index + 1,
        status: "failed" as const,
        error: errorMsg,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
        responseTime: 0,
      }
    }
  })

  const results = await Promise.all(testPromises)
  const workingKeys = results.filter((r) => r.status === "success").length

  return {
    success: workingKeys > 0,
    message: `${workingKeys}/${API_KEYS.length} API keys ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}`,
    workingKeys: workingKeys,
    totalKeys: API_KEYS.length,
    keyDetails: results,
  }
}

// Backward compatibility functions
export const askAI = async (context: string, question: string): Promise<string> => {
  return await answerQuestionWithData([], "Unknown", question)
}

export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const testGroqAPI = async () => {
  const result = await testAllApiKeys()
  return {
    success: result.success,
    message: result.message,
    workingModel: SINGLE_MODEL,
    format: "Equal Distribution CSV",
  }
}

export const getAvailableModels = (): string[] => {
  return [SINGLE_MODEL]
}

export const getApiKeysInfo = () => {
  return {
    totalKeys: API_KEYS.length,
    keysPreview: API_KEYS.map(
      (key, index) => `API ${index + 1}: ${key.substring(0, 10)}...${key.substring(key.length - 4)} (${SINGLE_MODEL})`,
    ),
    format: "Equal Distribution CSV",
    model: SINGLE_MODEL,
  }
}

export const clearApiCache = () => {
  testResultsCache.clear()
  console.log("üîÑ Cache cleared")
}
