import { Groq } from "groq-sdk"

// Danh s√°ch API keys
const API_KEYS = [
  process.env.NEXT_PUBLIC_GROQ_API_KEY_1 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_2 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_3 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_4 || "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",
  process.env.NEXT_PUBLIC_GROQ_API_KEY_5 || "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",
].filter((key) => key && !key.includes("account") && key.startsWith("gsk_"))

// üî• UPDATED: Chuy·ªÉn sang llama3-70b-8192
const SINGLE_MODEL = "llama3-70b-8192"

// üî• SIMPLIFIED: Cache ƒë∆°n gi·∫£n
const testResultsCache = new Map<string, boolean>()

// Function ∆∞·ªõc t√≠nh s·ªë tokens (1 token ‚âà 4 characters)
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4)
}

// üî• NEW: Function ƒë·ªÉ extract plain text t·ª´ Lark Base field values
const extractPlainTextFromField = (value: unknown): string => {
  if (value === null || value === undefined) {
    return ""
  }

  // N·∫øu l√† string ƒë∆°n gi·∫£n
  if (typeof value === "string") {
    return value.trim()
  }

  // N·∫øu l√† number
  if (typeof value === "number") {
    return String(value)
  }

  // N·∫øu l√† boolean
  if (typeof value === "boolean") {
    return value ? "Yes" : "No"
  }

  // N·∫øu l√† object, c·ªë g·∫Øng extract text
  if (typeof value === "object") {
    try {
      const jsonStr = JSON.stringify(value)

      // Handle array of text objects: [{"type":"text","text":"Intel Pentium"}]
      if (jsonStr.includes('"type":"text"') && jsonStr.includes('"text":')) {
        const textMatches = jsonStr.match(/"text":"([^"]+)"/g)
        if (textMatches) {
          const texts = textMatches.map((match) => match.replace(/"text":"([^"]+)"/, "$1"))
          return texts.join(", ")
        }
      }

      // Handle single option object: {"id":"optr5hYAsF","text":"SSD-128"}
      if (jsonStr.includes('"text":') && jsonStr.includes('"id":')) {
        const textMatch = jsonStr.match(/"text":"([^"]+)"/)
        if (textMatch) {
          return textMatch[1]
        }
      }

      // Handle array of option objects: [{"id":"optr5hYAsF","text":"SSD-128"},{"id":"opt6DIhdHV","text":"HDD-256"}]
      if (jsonStr.startsWith("[") && jsonStr.includes('"text":')) {
        const textMatches = jsonStr.match(/"text":"([^"]+)"/g)
        if (textMatches) {
          const texts = textMatches.map((match) => match.replace(/"text":"([^"]+)"/, "$1"))
          return texts.join(", ")
        }
      }

      // Handle [null] arrays
      if (jsonStr === "[null]" || jsonStr === "null") {
        return ""
      }

      // Fallback: return first text value found
      const anyTextMatch = jsonStr.match(/"([^"]+)"/g)
      if (anyTextMatch && anyTextMatch.length > 0) {
        // Filter out keys like "type", "id", "text"
        const values = anyTextMatch
          .map((match) => match.replace(/"/g, ""))
          .filter((val) => !["type", "id", "text"].includes(val))

        if (values.length > 0) {
          return values.join(", ")
        }
      }

      return ""
    } catch (error) {
      console.warn("Error parsing field value:", error)
      return String(value).substring(0, 50) // Truncate long values
    }
  }

  return String(value)
}

// üî• UPDATED: CSV Conversion v·ªõi plain text extraction
const convertToCSV = (data: Array<{ recordId: string; fields: Record<string, unknown> }>): string => {
  if (data.length === 0) return ""

  console.log(`üìä Converting ${data.length} records to clean CSV format...`)

  // Get all unique field names t·ª´ t·∫•t c·∫£ records
  const allFieldNames = new Set<string>()
  data.forEach((record) => {
    Object.keys(record.fields).forEach((fieldName) => {
      allFieldNames.add(fieldName)
    })
  })

  const fieldNames = Array.from(allFieldNames).sort()
  console.log(`üìã Found ${fieldNames.length} unique fields:`, fieldNames.slice(0, 5))

  // üî• NEW: Create simple headers (STT thay v√¨ recordId)
  const headers = ["STT", ...fieldNames]
  const csvHeaders = headers.join(",")

  // üî• UPDATED: Convert records to clean CSV rows
  const csvRows = data.map((record, index) => {
    const values = [
      String(index + 1), // STT thay v√¨ recordId d√†i
      ...fieldNames.map((fieldName) => {
        const rawValue = record.fields[fieldName]
        const cleanValue = extractPlainTextFromField(rawValue)

        // Escape commas and quotes for CSV
        if (cleanValue.includes(",") || cleanValue.includes('"') || cleanValue.includes("\n")) {
          return `"${cleanValue.replace(/"/g, '""')}"`
        }

        return cleanValue || ""
      }),
    ]

    return values.join(",")
  })

  const csvContent = [csvHeaders, ...csvRows].join("\n")

  // üî• NEW: Calculate compression stats
  const originalJsonSize = JSON.stringify(data).length
  const csvSize = csvContent.length
  const compressionRatio = Math.round((1 - csvSize / originalJsonSize) * 100)

  console.log(`‚úÖ Clean CSV Conversion Complete:`)
  console.log(`  üìä Records: ${data.length}`)
  console.log(`  üìã Fields: ${fieldNames.length}`)
  console.log(`  üìÑ Original JSON: ${originalJsonSize} chars`)
  console.log(`  üìÑ Clean CSV: ${csvSize} chars`)
  console.log(`  üéØ Compression: ${compressionRatio}% smaller`)
  console.log(`  üéØ Estimated tokens: ${estimateTokens(csvContent)}`)

  // Show sample of clean data
  const sampleRows = csvContent.split("\n").slice(0, 3)
  console.log(`üìã Sample clean CSV:`)
  sampleRows.forEach((row, i) => {
    console.log(`  ${i === 0 ? "Header" : `Row ${i}`}: ${row.substring(0, 100)}${row.length > 100 ? "..." : ""}`)
  })

  return csvContent
}

// üî• SIMPLIFIED: CSV Validation
const validateCSV = (csvContent: string): { isValid: boolean; rowCount: number; error?: string } => {
  try {
    const lines = csvContent.trim().split("\n")
    if (lines.length < 2) {
      return { isValid: false, rowCount: 0, error: "CSV must have at least header and one data row" }
    }

    const headerCount = lines[0].split(",").length
    let validRows = 0

    for (let i = 1; i < lines.length; i++) {
      const rowCols = lines[i].split(",").length
      if (rowCols === headerCount) {
        validRows++
      }
    }

    return {
      isValid: validRows > 0,
      rowCount: validRows,
      error: validRows === 0 ? "No valid data rows found" : undefined,
    }
  } catch (error) {
    return {
      isValid: false,
      rowCount: 0,
      error: `CSV validation error: ${error}`,
    }
  }
}

// üî• SIMPLIFIED: Chia CSV th√†nh chunks ƒë∆°n gi·∫£n
const createCSVChunks = (data: any[]): { chunks: any[][]; csvChunks: string[] } => {
  console.log(`üìä ===== CSV CHUNKING v·ªõi ${data.length} records =====`)

  // Chia data th√†nh 4 chunks ƒë·ªÅu nhau
  const recordsPerChunk = Math.ceil(data.length / 4)
  const chunks: any[][] = []
  const csvChunks: string[] = []

  for (let i = 0; i < 4; i++) {
    const startIndex = i * recordsPerChunk
    const endIndex = Math.min(startIndex + recordsPerChunk, data.length)
    const chunk = data.slice(startIndex, endIndex)

    if (chunk.length > 0) {
      const chunkCSV = convertToCSV(chunk)
      const chunkTokens = estimateTokens(chunkCSV)

      console.log(`üìä Chunk ${i + 1}: ${chunk.length} records, ${chunkTokens} tokens`)
      chunks.push(chunk)
      csvChunks.push(chunkCSV)
    }
  }

  console.log(`üìä Total chunks created: ${chunks.length}`)
  console.log(`===============================================`)

  return { chunks, csvChunks }
}

// üî• UPDATED: Test single API key v·ªõi llama3-70b-8192
const testSingleAPI = async (keyIndex: number): Promise<boolean> => {
  const cacheKey = `test_${keyIndex}`

  // Check cache first
  if (testResultsCache.has(cacheKey)) {
    console.log(`üîÑ Using cached test result for API ${keyIndex + 1}`)
    return testResultsCache.get(cacheKey)!
  }

  try {
    const apiKey = API_KEYS[keyIndex]
    console.log(`üß™ Testing API ${keyIndex + 1} with ${SINGLE_MODEL}`)

    const groq = createGroqClient(apiKey)

    // CH·ªà 1 REQUEST DUY NH·∫§T
    const testCompletion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: "Test: Return 'OK'",
        },
      ],
      temperature: 0.1,
      max_tokens: 10,
    })

    const response = testCompletion?.choices?.[0]?.message?.content
    const success = !!response

    // Cache result
    testResultsCache.set(cacheKey, success)

    console.log(`‚úÖ API ${keyIndex + 1} ${SINGLE_MODEL}: ${success ? "OK" : "FAILED"}`)
    if (success) {
      console.log(`üîç Response: "${response}"`)
    }
    return success
  } catch (error) {
    console.log(`‚ùå API ${keyIndex + 1} ${SINGLE_MODEL} failed:`, error)
    testResultsCache.set(cacheKey, false)
    return false
  }
}

const createGroqClient = (apiKey: string): Groq => {
  return new Groq({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
  })
}

// üî• UPDATED: Process single CSV chunk v·ªõi better error handling
const processSingleCSVChunk = async (
  apiKey: string,
  keyIndex: number,
  csvChunk: string,
  chunkIndex: number,
  totalChunks: number,
): Promise<{ success: boolean; optimizedData: string; keyIndex: number; error?: string }> => {
  try {
    const estimatedTokens = estimateTokens(csvChunk)

    console.log(`\nüîß ===== API ${keyIndex + 1} - CSV CHUNK ${chunkIndex + 1}/${totalChunks} =====`)
    console.log(`üìä INPUT: ${estimatedTokens} tokens`)
    console.log(`‚ö° Model: ${SINGLE_MODEL}`)

    // Validate CSV
    const csvValidation = validateCSV(csvChunk)
    if (!csvValidation.isValid) {
      console.log(`‚ùå CSV VALIDATION FAILED: ${csvValidation.error}`)
      return {
        success: false,
        optimizedData: "",
        keyIndex: keyIndex,
        error: `CSV validation failed: ${csvValidation.error}`,
      }
    }

    console.log(`‚úÖ CSV Validation: ${csvValidation.rowCount} valid rows`)

    const groq = createGroqClient(apiKey)

    // üî• UPDATED: Simple prompt cho clean CSV
    const optimizePrompt = `Clean this CSV data, remove empty rows, keep only meaningful data:

${csvChunk}

Return clean CSV with same structure:`

    const promptTokens = estimateTokens(optimizePrompt)
    console.log(`üì§ Sending request: ${promptTokens} input tokens`)

    const startTime = Date.now()

    // üî• CH·ªà 1 REQUEST DUY NH·∫§T v·ªõi llama3-70b-8192
    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [{ role: "user", content: optimizePrompt }],
      temperature: 0.1,
      max_tokens: 4000, // TƒÉng max_tokens cho llama3-70b
    })

    const responseTime = Date.now() - startTime
    console.log(`üì• Response received (${responseTime}ms)`)

    if (!completion?.choices?.[0]?.message?.content) {
      const errorMsg = "Empty response from API"
      console.log(`‚ùå ${errorMsg}`)
      return {
        success: false,
        optimizedData: "",
        keyIndex: keyIndex,
        error: errorMsg,
      }
    }

    const optimizedCSV = completion.choices[0].message.content.trim()
    const outputTokens = estimateTokens(optimizedCSV)

    console.log(`üìä OUTPUT: ${outputTokens} tokens`)
    console.log(`‚ö° Processing time: ${responseTime}ms`)

    // üî• IMPROVED: More lenient CSV validation
    const optimizedValidation = validateCSV(optimizedCSV)
    if (optimizedValidation.isValid) {
      console.log(`‚úÖ SUCCESS: Valid CSV with ${optimizedValidation.rowCount} rows`)
      console.log(`===== END CHUNK ${chunkIndex + 1} =====\n`)

      return {
        success: true,
        optimizedData: optimizedCSV,
        keyIndex: keyIndex,
      }
    } else {
      // üî• FALLBACK: N·∫øu validation fail, v·∫´n return original chunk
      console.log(`‚ö†Ô∏è Validation failed but using original chunk: ${optimizedValidation.error}`)
      return {
        success: true,
        optimizedData: csvChunk, // Use original chunk
        keyIndex: keyIndex,
      }
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå CHUNK ${chunkIndex + 1} FAILED: ${errorMsg}`)

    // üî• IMPROVED: Return original chunk instead of failing
    console.log(`üîÑ Using original chunk as fallback`)
    return {
      success: true,
      optimizedData: csvChunk, // Use original chunk as fallback
      keyIndex: keyIndex,
      error: `Processing failed, using original: ${errorMsg}`,
    }
  }
}

// üî• UPDATED: Main pipeline v·ªõi better error handling
export const preprocessDataWithPipeline = async (
  data: any[],
  tableName: string,
): Promise<{ success: boolean; optimizedData: string; analysis: string; keyUsage: any }> => {
  try {
    console.log(`üöÄ Clean CSV Pipeline v·ªõi ${data.length} records - Model: ${SINGLE_MODEL}`)

    if (!API_KEYS || API_KEYS.length < 5) {
      throw new Error("C·∫ßn √≠t nh·∫•t 5 API keys")
    }

    // üî• B∆Ø·ªöC 1: Chia CSV th√†nh chunks
    console.log(`üìä B∆Ø·ªöC 1: Chia CSV th√†nh clean chunks...`)
    const { chunks, csvChunks } = createCSVChunks(data)

    if (csvChunks.length === 0) {
      throw new Error("Kh√¥ng th·ªÉ t·∫°o CSV chunks")
    }

    // Show sample of clean CSV
    console.log(`üìã Sample clean CSV chunk:`)
    const sampleLines = csvChunks[0].split("\n").slice(0, 3)
    sampleLines.forEach((line, i) => {
      console.log(`  ${i === 0 ? "Header" : `Row ${i}`}: ${line}`)
    })

    // üî• B∆Ø·ªöC 2: Test API keys v·ªõi detailed logging
    console.log(`üß™ B∆Ø·ªöC 2: Test ${Math.min(4, csvChunks.length)} API keys...`)
    const keyTests = []

    for (let i = 0; i < Math.min(4, csvChunks.length); i++) {
      console.log(`üß™ Testing API ${i + 1}...`)
      const testResult = await testSingleAPI(i)
      keyTests.push(testResult)

      if (testResult) {
        console.log(`‚úÖ API ${i + 1} working`)
      } else {
        console.log(`‚ùå API ${i + 1} failed`)
      }
    }

    const workingKeys = keyTests.filter(Boolean).length
    console.log(`üîë ${workingKeys}/${Math.min(4, csvChunks.length)} APIs ho·∫°t ƒë·ªông`)

    if (workingKeys === 0) {
      console.log(`‚ö†Ô∏è No working APIs, using clean raw CSV`)
      const rawCSV = convertToCSV(data)
      return {
        success: true,
        optimizedData: rawCSV,
        analysis: `‚ö†Ô∏è Kh√¥ng c√≥ API keys ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}, s·ª≠ d·ª•ng clean CSV v·ªõi ${data.length} records.`,
        keyUsage: { error: true, format: "Clean CSV", fallback: true, model: SINGLE_MODEL },
      }
    }

    // üî• B∆Ø·ªöC 3: Process t·ª´ng chunk v·ªõi better error handling
    console.log(`‚è≥ B∆Ø·ªöC 3: Process ${csvChunks.length} clean CSV chunks...`)

    const processResults = []

    // X·ª≠ l√Ω t·ª´ng chunk v·ªõi API t∆∞∆°ng ·ª©ng
    for (let i = 0; i < csvChunks.length; i++) {
      const csvChunk = csvChunks[i]
      const keyIndex = i % workingKeys // Cycle through working keys

      console.log(`üîß Processing clean CSV chunk ${i + 1} v·ªõi API ${keyIndex + 1}`)

      // CH·ªà 1 REQUEST DUY NH·∫§T
      const result = await processSingleCSVChunk(API_KEYS[keyIndex], keyIndex, csvChunk, i, csvChunks.length)

      processResults.push(result)

      // Delay nh·ªè gi·ªØa c√°c chunks
      if (i < csvChunks.length - 1) {
        await new Promise((resolve) => setTimeout(resolve, 1000)) // TƒÉng delay l√™n 1s
      }
    }

    // üî• IMPROVED: Accept partial success
    const successfulResults = processResults.filter((r) => r && r.success)
    console.log(`üìä Results: ${successfulResults.length}/${csvChunks.length} chunks th√†nh c√¥ng`)

    // üî• IMPROVED: Ch·ªâ fail n·∫øu kh√¥ng c√≥ chunk n√†o th√†nh c√¥ng
    if (successfulResults.length === 0) {
      console.log(`‚ùå All chunks failed, using clean raw CSV`)
      const rawCSV = convertToCSV(data)
      return {
        success: true,
        optimizedData: rawCSV,
        analysis: `‚ö†Ô∏è T·∫•t c·∫£ chunks th·∫•t b·∫°i v·ªõi ${SINGLE_MODEL}, s·ª≠ d·ª•ng clean CSV v·ªõi ${data.length} records.`,
        keyUsage: {
          totalKeys: API_KEYS.length,
          processedChunks: 0,
          fallback: true,
          format: "Clean CSV",
          model: SINGLE_MODEL,
        },
      }
    }

    // üî• B∆Ø·ªöC 4: G·ªôp CSV chunks
    console.log(`üìä B∆Ø·ªöC 4: G·ªôp ${successfulResults.length} clean CSV chunks`)

    let combinedCSVData = ""
    let headers = ""
    const allRows: string[] = []
    let validChunks = 0

    successfulResults.forEach((result, index) => {
      try {
        const csvLines = result.optimizedData.trim().split("\n")
        if (csvLines.length < 2) {
          console.log(`‚ö†Ô∏è Chunk ${index + 1} has insufficient data`)
          return
        }

        if (validChunks === 0) {
          headers = csvLines[0]
          allRows.push(...csvLines.slice(1))
        } else {
          allRows.push(...csvLines.slice(1))
        }
        validChunks++
        console.log(`‚úÖ Merged chunk ${index + 1}: ${csvLines.length - 1} rows`)
      } catch (parseError) {
        console.warn(`‚ö†Ô∏è Kh√¥ng th·ªÉ parse CSV result t·ª´ chunk ${index + 1}:`, parseError)
      }
    })

    combinedCSVData = headers + "\n" + allRows.join("\n")
    const finalTokens = estimateTokens(combinedCSVData)

    console.log(`üìä Final Clean CSV: ${allRows.length} total rows, ${finalTokens} tokens`)

    // Show final sample
    const finalSample = combinedCSVData.split("\n").slice(0, 5)
    console.log(`üìã Final clean CSV sample:`)
    finalSample.forEach((line, i) => {
      console.log(`  ${i === 0 ? "Header" : `Row ${i}`}: ${line}`)
    })

    // üî• B∆Ø·ªöC 5: Ph√¢n t√≠ch t·ªïng h·ª£p
    console.log(`ü§ñ B∆Ø·ªöC 5: Ph√¢n t√≠ch clean CSV v·ªõi API 5 - Model: ${SINGLE_MODEL}`)

    const analysisPrompt = `Ph√¢n t√≠ch d·ªØ li·ªáu CSV t·ª´ b·∫£ng "${tableName}" (${data.length} records):

${combinedCSVData.substring(0, 3000)}${combinedCSVData.length > 3000 ? "..." : ""}

T√≥m t·∫Øt:
1. T·ªïng quan d·ªØ li·ªáu
2. Th·ªëng k√™ ch√≠nh  
3. Insights quan tr·ªçng

Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát.`

    const finalAnalysis = await analyzeWithLlama(API_KEYS[4], analysisPrompt)

    const keyUsage = {
      totalKeys: API_KEYS.length,
      processedChunks: successfulResults.length,
      successRate: `${Math.round((successfulResults.length / csvChunks.length) * 100)}%`,
      chunks: csvChunks.length,
      validChunks: validChunks,
      finalDataSize: combinedCSVData.length,
      finalTokens: finalTokens,
      format: "Clean CSV",
      model: SINGLE_MODEL,
    }

    return {
      success: true,
      optimizedData: combinedCSVData,
      analysis: finalAnalysis,
      keyUsage: keyUsage,
    }
  } catch (error) {
    console.error("‚ùå Clean CSV Pipeline failed:", error)

    // üî• IMPROVED: Always return clean raw CSV as fallback
    const rawCSV = convertToCSV(data)
    return {
      success: true,
      optimizedData: rawCSV,
      analysis: `‚ùå Pipeline error v·ªõi ${SINGLE_MODEL}: ${error}. S·ª≠ d·ª•ng clean CSV v·ªõi ${data.length} records.`,
      keyUsage: { error: true, format: "Clean CSV", model: SINGLE_MODEL, fallback: true },
    }
  }
}

// üî• UPDATED: Analysis v·ªõi llama3-70b-8192
const analyzeWithLlama = async (apiKey: string, prompt: string): Promise<string> => {
  try {
    const promptTokens = estimateTokens(prompt)
    console.log(`ü§ñ Analysis v·ªõi ${SINGLE_MODEL}: ${promptTokens} tokens`)

    const groq = createGroqClient(apiKey)
    const startTime = Date.now()

    // CH·ªà 1 REQUEST DUY NH·∫§T
    const completion = await groq.chat.completions.create({
      model: SINGLE_MODEL,
      messages: [
        {
          role: "user",
          content: prompt,
        },
      ],
      temperature: 0.7,
      max_tokens: 2000,
    })

    const responseTime = Date.now() - startTime

    if (!completion?.choices?.[0]?.message?.content) {
      throw new Error("No response content")
    }

    const analysis = completion.choices[0].message.content || "Kh√¥ng c√≥ ph√¢n t√≠ch"
    const outputTokens = estimateTokens(analysis)

    console.log(`‚úÖ Analysis complete: ${outputTokens} tokens (${responseTime}ms)`)

    return analysis
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`‚ùå Analysis failed: ${errorMsg}`)
    return `‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch v·ªõi ${SINGLE_MODEL}: ${errorMsg}`
  }
}

// üî• UPDATED: Answer question v·ªõi CSV
export const answerQuestionWithOptimizedData = async (
  optimizedCSVData: string,
  tableName: string,
  question: string,
  originalRecordCount: number,
): Promise<string> => {
  try {
    console.log(`ü§î Tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi clean CSV data (${originalRecordCount} records)`)

    // üî• IMPROVED: Truncate CSV if too long
    const maxCSVLength = 4000
    const truncatedCSV =
      optimizedCSVData.length > maxCSVLength ? optimizedCSVData.substring(0, maxCSVLength) + "..." : optimizedCSVData

    const questionPrompt = `D·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}" (${originalRecordCount} records):

${truncatedCSV}

C√¢u h·ªèi: ${question}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát:`

    // CH·ªà 1 REQUEST DUY NH·∫§T
    return await analyzeWithLlama(API_KEYS[4], questionPrompt)
  } catch (error) {
    console.error("‚ùå answerQuestionWithOptimizedData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi ${SINGLE_MODEL}: ${error}`
  }
}

// Export function ch√≠nh
export const analyzeDataWithParallelKeys = preprocessDataWithPipeline

// üî• SIMPLIFIED: Answer question function
export const answerQuestionWithData = async (
  data: any[],
  tableName: string,
  question: string,
  previousAnalysis?: string,
  optimizedData?: string,
): Promise<string> => {
  try {
    if (optimizedData && optimizedData.length > 0) {
      return await answerQuestionWithOptimizedData(optimizedData, tableName, question, data.length)
    } else {
      const quickCSV = convertToCSV(data.slice(0, 30))
      return await answerQuestionWithOptimizedData(quickCSV, tableName, question, data.length)
    }
  } catch (error) {
    console.error("‚ùå answerQuestionWithData failed:", error)
    return `‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi: ${error}`
  }
}

// üî• UPDATED: Test all API keys v·ªõi llama3-70b-8192
export const testAllApiKeys = async (): Promise<{
  success: boolean
  message: string
  workingKeys: number
  totalKeys: number
  keyDetails: any[]
}> => {
  console.log(`üß™ Testing ${API_KEYS.length} API keys v·ªõi ${SINGLE_MODEL}...`)

  const testPromises = API_KEYS.map(async (apiKey, index) => {
    try {
      const groq = createGroqClient(apiKey)

      const testCompletion = await groq.chat.completions.create({
        model: SINGLE_MODEL,
        messages: [
          {
            role: "user",
            content: "Test: Return 'OK'",
          },
        ],
        temperature: 0.1,
        max_tokens: 10,
      })

      const response = testCompletion?.choices?.[0]?.message?.content || "No response"
      console.log(`‚úÖ API ${index + 1}: OK`)

      return {
        keyIndex: index + 1,
        status: "success" as const,
        response: response,
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
      }
    } catch (error) {
      console.log(`‚ùå API ${index + 1}: ${error}`)
      return {
        keyIndex: index + 1,
        status: "failed" as const,
        error: error instanceof Error ? error.message : String(error),
        preview: `${apiKey.substring(0, 10)}...${apiKey.substring(apiKey.length - 4)}`,
        model: SINGLE_MODEL,
      }
    }
  })

  const results = await Promise.all(testPromises)
  const workingKeys = results.filter((r) => r.status === "success").length

  return {
    success: workingKeys > 0,
    message: `${workingKeys}/${API_KEYS.length} API keys ho·∫°t ƒë·ªông v·ªõi ${SINGLE_MODEL}`,
    workingKeys: workingKeys,
    totalKeys: API_KEYS.length,
    keyDetails: results,
  }
}

// Backward compatibility functions
export const askAI = async (context: string, question: string): Promise<string> => {
  return await answerQuestionWithData([], "Unknown", question)
}

export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  return await answerQuestionWithData(data, tableName, question)
}

export const testGroqAPI = async () => {
  const result = await testAllApiKeys()
  return {
    success: result.success,
    message: result.message,
    workingModel: SINGLE_MODEL,
    format: "Clean CSV",
  }
}

export const getAvailableModels = (): string[] => {
  return [SINGLE_MODEL]
}

export const getApiKeysInfo = () => {
  return {
    totalKeys: API_KEYS.length,
    keysPreview: API_KEYS.map(
      (key, index) => `API ${index + 1}: ${key.substring(0, 10)}...${key.substring(key.length - 4)} (${SINGLE_MODEL})`,
    ),
    format: "Clean CSV",
    model: SINGLE_MODEL,
  }
}

// üî• SIMPLIFIED: Clear cache function
export const clearApiCache = () => {
  testResultsCache.clear()
  console.log("üîÑ Cache cleared")
}
