import { Groq } from "groq-sdk"

const groq = new Groq({
  apiKey: process.env.NEXT_PUBLIC_GROQ_API_KEY || "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",
  dangerouslyAllowBrowser: true,
})

// Danh s√°ch c√°c model kh·∫£ d·ª•ng (theo th·ª© t·ª± ∆∞u ti√™n)
const AVAILABLE_MODELS = [
  "mixtral-8x7b-32768",
  "llama-3.3-70b-versatile", // Model c√≥ context length l·ªõn nh·∫•t
  "llama-3.1-8b-instant",
  "llama3-70b-8192",
  "llama3-8b-8192",
  "gemma-7b-it",
]

// Function th·ª≠ c√°c model kh√°c nhau
const tryWithDifferentModels = async (messages: any[], currentModelIndex = 0): Promise<string> => {
  if (currentModelIndex >= AVAILABLE_MODELS.length) {
    throw new Error("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")
  }

  const model = AVAILABLE_MODELS[currentModelIndex]
  console.log(`ü§ñ Th·ª≠ model: ${model}`)

  try {
    const chatCompletion = await groq.chat.completions.create({
      model: model,
      messages: messages,
      temperature: 0.7,
      max_tokens: 4000, // TƒÉng max_tokens
      top_p: 1,
    })

    const response = chatCompletion.choices[0].message.content
    console.log(`‚úÖ Model ${model} ho·∫°t ƒë·ªông th√†nh c√¥ng`)
    return response || "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ AI."
  } catch (error) {
    console.log(`‚ùå Model ${model} th·∫•t b·∫°i:`, error)

    // N·∫øu model b·ªã decommission ho·∫∑c kh√¥ng kh·∫£ d·ª•ng, th·ª≠ model ti·∫øp theo
    if (
      error instanceof Error &&
      (error.message.includes("decommissioned") ||
        error.message.includes("not found") ||
        error.message.includes("invalid_request_error"))
    ) {
      console.log(`üîÑ Th·ª≠ model ti·∫øp theo...`)
      return await tryWithDifferentModels(messages, currentModelIndex + 1)
    }

    // N·∫øu l√† l·ªói kh√°c (rate limit, network, etc.), throw ngay
    throw error
  }
}

// Function t·ªëi ∆∞u d·ªØ li·ªáu cho AI - gi·ªØ nguy√™n t·∫•t c·∫£ d·ªØ li·ªáu
const optimizeDataForAI = (data: any[]): string => {
  // Lo·∫°i b·ªè c√°c field null/empty ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc nh∆∞ng gi·ªØ nguy√™n c·∫•u tr√∫c
  const optimizedData = data.map((record) => {
    const optimizedFields: Record<string, any> = {}

    // Ch·ªâ gi·ªØ l·∫°i fields c√≥ gi√° tr·ªã
    for (const [key, value] of Object.entries(record.fields)) {
      if (value !== null && value !== undefined && value !== "") {
        optimizedFields[key] = value
      }
    }

    return {
      recordId: record.recordId,
      fields: optimizedFields,
    }
  })

  return JSON.stringify(optimizedData, null, 1) // D√πng indent = 1 ƒë·ªÉ ti·∫øt ki·ªám space
}

// Function chia d·ªØ li·ªáu th√†nh chunks v√† g·ª≠i ri√™ng bi·ªát
export const askAIWithFullData = async (data: any[], tableName: string, question: string): Promise<string> => {
  try {
    console.log(`ü§ñ X·ª≠ l√Ω ${data.length} records v·ªõi full data approach`)

    // Th·ª≠ g·ª≠i to√†n b·ªô d·ªØ li·ªáu ƒë√£ t·ªëi ∆∞u tr∆∞·ªõc
    const optimizedDataString = optimizeDataForAI(data)
    console.log(`üìä Optimized data length: ${optimizedDataString.length} characters`)

    // N·∫øu d·ªØ li·ªáu v·∫´n nh·ªè, g·ª≠i to√†n b·ªô
    if (optimizedDataString.length < 30000) {
      console.log("üì§ G·ª≠i to√†n b·ªô d·ªØ li·ªáu optimized...")

      const context = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† TO√ÄN B·ªò ${data.length} records t·ª´ b·∫£ng "${tableName}" trong Lark Base:

${optimizedDataString}

ƒê√¢y l√† t·∫•t c·∫£ ${data.length} b·∫£n ghi ƒë·∫ßy ƒë·ªß, kh√¥ng b·ªã c·∫Øt b·ªõt hay t√≥m t·∫Øt.

H√£y ph√¢n t√≠ch d·ªØ li·ªáu n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

      return await askAI(context, question)
    }

    // N·∫øu d·ªØ li·ªáu l·ªõn, chia th√†nh chunks
    const chunkSize = 25 // Chia nh·ªè h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o AI nh·∫≠n ƒë∆∞·ª£c t·∫•t c·∫£
    const chunks = []
    for (let i = 0; i < data.length; i += chunkSize) {
      chunks.push(data.slice(i, i + chunkSize))
    }

    console.log(`üìä Chia d·ªØ li·ªáu th√†nh ${chunks.length} chunks`)

    // G·ª≠i t·ª´ng chunk v√† t·ªïng h·ª£p k·∫øt qu·∫£
    let combinedAnalysis = `T√¥i ƒë√£ ph√¢n t√≠ch to√†n b·ªô ${data.length} b·∫£n ghi ƒë∆∞·ª£c chia th√†nh ${chunks.length} ph·∫ßn:\n\n`

    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i]
      const chunkDataString = JSON.stringify(
        chunk.map((record) => ({
          recordId: record.recordId,
          fields: record.fields,
        })),
        null,
        1,
      )

      console.log(`üì§ X·ª≠ l√Ω chunk ${i + 1}/${chunks.length} (${chunk.length} records)`)

      const chunkContext = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† ph·∫ßn ${i + 1}/${chunks.length} c·ªßa d·ªØ li·ªáu t·ª´ b·∫£ng "${tableName}":

${chunkDataString}

ƒê√¢y l√† ${chunk.length} b·∫£n ghi (t·ª´ ${i * chunkSize + 1} ƒë·∫øn ${Math.min((i + 1) * chunkSize, data.length)}) trong t·ªïng s·ªë ${data.length} b·∫£n ghi.

H√£y ph√¢n t√≠ch ph·∫ßn d·ªØ li·ªáu n√†y v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ng·∫Øn g·ªçn. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

      try {
        const chunkResult = await askAI(chunkContext, `Ph√¢n t√≠ch ph·∫ßn ${i + 1}: ${question}`)
        combinedAnalysis += `**Ph·∫ßn ${i + 1} (Records ${i * chunkSize + 1}-${Math.min((i + 1) * chunkSize, data.length)}):**\n${chunkResult}\n\n`
      } catch (error) {
        console.error(`‚ùå L·ªói x·ª≠ l√Ω chunk ${i + 1}:`, error)
        combinedAnalysis += `**Ph·∫ßn ${i + 1}:** L·ªói x·ª≠ l√Ω - ${error}\n\n`
      }

      // Delay ƒë·ªÉ tr√°nh rate limit
      await new Promise((resolve) => setTimeout(resolve, 1000))
    }

    // G·ª≠i c√¢u h·ªèi t·ªïng h·ª£p cu·ªëi c√πng
    const finalContext = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. T√¥i ƒë√£ ph√¢n t√≠ch to√†n b·ªô ${data.length} b·∫£n ghi t·ª´ b·∫£ng "${tableName}" theo t·ª´ng ph·∫ßn. D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ ph√¢n t√≠ch:

${combinedAnalysis}

D·ª±a tr√™n t·∫•t c·∫£ th√¥ng tin tr√™n, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p v√† ƒë·∫ßy ƒë·ªß cho c√¢u h·ªèi. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

    return await askAI(finalContext, question)
  } catch (error) {
    console.error("‚ùå askAIWithFullData failed:", error)
    return `‚ùå L·ªói khi ph√¢n t√≠ch d·ªØ li·ªáu: ${error}`
  }
}

export const askAI = async (context: string, question: string): Promise<string> => {
  try {
    console.log("ü§ñ B·∫Øt ƒë·∫ßu g·ªçi Groq API...")
    console.log("üìù Context length:", context.length)
    console.log("‚ùì Question:", question)

    const messages = [
      {
        role: "system" as const,
        content: context,
      },
      {
        role: "user" as const,
        content: question,
      },
    ]

    // Th·ª≠ v·ªõi c√°c model kh√°c nhau
    const response = await tryWithDifferentModels(messages)
    console.log("‚úÖ Groq API response received")

    return response
  } catch (error) {
    console.error("‚ùå Chi ti·∫øt l·ªói Groq API:", error)

    // X·ª≠ l√Ω c√°c lo·∫°i l·ªói c·ª• th·ªÉ
    if (error instanceof Error) {
      if (error.message.includes("rate_limit")) {
        return "‚è∞ API ƒë√£ ƒë·∫°t gi·ªõi h·∫°n t·ªëc ƒë·ªô. Vui l√≤ng th·ª≠ l·∫°i sau v√†i gi√¢y."
      } else if (error.message.includes("invalid_api_key")) {
        return "üîë API key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh."
      } else if (error.message.includes("context_length")) {
        return "üìè D·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω. H√£y th·ª≠ v·ªõi c√¢u h·ªèi c·ª• th·ªÉ h∆°n ho·∫∑c chia nh·ªè d·ªØ li·ªáu."
      } else if (error.message.includes("network") || error.message.includes("fetch")) {
        return "üåê L·ªói k·∫øt n·ªëi m·∫°ng. Vui l√≤ng ki·ªÉm tra internet v√† th·ª≠ l·∫°i."
      } else if (error.message.includes("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")) {
        return "ü§ñ T·∫•t c·∫£ c√°c AI model hi·ªán t·∫°i ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau."
      }

      return `‚ùå L·ªói AI: ${error.message}`
    }

    return "‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi AI. Vui l√≤ng th·ª≠ l·∫°i."
  }
}

// Function test mode - g·ª≠i to√†n b·ªô d·ªØ li·ªáu raw
export const askAIWithRawData = async (data: any[], tableName: string, question: string): Promise<string> => {
  try {
    console.log(`üß™ TEST MODE: G·ª≠i to√†n b·ªô ${data.length} records raw data`)

    const rawDataString = JSON.stringify(data, null, 2)
    console.log(`üìä Raw data length: ${rawDataString.length} characters`)

    const context = `B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D∆∞·ªõi ƒë√¢y l√† TO√ÄN B·ªò ${data.length} records t·ª´ b·∫£ng "${tableName}" trong Lark Base (RAW DATA):

${rawDataString}

QUAN TR·ªåNG: ƒê√¢y l√† t·∫•t c·∫£ ${data.length} b·∫£n ghi ƒë·∫ßy ƒë·ªß, kh√¥ng b·ªã t√≥m t·∫Øt hay c·∫Øt b·ªõt. B·∫°n c√≥ th·ªÉ truy c·∫≠p v√† ph√¢n t√≠ch t·∫•t c·∫£ d·ªØ li·ªáu n√†y.

H√£y ph√¢n t√≠ch d·ªØ li·ªáu n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.`

    return await askAI(context, question)
  } catch (error) {
    console.error("‚ùå askAIWithRawData failed:", error)
    return `‚ùå L·ªói khi g·ª≠i raw data: ${error}`
  }
}

// Function test API v·ªõi model m·ªõi
export const testGroqAPI = async (): Promise<{ success: boolean; message: string; workingModel?: string }> => {
  console.log("üß™ Testing Groq API v·ªõi c√°c model kh·∫£ d·ª•ng...")

  for (const model of AVAILABLE_MODELS) {
    try {
      console.log(`üß™ Testing model: ${model}`)

      const testCompletion = await groq.chat.completions.create({
        model: model,
        messages: [
          {
            role: "user",
            content: "Xin ch√†o! H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát: 1+1 b·∫±ng bao nhi√™u?",
          },
        ],
        temperature: 0.1,
        max_tokens: 50,
      })

      const response = testCompletion.choices[0].message.content
      console.log(`‚úÖ Model ${model} ho·∫°t ƒë·ªông! Response:`, response)

      return {
        success: true,
        message: `Model ${model} ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng. Test response: ${response}`,
        workingModel: model,
      }
    } catch (error) {
      console.log(`‚ùå Model ${model} failed:`, error)
      continue
    }
  }

  return {
    success: false,
    message: "T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i sau.",
  }
}

// Function ƒë·ªÉ l·∫•y danh s√°ch model kh·∫£ d·ª•ng
export const getAvailableModels = (): string[] => {
  return AVAILABLE_MODELS
}
